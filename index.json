[{"categories":["Blog"],"contents":"이번에 Github Blog를 이전하게 되었습니다!\n기존에는 Jekyll 기반의 블로그를 사용했었는데요.\n이게 약간.. 편한 듯 불편한 듯(특히나 gem bundle\u0026hellip;.관리).. 뭔가 부족한 느낌이였습니다.\nJekyll이 아닌 다른 기반의 Generator 를 찾아보기 시작했습니다.\n후보군은 세 개가 있었습니다. Hexo, Hugo, Gatsby\u0026hellip;\nHexo는 Jekyll 보다는 낫지만 별로라는 평이 있었고 Gastby는 프론트 알못인 저에겐 그냥 어렵\u0026hellip;더군요.\n그래서 Hugo를 이용해보기로 했습니다.\n그럼 Hugo를 이용한 블로그 세팅 시작하겠습니다.\n준비물   2개의 github repository\n 블로그 소스용: 이름 막 지어도 상관없음. ex) blog_source 호스팅용: {Username}.github.io 로 만드셔야 합니다. ex) jjerry-k.github.io    원하는 테마\n https://themes.gohugo.io 여기서 원하는걸 골라놓으세요.    세팅 과정 1. Repository 생성 준비문에서 얘기했듯 두개의 repository를 준비합니다.\n  블로그 소스용     호스팅용\n 사진을 올려야하는데 저는 이미 \u0026hellip; Jekyll로 쓰고 있던지라 \u0026hellip; 패스 블로그 소스용 repository 처럼 비어있도록 만들어주세요.    2. Hugo 설치 및 프로젝트 생성. brew install hugo hugo new site {프로젝트 이름} hugo를 설치하고 새 프로젝트를 생성합니다.\n저는 편의를 위해 프로젝트 이름을 blog_source 라고 했습니다.\n  3. 테마 설치 프로젝트 디렉토리에 themes 디렉토리가 보입니다. themes 디렉토리로 이동하여 위에서 골랐던 테마를 다운로드 합니다. 저는 liva-hugo 를 사용했습니다.\ncd blog_source cd themes git clone https://github.com/themefisher/liva-hugo.git     liva-hugo 는 이해가 쉽도록 exampleSite를 이용해보려고 합니다. blog_source/themes/liva-hugo/exampleSite 에 있는 content, static, config.toml 을 blog_source 에 있는 위치에 복사해줍니다.\ncp -r themes/liva-hugo/exampleSite/content/* ./content cp -r themes/liva-hugo/exampleSite/static/* ./static cp themes/liva-hugo/exampleSite/config.toml ./   4. 서버 실행 및 글쓰기 현재 블로그 상태가 어떤지 궁금합니다. 일단 hugo server를 실행시켜 봅니다.\n# 현재 위치: 프로젝트 디렉토리 hugo server -p {원하는 포트 번호} -D # Default PORT 1313    그리고 localhost:1313 로 접속을 하면 예시를 사이트 양식을 가져왔기 때문에 다음과 같이 나옵니다.   포스팅을 하기 전에 다른 포스팅을 확인해봅니다.   liva-hugo는 이러한 템플릿을 바탕으로 포스팅을 작성해야합니다.\n세부 내용은 직접 사용하시면서 익히시길\u0026hellip;.\n자 그럼 포스팅을 한번 써보도록 합시다.\nhugo new blog/hello.md 라고 입력을 해도 되고 blog/hello.md 를 직접 생성하셔도 됩니다.\n그 후 템플릿에 맞게 작성을 해줍니다.\n  이렇게 작성 후 저장을 하면 블로그에 다음과 같이 글이 생성된 것을 볼 수 있습니다.\n  hugo 공통적으로 템플릿에 draft:  부분을 true로 한다면 블로그에 반영되지 않습니다.\n반대로 false 를 적는다면 블로그에 반영이 됩니다.\n5. Github에 배포 커맨드로 바로 적겠습니다.\n# 현재 위치: 프로젝트 디렉토리 git init git remote add origin https://github.com/jjerry-k/blog_source #{블로그 소스용 repository} git submodule add -b master https://github.com/jjerry-k/jjerry-k.github.io public #{호스팅용 repository} 그 후 빌드 \u0026amp; 배포를 합니다.\n# 현재 위치: 프로젝트 디렉토리 hugo -t liva-hugo # hugo -t {테마 이름} cd public git add . git commit -m \u0026#34;Update\u0026#34; git push cd .. git add . git commit -m \u0026#34;Update\u0026#34; git push 1~5번 까지의 과정을 거친 후 {Username}.github.io 로 접속을 해보면 정상적으로 블로그가 생긴 것을 볼 수 있습니다.\n만약 새 글을 쓰고 싶다면?\n4번과 5번을 반복하시면 됩니다.\nP.S  블로그 이전 느므 힘들다\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/hugo-blog/","tags":["Hugo"],"title":"Hugo Blog"},{"categories":["Living"],"contents":"현재 D-Link 공유기를 사용중인데 올해 초? D-Link 측에서 DDNS 서비스를 종료한다는 공지가 떴습니다. (참고 포스팅)\n그래서..다른 서비스를 찾고 있던중 CODNS 라는 국내 서비스를 찾게 되었습니다.\n근데 이 서비스는 일정 기간 사용하면 수동으로 갱신을 해줘야하는 불편함이 있더라구요.\n그래서\u0026hellip;라즈베리파이에 crontab을 이용하여 자동 갱신 스크립트를 돌려 사용중입니다.\n# Edit this file to introduce tasks to be run by cron. # # Each task to run has to be defined through a single line # indicating with different fields when the task will be run # and what command to run for the task # # To define the time you can provide concrete values for # minute (m), hour (h), day of month (dom), month (mon), # and day of week (dow) or use \u0026#39;*\u0026#39; in these fields (for \u0026#39;any\u0026#39;). # # Notice that tasks will be started based on the cron\u0026#39;s system # daemon\u0026#39;s notion of time and timezones. # # Output of the crontab jobs (including errors) is sent through # email to the user the crontab file belongs to (unless redirected). # # For example, you can run a backup of all your user accounts # at 5 a.m every week with: # 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/ # # For more information see the manual pages of crontab(5) and cron(8) # # m h dom mon dow command 0 12 * * * /home/pi/codns/CODNS_CLIENT.LINUX -ipupdate ","permalink":"https://jjerry-test.github.io/blog/codns/","tags":["Hardware"],"title":"개인적인 CODNS CRONTAB"},{"categories":["DeepLearning"],"contents":"저번 Mediapipe의 Hands 포스팅에 이어서 Mediapipe의 Pose 를 테스트 해보겠습니다.\n실행 코드 import time import cv2 as cv import mediapipe as mp mp_drawing = mp.solutions.drawing_utils mp_pose = mp.solutions.pose prevTime = 0 idx = 0 pose = mp_pose.Pose( min_detection_confidence=0.5, min_tracking_confidence=0.5) cap = cv.VideoCapture(\u0026#39;./ufc.gif\u0026#39;) while cap.isOpened(): success, image = cap.read() curTime = time.time() if not success: break image = cv.cvtColor(cv.flip(image, 1), cv.COLOR_BGR2RGB) image.flags.writeable = False results = pose.process(image) image.flags.writeable = True image = cv.cvtColor(image, cv.COLOR_RGB2BGR) mp_drawing.draw_landmarks( image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS) sec = curTime - prevTime prevTime = curTime fps = 1/(sec) str = f\u0026#34;FPS : {fps:0.1f}\u0026#34; cv.putText(image, str, (0, 100), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0)) cv.imshow(\u0026#39;MediaPipe Pose\u0026#39;, image) cv.imwrite(f\u0026#34;./sample_{idx:05d}.jpg\u0026#34;, image) # for making gif idx += 1 if cv.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break pose.close() cap.release() 좀 더 해봐야겠지만 일단 지금 코드로는 제대로 잡진 못하는 것 같네요..\n흠\u0026hellip;.코드를 좀 수정해봐야겠습니다..\n  ","permalink":"https://jjerry-test.github.io/blog/mediapipe_3/","tags":["Tools"],"title":"MediaPipe - (3)"},{"categories":["DeepLearning"],"contents":"저번 Mediapipe와 Face mesh 포스팅에 이어서 Mediapipe의 Hands 를 테스트 해보겠습니다.\n실행 코드 import time import cv2 as cv import mediapipe as mp mp_drawing = mp.solutions.drawing_utils mp_hands = mp.solutions.hands hands = mp_hands.Hands( min_detection_confidence=0.7, min_tracking_confidence=0.5) cap = cv.VideoCapture(0) prevTime = 0 # idx = 0 while cap.isOpened(): success, image = cap.read() curTime = time.time() if not success: break image = cv.cvtColor(cv.flip(image, 1), cv.COLOR_BGR2RGB) image.flags.writeable = False results = hands.process(image) image.flags.writeable = True image = cv.cvtColor(image, cv.COLOR_RGB2BGR) if results.multi_hand_landmarks: for hand_landmarks in results.multi_hand_landmarks: mp_drawing.draw_landmarks( image, hand_landmarks, mp_hands.HAND_CONNECTIONS) sec = curTime - prevTime prevTime = curTime fps = 1/(sec) str = f\u0026#34;FPS : {fps:0.1f}\u0026#34; cv.putText(image, str, (0, 100), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0)) cv.imshow(\u0026#39;MediaPipe Hands\u0026#39;, image) # cv.imwrite(f\u0026#34;./sample_{idx:05d}.jpg\u0026#34;, image) # for making gif # idx += 1 if cv.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break hands.close() cap.release() 음\u0026hellip;.살짝살짝 끊기긴 하네요.\n  다음엔 파이썬으로 할 수 있는 마지막인 Pose 예제를 해보겠습니다.\n","permalink":"https://jjerry-test.github.io/blog/mediapipe_2/","tags":["Tools"],"title":"MediaPipe - (2)"},{"categories":["DeepLearning"],"contents":"이번 포스팅에선 Google의 MediaPipe에 대해서 다뤄보려합니다.\nMediaPipe is a framework for building multimodal (eg. video, audio, any time series data), cross platform (i.e Android, iOS, web, edge devices) applied ML pipelines. With MediaPipe, a perception pipeline can be built as a graph of modular components, including, for instance, inference models (e.g., TensorFlow, TFLite) and media processing functions.\n MediaPipe란 multi modal, cross platform 을 구축하기위한 프레임워크입니다. MediaPipe를 사용하면 TensorFlow, TFLite 같은 inference model과 미디어 처리 기능들을 모듈형식으로 구축할 수 있습니다.\n 라고 적혀있는 듯합니다.\n그냥 간단히 말하면..\n 만들어 놓은거 있으니까 모바일, Edge device, Web 필요한 곳에 가져다 쓰세요.\n 그렇다면 써봐야죠.\n먼저 지원하는 항목입니다.\nSolution list 이번 포스팅에선 Face Mesh를 테스트 해보겠습니다.\n설치법 # mediapipe가 opencv 4.0.0 이하와 호환이래요.. pip install mediapipe opencv-python==3.4.11.45 \u0026hellip;? 겁나\u0026hellip;간단합니다.\n실행 코드 import cv2 as cv import mediapipe as mp mp_drawing = mp.solutions.drawing_utils mp_face_mesh = mp.solutions.face_mesh face_mesh = mp_face_mesh.FaceMesh( min_detection_confidence=0.5, min_tracking_confidence=0.5) drawing_spec = mp_drawing.DrawingSpec(color=(128,128,128), thickness=1, circle_radius=1) cap = cv.VideoCapture(0) while cap.isOpened(): success, image = cap.read() if not success: break image = cv.cvtColor(cv.flip(image, 1), cv.COLOR_BGR2RGB) image.flags.writeable = False results = face_mesh.process(image) image.flags.writeable = True image = cv.cvtColor(image, cv.COLOR_RGB2BGR) if results.multi_face_landmarks: for face_landmarks in results.multi_face_landmarks: mp_drawing.draw_landmarks( image=image, landmark_list=face_landmarks, connections=mp_face_mesh.FACE_CONNECTIONS, landmark_drawing_spec=drawing_spec, connection_drawing_spec=drawing_spec) cv.imshow(\u0026#39;MediaPipe FaceMesh\u0026#39;, image) if cv.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break face_mesh.close() cap.release() 이걸 돌린다면 우째 나올까요???\n다음 애니메이션은 코드를 깨작 수정하여 GIF로 만든 것입니다.\n  별 기대 안하고 돌렸는데 제 맥미니 (Intel(R) Core(TM) i5-8500B CPU @ 3.00GHz) 으로 평균 30fps 이상 나오네요!\nGIF는 이것저것 처리를 한거라 30fps 처럼 안보입니다..\n이용해서 뭔가 재밌는걸 만들 수 있을 것 같습니다!\n추후에 시간이 된다면 Hands랑 Pose에 대한 결과도 추가로 올리겠습니다!\n참고 자료 https://mediapipe.dev/\nhttps://google.github.io/mediapipe/\nhttps://opensource.google/projects/mediapipe\nP.S  구글\u0026hellip;갓글\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/mediapipe_1/","tags":["Tools"],"title":"MediaPipe - (1)"},{"categories":["Ubuntu"],"contents":"Docker 에 대한 설명은 안합니다.\n사용법을 보러 오시는 분이라면 어느 정도 Docker 가 뭔지는 아실테니\u0026hellip;\n자세한 설명이 보고 싶으시다면 공식 홈페이지 혹은 책을 참고하세요.\n설치 방법은 간단하게 다음과 같습니다.\nsudo apt-get update sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; # DOCKER ENGINE 설치 sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # Docker 동작 확인 sudo docker run hello-world # sudo 를 붙이지 않기 위해 사용자 계정을 docker 그룹에 추가 sudo usermode -aG docker {계정명} sudo reboot 커맨드 소개 docker 라고 입력하면 수 많은 커맨드가 나옵니다.\n그 중에 이번 포스팅에 등장하는 커맨드를 정리 해보려 합니다.\ndocker ps: 컨테이너 리스트 출력\ndocker images: 이미지 리스트 출력\ndocker rm: 컨테이너 제거\ndocker rmi: 이미지 제거\ndocker build: Dockerfile 을 이용하여 이미지 구축\ndocker pull: Docker hub 에 있는 이미지 다운로드\ndocker run: 이미지를 실행하여 컨테이너 생성\n자신만의 Image, Container 만들어보기 예시는 ubuntu에 python 3.x를 설치하는 과정입니다.\ndocker run 옵션에 대해서는 링크(여기) 에서 확인하세요.\n# 지원하는 ubuntu tag # 14.04, trusty-20191217, trusty # 16.04, xenial-20200916, xenial # 18.04, bionic-20200921, bionic # 20.04, focal-20200925, focal, latest, rolling # 20.10, groovy-20200921, groovy, devel # Docker image download docker pull ubuntu:{원하는 버전 tag} # ex) # docker pull ubuntu:18.04 # docker pull ubuntu:groovy # Container 생성 docker run -ti ubuntu:{원하는 버전 tag} bash     # Docker container 에서 실행. apt update apt install -y build-essential cmake git curl wget vim unzip apt install -y ca-certificates libjpeg-dev libpng-dev software-properties-common add-apt-repository ppa:deadsnakes/ppa apt install python{원하는 버전} # 설치 확인 ls /usr/bin/ | grep python exit apt install 부분에서 python 3 중 stable 버전이 설치될 겁니다.\n그래서 지금은 3.8이 설치 됩니다. ( 3.8 을 설치하실 분들은 apt로 python을 설치할 필요 없음 )\n추가적으로 저는 3.7을 설치했습니다.\n그 결과 마지막 커맨드 (ls /usr/bin/ | grep python)를 입력하면 다음과 같이 출력이 나옵니다.\n  이렇게 하면 Container에 원하는 환경을 세팅했습니다.\ndocker ps -a   이를 Image로 만들어야 합니다.\ndocker commit {컨테이너 이름} {이미지 이름:태그} docker images docker rmi {컨테이너 이름}   앞으로는 docker run -ti --rm {이미지 이름:태그} bash 와 같이 실행하면 됩니다!\nDockerfile 이용해보기 이전의 예시는 Image download 부터 commit 까지 하나하나 직접 커맨드를 입력했습니다.\n그렇지 않고 파일을 만들고 그걸 이용해서 Image를 생성하는 방법이 있습니다.\nDockerfile과 docker build를 이용합니다.\nDockerfile을 만들 때도 커맨드를 사용하게 되는데 예시에서 사용할 커맨드는 다음과 같습니다.\nFROM: 어떤 image를 베이스로 할 것인지 지정.\nLABEL: 해당 Dockerfile로 만든 이미지의 정보, 작성자등의 정보 작성. (꼭 할 필요는 없음)\nARG: Dockerfile 내에서 사용되는 환경변수 (ENV와 혼동되기 쉬움)\nRUN: image를 구성에 필요한 각 단계를 실행\nFROM ubuntu:{원하는 버전 tag} LABEL maintainer \u0026#34;Jerry Kim \u0026lt;jaeyeol2931@gmail.com\u0026gt;\u0026#34; ARG PYTHON_VERSION={원하는 버전} # Docker container 에서 실행. RUN apt update -q RUN DEBIAN_FRONTEND=\u0026#39;noninteractive\u0026#39; apt install -y build-essential cmake git curl wget vim unzip RUN DEBIAN_FRONTEND=\u0026#39;noninteractive\u0026#39; apt install -y ca-certificates libjpeg-dev libpng-dev software-properties-common RUN add-apt-repository ppa:deadsnakes/ppa RUN apt install -y python$PYTHON_VERSION # 설치 확인 RUN ls /usr/bin/ | grep python 대충 디렉토리를 하나 만들고 위 내용이 담긴 Dockerfile을 만듭니다.\n  그리고 Dockerfile이 있는 디렉토리로 이동하여 다음 커맨드를 입력합니다.\ndocker build -t {이미지 이름:태그} .   그럼 다음과 마지막에 다음과 같은 출력이 나오고 docker images를 해보면 잘 생성된 것을 볼 수 있습니다.\n    P.S  apt → apt-get 으로 바꿔도 상관없어요. 피곤하네요. 의욕이 없네요. Docker Usage (2) 는 뭐로 하지..  ","permalink":"https://jjerry-test.github.io/blog/docker_usage_01/","tags":["Docker"],"title":"Docker Usage (1)"},{"categories":["DeepLearning"],"contents":"Deep learning model 을 실험하다보면 reproducibility 라는 말을 자주 듣게 됩니다.\n대충\u0026hellip; 돌릴 때 마다 결과가 달라요...라는 의미죠.\n그러면서 seed 라는 단어를 많이 듣게 되는데요.\nseed는 위키피디아, 혹은 다른 블로그를 참고하세요.\n이번엔 seed 에 따라 어떤 결과를 보이는지 실험을 해보려 합니다.\n실험 방법  실험 변수  데이터: Mnist 모델: Simple cnn GPU: V100 32GB 학습 횟수: 10 배치 사이즈: 1024 사용 Seed 종류: random seed, numpy seed, tensorflow seed, python hashseed   아무 seed 고정 없이 5번 씩 진행. 각 seed 별로 5번씩 진행.  실험 결과 Seed 고정 없음   PYTHONHASHSEED   Random   Numpy   TensorFlow   원래 마지막으로 4개의 seed 를 모두 고정하고 실험도 해보려고 했는데 TensorFlow 만 고정해도 거의 동일한 결과가\u0026hellip;나오더군요. (예전엔 아니었는데\u0026hellip;)\nTensorFlow 를 쓰시는 분들은 tf 만 고정해도 어느정도 균일한 결과를 볼 수 있을 것 같습니다.\nP.S  정말 간단한 실험이니 너무 믿지는 마세요.  ","permalink":"https://jjerry-test.github.io/blog/seed/","tags":["TensorFlow"],"title":"Seed 가 뭐길래.."},{"categories":["Ubuntu"],"contents":"# nvidia docker 서비스 시작 sudo service nvidia-docker start # 특정 포트 터널링, 경로 마운트, Docker image:tag 를 이용하여 container 로 실행 docker run -ti -rm -p {호스트포트}:{도커포트} -v {호스트경로}:{도커경로} {IMAGE_NAME:tag} bash # 종료된 Container 삭제 docker rm {CONTAINER_NAEM} # Docker Image 삭제 docker rmi {IMAGE_NAME} # 종료된 Container를 삭제하지 않고 새로운 IMAGE로 생성 docker commit {CONTAINER_NAEM} {IMAGE_NAME:TAG} # Docker container 내에서 jupyter notebook 혹은 jupyter lab을 실행할때 사용되는 커맨드 CUDA_VISIBLE_DEVICES=0 jupyter notebook --ip=0.0.0.0 --port=포트번호 --allow-root ","permalink":"https://jjerry-test.github.io/blog/docker/","tags":["Docker"],"title":"자주 사용하는 Docker Command 정리"},{"categories":["DeepLearning"],"contents":"오늘은 Weights \u0026amp; Biases 의 기능중 하나인 sweep에 대해서 알아보려고 합니다.\nsweep은 정말 쉽게 말해서 Hyper parameter search and model optimization 을 쉽게 할 수 있도록 해주는 기능입니다.\n자세한 장단점에 대해서 궁금하신 분은 여기를 눌러서 확인해주세요!\n그럼 사용법을 알아보겠습니다.\n1. 학습 스크립트 작성.  model.py와 train.py 이렇게 두 개의 스크립트를 작성했습니다.  # model.py import tensorflow as tf from tensorflow.keras import models, layers, losses, optimizers from tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201 from tensorflow.keras.applications import VGG16, VGG19, Xception, InceptionResNetV2, InceptionV3 from tensorflow.keras.applications import MobileNet, MobileNetV2, NASNetLarge, NASNetMobile from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152, ResNet50V2, ResNet101V2, ResNet152V2 model_dict = { \u0026#34;vgg16\u0026#34;: VGG16, \u0026#34;vgg19\u0026#34;: VGG19, \u0026#34;resnet50\u0026#34;: ResNet50, \u0026#34;resnet101\u0026#34;: ResNet101, \u0026#34;resnet152\u0026#34;: ResNet152, \u0026#34;resnet50v2\u0026#34;: ResNet50V2, \u0026#34;resnet101v2\u0026#34;: ResNet101V2, \u0026#34;resnet152v2\u0026#34;: ResNet152V2, \u0026#34;densenet121\u0026#34;: DenseNet121, \u0026#34;densenet169\u0026#34;: DenseNet169, \u0026#34;densenet201\u0026#34;: DenseNet201, \u0026#34;mobilenet\u0026#34;: MobileNet, \u0026#34;mobilenetv2\u0026#34;: MobileNetV2, \u0026#34;xception\u0026#34;: Xception, \u0026#34;inceptionresnetv2\u0026#34;: InceptionResNetV2, \u0026#34;inceptionv3\u0026#34;: InceptionV3, \u0026#34;nasnetlarge\u0026#34;: NASNetLarge, \u0026#34;nasnetmobile\u0026#34;: NASNetMobile } def build_model(config, num_classes=10, name=\u0026#34;model\u0026#34;): assert config.model_name.lower() in model_dict.keys(), f\u0026#34;Please, check pretrained model list {list(model_dict.keys())}\u0026#34; last_activation = \u0026#34;softmax\u0026#34; if num_classes \u0026gt; 1 else \u0026#34;sigmoid\u0026#34; base_model = model_dict[config.model_name.lower()](include_top=False, weights=\u0026#34;imagenet\u0026#34;, pooling=\u0026#34;avg\u0026#34;) if config.freeze: base_model.trainable = False output = layers.Dropout(config.dropout, name=f\u0026#34;{name}_dropout\u0026#34;)(base_model.output) output = layers.Dense(num_classes, last_activation, name=f\u0026#34;{name}_output\u0026#34;)(output) model = models.Model(base_model.input, output, name=name) return model #train.py import os import cv2 as cv import numpy as np import tensorflow as tf from tensorflow.keras import optimizers, utils from model import * import wandb from wandb.keras import WandbCallback tf.random.set_seed(42) hyperparameter_defaults = dict( model_name=\u0026#34;mobilenet\u0026#34;, dropout = 0.5, freeze = 1, batch_size = 128, learning_rate = 0.001, epochs = 5, GPUs=\u0026#34;0\u0026#34; ) wandb.init(project=\u0026#34;usage_02\u0026#34;, config=hyperparameter_defaults) config = wandb.config os.environ[\u0026#34;CUDA_DEVICE_ORDER\u0026#34;]=\u0026#34;PCI_BUS_ID\u0026#34; os.environ[\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;]=config.GPUs # For Efficiency gpus = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(\u0026#39;GPU\u0026#39;) print(len(gpus), \u0026#34;Physical GPUs,\u0026#34;, len(logical_gpus), \u0026#34;Logical GPUs\u0026#34;) except RuntimeError as e: print(e) # Data Prepare URL = \u0026#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\u0026#39; path_to_zip = tf.keras.utils.get_file(\u0026#39;flower_photos.tgz\u0026#39;, origin=URL, extract=True) PATH = os.path.join(os.path.dirname(path_to_zip), \u0026#39;flower_photos\u0026#39;) category_list = [i for i in os.listdir(PATH) if os.path.isdir(os.path.join(PATH, i)) ] print(category_list) num_classes = len(category_list) img_size = 128 def read_img(path, img_size): img = cv.imread(path) img = cv.cvtColor(img, cv.COLOR_BGR2RGB) img = cv.resize(img, (img_size, img_size)) return img imgs_tr = [] labs_tr = [] imgs_val = [] labs_val = [] for i, category in enumerate(category_list): path = os.path.join(PATH, category) imgs_list = os.listdir(path) print(\u0026#34;Total \u0026#39;%s\u0026#39; images : %d\u0026#34;%(category, len(imgs_list))) ratio = int(np.round(0.05 * len(imgs_list))) print(\u0026#34;%sImages for Training : %d\u0026#34;%(category, len(imgs_list[ratio:]))) print(\u0026#34;%sImages for Validation : %d\u0026#34;%(category, len(imgs_list[:ratio]))) print(\u0026#34;=============================\u0026#34;) imgs = [read_img(os.path.join(path, img),img_size) for img in imgs_list] labs = [i]*len(imgs_list) imgs_tr += imgs[ratio:] labs_tr += labs[ratio:] imgs_val += imgs[:ratio] labs_val += labs[:ratio] imgs_tr = np.array(imgs_tr)/255. labs_tr = utils.to_categorical(np.array(labs_tr), num_classes) imgs_val = np.array(imgs_val)/255. labs_val = utils.to_categorical(np.array(labs_val), num_classes) print(imgs_tr.shape, labs_tr.shape) print(imgs_val.shape, labs_val.shape) # Build Network strategy = tf.distribute.MirroredStrategy() with strategy.scope(): model = build_model(config, num_classes) loss = \u0026#39;binary_crossentropy\u0026#39; if num_classes==1 else \u0026#39;categorical_crossentropy\u0026#39; model.compile(optimizer=optimizers.Adam(config.learning_rate), loss=loss, metrics=[\u0026#39;acc\u0026#39;]) # Training Network model.fit(x=imgs_tr, y=labs_tr, batch_size=config.batch_size, epochs=config.epochs, callbacks = [WandbCallback()], validation_data=(imgs_val, labs_val)) 2. 기본 값으로 학습 진행. python train.py 그 후에 Weights \u0026amp; Biases 로 가보면 다음과 같이 프로젝트가 만들어진 것을 확인 할 수 있습니다.\n  3. sweep configuration 프로젝트 창에서 왼쪽에 빗자루 모양 아이콘을 누른 후에 오른쪽 상단에 Create sweep 를 눌러줍니다. ****\n  그러면 다음과 같은 창이 나오는데요. 이제 hyper parameter search를 어떻게 할건지 세팅하는 단계입니다.\n  저는 다음과 같이 세팅을 했습니다.\n이렇게 하면 grid search\u0026hellip;니까\u0026hellip;.\n2 * 5 * 3 * 2 * 3 = 180 개의 결과가 나오겠군요\u0026hellip;\nprogram: train.py method: grid metric: name: loss goal: minimize parameters: GPUs: value: \u0026#34;0\u0026#34; epochs: value: 10 freeze: values: [0, 1] dropout: values: [0.1, 0.2, 0.4, 0.5, 0.7] batch_size: values: [64, 128, 256] model_name: values: [mobilenet, mobilenetv2] learning_rate: values: [0.001, 0.005, 0.0005] 그리고 Initialize Sweep 를 눌러줍니다! 그러면 다음과 같은 화면이 나올거에요!\n  4. Sweep 실행 가운데에 적혀있는 커맨드를 터미널에서 실행시켜줍니다!\nwandb agent {sweep-id} 그러면 터미널에 wandb: Starting wandb agent 🕵️ 이런 문구와 함께 세팅대로 Network를 학습하게 됩니다!\n이제 잠시 티타임을\u0026hellip;가집니다.. (잠깐이 아닐 수도 있음..)\n  5. 결과 확인 Weights \u0026amp; Biases 화면에서 View Sweep 을 눌러주세요!\n그리고 좌측에 Plot 아이콘을 누르면 다음과 같은 화면이 나옵니다!\n원래는 위에 Chart Panel 있는데 접어놨습니다.\n이 Panel은 기본적으로 세 개의 그래프로 구성되어 있습니다.\n 각 학습 별 결과 분포도 hyper parameter와 목표 값( 저는 loss ) 간의 중요도, 상관관계 각 학습 별 Parallel graph    이번엔 Hyper parameter tuning을 편하게 할 수 있는 Weighs \u0026amp; Biases의 Sweep에 대해 알아봤습니다.\n일일히 값을 변경하지 않고 지정해 놓으면 알아서 진행을 할테니..사용자는 좀 쉴\u0026hellip;\u0026hellip;수 있겠죠..? (아닐 듯..)\n그럼 포스팅을 마치겠습니다! 감사합니다!\nP.S   wandb sweep 실행을 background 에서 하는 방법\u0026hellip;?\nnohup wandb agent {sweep_id} \u0026gt; nohup.out   이 다음은 뭐 하지\u0026hellip;\n  ","permalink":"https://jjerry-test.github.io/blog/sweep/","tags":["Tools"],"title":"Weights \u0026 Biases Usage - [2]"},{"categories":["DeepLearning"],"contents":"이번 포스팅은 Weights \u0026amp; Biases 를 활용하여 Network 성능 비교 예시를 하려고 합니다.\n어려운 글이 아니기 때문에 금방 금방 따라하실 수 있을 것 같습니다!\nTask  Flower classification  daisy, roses, dandelion, sunflowers, tulips   List of pretrained-model  tf.keras.applications   Detail of training  Image size: 150x150x3 Epochs: 5 Batch size: 256 Freezing: True    자세한 코드는 for_wandb 라는 repository에 올려놨으니 확인하시면 됩니다.\ntrain.sh를 실행하게 되면 위에 적혀있는 pretrined model 만 변경해서 classification 을 수행하게 됩니다!\n그 후 weights \u0026amp; biases 화면을 가서 확인해 보면\u0026hellip;\n각 모델별 loss, acc, val_loss, val_acc graph를 확인할 수 있습니다.\n  좌측 상단에 Run 옆을 보시면 테이블 모양 아이콘이 있는데 이를 누르면 다음과 같이 테이블로 정리되어 있는 것을 확인 하실 수 있습니다.\n  이번 포스팅은 Weights \u0026amp; Biases를 이용하여 모델 별 성능 비교를 해보았습니다.\n조만간\u0026hellip;Sweep 이라는 Hyperparameter search and model optimization 방법에 대해 포스팅을 해보려 합니다!\nP.S  마스크 답답\u0026hellip;.. 날씨 후덥지근, 짜증\u0026hellip;.  ","permalink":"https://jjerry-test.github.io/blog/wandb_usage_1/","tags":["Tools"],"title":"Weights \u0026 Biases Usage - [1]"},{"categories":["DeepLearning"],"contents":" 본 포스팅은 고려대학교 산업경영공학부 Data Science \u0026amp; Business Analytics 연구실의 강필성 교수님의 자료를 정리한 포스팅입니다.\n  Contents of Posting  Contents of Posting Paper Reading Roadmap  ML Basics Data Mining  General Patter Mining Clustering   Artificial Intelligence  General Reinforcement Learning Transfer Learning   Supervised Learning  Kernel Machines Ensemble   Semi-supervvised Learning Unsupervised Learning Neural Network  General Structure Learning Strategies   NLP  General Topic Modeling Repersentation Learning Classification Summarization Machine Translation Question Answering   Vision  Classification Object Detection Localization \u0026amp; Segmentation      Paper Reading Roadmap ML Basics  The matrix calculus you need for deep learning Statistical Modeling: The Two Cultures Machine learning: Trends, perspectives, and prospects An introduction to ROC analysis Learning from imbalanced data Variational inference: A review for statisticians The expectation-maximization algorithm Dimension Reduction: A Guided Tour  Data Mining General  Interestingness Measures for Data Mining: A Survey The PageRank citation ranking: Bringing order to the web Process Mining Manifesto An Introduction to Variable and Feature Selection  Patter Mining  Fast Algorithm for Mining Association Rules A survey of sequential pattern mining A Survey of Parallel Sequential Pattern Mining  Clustering  A density-based algorithm for discovering clusters in large spatial databases with noise Data Clustering: A Review Techniques of Cluster Algorithms in Data Mining Survey of Clustering Data Mining Techniques On Clustering Validation Techniques clValid: An R Package for Cluster Validation  Artificial Intelligence General  Learning Deep Architectures for AI Representation learning: A review and new perspectives Generative Adversarial Networks From evolutionary computation to the evolution of things Probabilistic machine learning and artificial intelligence AutoML: A Survey of the State-of-the-Art  Reinforcement Learning  Human-level control through deep reinforcement Mastering the game of Go with deep neural networks and tree search An Introduction to Deep Reinforcement Learning World Models  Transfer Learning  Zero-shot learning through cross-modal transfer Lifelong Learning with Dynamically Expandable Networks  Supervised Learning Kernel Machines  An Introduction to Kernel-based Learning Algorithms A Tutorial on Support Vector Machine for Pattern Recognition A Tutorial on Support Vector Regression A Tutorial on nu-Support Vector Machines  Ensemble  Bagging Predictors Random Forests A short introduction to boosting Greedy Function Approximation: A Gradient Boosting Machine Gradient Boosting Machine, A Tutorial XGBoost: A Scalable Tree Boosting System LightGBM: A Highly Efficient Gradient Boosting Decision Tree CatBoost : unbiased boosting with categorical features  Semi-supervvised Learning  Combining Labeled and Unlabeled Data with Co-Training Semi-supervised Learning with Deep Generative Models Semi-Supervised Classification with Graph Convolutional Networks MixMatch: A Holistic Approach to Semi-Supervised Learning ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence  Unsupervised Learning  Anomaly Detection: A Survey Deep Learning for Anomaly Detection: A Survey A Review of Novelty Detection LOF: Identifying Density-Based Local Outliers Support Vector Data Description Isolation Forest Isolation-based Anomaly Detection DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning  Neural Network General  Deep learning  Structure  Long Short-Term Memory LSTM: A Search Space Odyssey Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling Sequence to sequence learning with neural networks Memory Networks End-To-End Memory Networks WaveNet: A Generative Model for Raw Audio An Introduction to Variational Autoencoders A Comprehensive Survey on Graph Neural Networks  Learning Strategies  Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Dropout: A Simple Way to Prevent Neural Networks from Overtting ADAM: A Method for Stochastic Optimization An overview of gradient descent optimization algorithms Layer normalization Group normalization  NLP General  Natural Language Processing (Almost) from Scratch Advances in natural language processing Recent trends in deep learning based natural language processing  Topic Modeling  An introduction to latent semantic analysis Probabilistic latent semantic analysis Probabilistic topic models Latent Dirichlet Allocation  Repersentation Learning  A Neural Probabilistic Language Model Distributed representations of words and phrases and their compositionality Efficient Estimation of Word Representations in Vector Space Glove: Global vectors for word representation Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation Enriching word vectors with subword information Bert: Pre-training of deep bidirectional transformers for language understanding Deep contextualized word representations Improving language understanding by generative pre-training Language models are unsupervised multitask learners Language Models are Few-Shot Learners  Classification  Convolutional neural networks for sentence classification Deep learning for sentiment analysis: A survey  Summarization  TextRank: Bringing Order into Texts A Neural Attention Model for Abstractive Sentence Summarization  Machine Translation  On the Properties of Neural Machine Translation: Encoder-Decoder Approaches Effective Approaches to Attention-based Neural Machine Translation Neural Machine Translation by Jointly Learning to Aligh and Translate Google\u0026rsquo;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation Attention is all you need  Question Answering  VQA: Visual Question Answering Ask Me Anything: Dynamic Memory Networks for Natural Language Processing Squad: 100,000+ questions for machine comprehension of text Know what you don\u0026rsquo;t know: Unanswerable questions for SQuAD  Vision Classification  Imagenet classification with deep convolutional neural networks Visualizing and understanding convolutional networks Very Deep Convolutional Networks for Large-Scale Image Recognition Going deeper with convolutions Deep residual learning for image recognition Densely Connected Convolutional Networks  Object Detection  Overfeat: Integrated recognition, localization and detection using convolutional networks Rich feature hierarchies for accurate object detection and semantic segmentation Fast R-CNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks You Only Look Once: Unified, Real-Time Object Detection YOLO9000: Better, Faster, Stronger YOLOv3: An Incremental Improvement YOLOv4: Optimal Speed and Accuracy of Object Detection  Localization \u0026amp; Segmentation  U-Net: Convolutional Networks for Biomedical Image Segmentation Learning deep features for discriminative localization Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization  ","permalink":"https://jjerry-test.github.io/blog/paper_roadmap/","tags":["Paper"],"title":"Paper Reading Roadmap"},{"categories":["DeepLearning"],"contents":"어제는 RAPIDS에 대한 소개와 RAPIDS APIs 중 cuDF 에 대한 예제에 대한 포스팅을 했습니다.\n오늘은 RAPIDS APIS 중 cuML 에 대한 예제 포스팅을 간.단.하.게 해보겠습니다.\n비교를 위해 KNN Classifier를 준비하였고 성능 비교를 위해 Scikit-learn 을 사용하였습니다.\n# %% # RAPIDS cuML kNN model import time import cudf, cuml import pandas as pd from cuml.neighbors import KNeighborsClassifier as cuKNeighbors from sklearn.neighbors import KNeighborsClassifier as skKNeighbors print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;============= Using cuML ===============\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========================================\u0026#34;) train = cudf.read_csv(\u0026#39;./mnist/train.csv\u0026#39;) test = cudf.read_csv(\u0026#39;./mnist/test.csv\u0026#39;) start = time.time() model = cuKNeighbors(n_neighbors=7) model.fit(train.iloc[:,1:785], train.iloc[:,0]) # y_hat = model.predict(test) # Exception occured (version 0.14.0)  print(f\u0026#34;Elapsed Time: {time.time()-start}\\n\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========== Using Scikit-learn ==========\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========================================\u0026#34;) train = pd.read_csv(\u0026#39;./mnist/train.csv\u0026#39;) test = pd.read_csv(\u0026#39;./mnist/test.csv\u0026#39;) start = time.time() model = skKNeighbors(n_neighbors=7) model.fit(train.iloc[:,1:785], train.iloc[:,0]) # y_hat = model.predict(test) print(f\u0026#34;Elapsed Time: {time.time()-start}\u0026#34;) ======================================== ======================================== ============= Using cuML =============== ======================================== ======================================== Elapsed Time: 0.3921339511871338 ======================================== ======================================== ========== Using Scikit-learn ========== ======================================== ======================================== Elapsed Time: 23.88076663017273 Test 시 성능도 비교하려했으나 cuML 버전 에러로 인해 테스트는 못했습니다.\n그래도 fit 부분에서 매우 큰 차이를 보입니다!\n역시나\u0026hellip;.GPU네요!\nP.S  너무 간단하게 테스트하는건가\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/rapids-cuml/","tags":["Tools"],"title":"RAPIDS APIs ( cuML )"},{"categories":["DeepLearning"],"contents":" RAPIDS is a suite of software libraries for executing end-to-end data science \u0026amp; analytics pipelines entirely on GPUs.\n 위 글은 RAPIDS 공식 홈페이지에서 가져온 내용입니다.\n말 그대로 모든 과정을 GPU 에서 실행하도록 도와주는 라이브러리입니다.\nPREREQUISITES GPU\n NVIDIA Pascal™ or better with compute capability 6.0+  OS\n Ubuntu 16.04/18.04 or CentOS 7 with gcc/++ 7.5+ See RSN 1 for details on our recent update to gcc/++ 7.5 RHEL 7 support is provided through CentOS 7 builds/installs  Docker\n Docker CE v19.03+ and nvidia-container-toolkit Legacy Support - Docker CE v17-18 and nvidia-docker2  CUDA \u0026amp; NVIDIA Drivers\n One of the following supported versions: 10.0 \u0026amp; v410.48+ 10.1.2 \u0026amp; v418.87+ 10.2 \u0026amp; v440.33+  RAPIDS APIs  RAPIDS 에는 현재 5개의 라이브러리를 제공. cuDF ( 요것이 Main )  Python GPU DataFrame library   cuML  a suite of libraries that implement machine learning algorithms and mathematical primitives functions   cuGraph  a GPU accelerated graph analytics library   nvStrings  a pandas-like API that will be familiar to data engineers \u0026amp; data scientists   Cyber Log Accelerators(CLX)  a collection of RAPIDS examples for security analysts, data scientists, and engineers to quickly get started applying RAPIS and GPU acceleration to real-world cybersecurity use cases    Installation Guide https://rapids.ai/start.html#get-rapids\nAnaconda or Miniconda  설치하는데 시간이 꽤나 오래 걸려요\u0026hellip;  conda install -c rapidsai -c nvidia -c conda-forge \\  -c defaults rapids=0.14 python=3.7 cudatoolkit=10.1 # cudatoolkit은 사용하는 DL 프레임워크 호환성 고려하세요! Example 이번 포스팅에선 cuDF 에 대한 예제를 보여드리려 합니다.\n추가적으로 Dask, Dask-cuDF 라는 2개의 라이브러리를 사용합니다.\n먼저 사용할 Package 들을 import 해줍니다.\nimport os import numpy as np import pandas as pd import cudf import dask_cudf 다음 예시는 pandas와 cudf 를 비교하는 예시입니다.\n# Series s = pd.Series([1,2,3,None,4]) print(\u0026#34;Pandas\u0026#34;) print(s) gs = cudf.Series([1,2,3,None,4]) print(\u0026#34;\\ncuDF\u0026#34;) print(gs) dask_gs = dask_cudf.from_cudf(gs, npartitions=2) print(\u0026#34;\\nDask-cuDF\u0026#34;) print(dask_gs.compute()) Pandas 0 1.0 1 2.0 2 3.0 3 NaN 4 4.0 dtype: float64 cuDF 0 1 1 2 2 3 3 null 4 4 dtype: int64 Dask-cuDF 0 1 1 2 2 3 3 null 4 4 dtype: int64 # DataFrame  pdf = pd.DataFrame({\u0026#39;a\u0026#39;: list(range(20)), \u0026#39;b\u0026#39;: list(reversed(range(20))), \u0026#39;c\u0026#39;: list(range(20))}) print(\u0026#34;\\nPandas\u0026#34;) print(pdf.head(5)) gdf = cudf.DataFrame.from_pandas(pdf) print(\u0026#34;\\ncuDF\u0026#34;) print(gdf.head(5)) dask_gdf = dask_cudf.from_cudf(gdf, npartitions=2) print(\u0026#34;\\nDask-cuDF\u0026#34;) print(dask_gdf.compute().head(5)) Pandas a b c 0 0 19 0 1 1 18 1 2 2 17 2 3 3 16 3 4 4 15 4 cuDF a b c 0 0 19 0 1 1 18 1 2 2 17 2 3 3 16 3 4 4 15 4 Dask-cuDF a b c 0 0 19 0 1 1 18 1 2 2 17 2 3 3 16 3 4 4 15 4 # Sorting print(\u0026#34;\\nPandas\u0026#34;) print(pdf.sort_values(by=\u0026#39;b\u0026#39;).head(5)) print(\u0026#34;\\ncuDF\u0026#34;) print(gdf.sort_values(by=\u0026#39;b\u0026#39;).head(5)) print(\u0026#34;\\nDask-cuDF\u0026#34;) print(dask_gdf.sort_values(by=\u0026#39;b\u0026#39;).compute().head(5)) Pandas a b c 19 19 0 19 18 18 1 18 17 17 2 17 16 16 3 16 15 15 4 15 cuDF a b c 19 19 0 19 18 18 1 18 17 17 2 17 16 16 3 16 15 15 4 15 Dask-cuDF a b c 19 19 0 19 18 18 1 18 17 17 2 17 16 16 3 16 15 15 4 15 이번엔 큰 DataFrame을 만들어서 sorting 속도를 비교하는 코드입니다.\n# %% # Speed Test num_element = 1000000 pdf = pd.DataFrame({\u0026#39;a\u0026#39;: list(range(num_element)), \u0026#39;b\u0026#39;: list(reversed(range(num_element))), \u0026#39;c\u0026#39;: np.random.randint(0, 1200000, num_element)}) print(pdf.head(5)) gdf = cudf.DataFrame.from_pandas(pdf) print(gdf.head(5)) dask_gdf = dask_cudf.from_cudf(gdf, npartitions=2) print(dask_gdf.compute().head(5)) 실행 속도는 ipython의 timeit 이라는 magic function 을 이용하였습니다.\n%%timeit pdf.sort_values(by=\u0026#34;c\u0026#34;) %%timeit gdf.sort_values(by=\u0026#34;c\u0026#34;) %%timeit dask_gdf.sort_values(by=\u0026#34;c\u0026#34;).compute() num_element를 바꿔가면서 테스트를 해봤습니다.\n# num_element = 1000 448 µs ± 38.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 2.84 ms ± 1.56 ms per loop (mean ± std. dev. of 7 runs, 100 loops each) 83.4 ms ± 47.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) # num_element = 10000 1.39 ms ± 42.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 3.14 ms ± 1.99 ms per loop (mean ± std. dev. of 7 runs, 100 loops each) 89.9 ms ± 24.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) # num_element = 100000 13.8 ms ± 467 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) 5.61 ms ± 2.55 ms per loop (mean ± std. dev. of 7 runs, 100 loops each) 116 ms ± 29.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) #num_element = 1000000 225 ms ± 31.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 8.17 ms ± 534 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) 118 ms ± 27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 적은 양에서는 CPU 가 더 빠르지만 점점 대규모로 늘어나니\u0026hellip;.GPU 가 빠르군요.\n아! Dask_cuDF는 보통 multi gpu 일 때 사용하는데 제가 실험한 환경은 NIPA 수시 사용자\u0026hellip;환경이라 V100 단일 환경이었습니다.\n그래서\u0026hellip;dask_gdf 에 대한 속도는 믿지 마세요..\nP.S  서터레스를 많이 받습니다. 원인은\u0026hellip;.\u0026lsquo;그\u0026rsquo; 집회\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/rapids/","tags":["Tools"],"title":"RAPIDS!!!"},{"categories":["Ubuntu"],"contents":"지극히 개인이 사용하기 위한 vimrc   마음껏 편하신대로 Copy \u0026amp; Paste 하세요!  call plug#begin(\u0026#39;~/.vim/plugged\u0026#39;)Plug \u0026#39;davidhalter/jedi-vim\u0026#39;Plug \u0026#39;preservim/nerdtree\u0026#39;Plug \u0026#39;jeetsukumaran/vim-pythonsense\u0026#39;Plug \u0026#39;jiangmiao/auto-pairs\u0026#39;Plug \u0026#39;Vimjas/vim-python-pep8-indent\u0026#39;Plug \u0026#39;dense-analysis/ale\u0026#39;Plug \u0026#39;neoclide/coc.nvim\u0026#39;, {\u0026#39;branch\u0026#39;: \u0026#39;release\u0026#39;}\u0026#34;Plug \u0026#39;liuchengxu/vista.vim\u0026#39;Plug \u0026#39;sheerun/vim-polyglot\u0026#39;\u0026#34;Plug \u0026#39;python-mode/python-mode\u0026#39;, { \u0026#39;for\u0026#39;: \u0026#39;python\u0026#39;, \u0026#39;branch\u0026#39;: \u0026#39;develop\u0026#39; }Plug \u0026#39;junegunn/seoul256.vim\u0026#39;call plug#end()\u0026#34; 세부 정보 출력set nuset titleset showmatchset ruler\u0026#34; 구문 강조 사용if has(\u0026#34;syntax\u0026#34;) syntax onendif\u0026#34; 들여쓰기 설정set autoindentset smartindentset tabstop=4set shiftwidth=4set softtabstop=4set smarttabset expandtab\u0026#34; 한글 입력 설정set encoding=utf-8set termencoding=utf-8\u0026#34; 커서가 있는 줄을 강조함set cursorline\u0026#34; 상태바 표시를 항상한다set laststatus=2set statusline=\\ %\u0026lt;%l:%v\\ [%P]%=%a\\ %h%m%r\\ %F\\\u0026#34; 검색 설정set ignorecase\u0026#34; 마지막으로 수정된 곳에 커서를 위치함au BufReadPost *\\ if line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026gt; 0 \u0026amp;\u0026amp; line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026lt;= line(\u0026#34;$\u0026#34;) |\\ exe \u0026#34;norm g`\\\u0026#34;\u0026#34; |\\ endif\u0026#34;seoul256let g:seoul256_background = 233colo seoul256","permalink":"https://jjerry-test.github.io/blog/vimrc/","tags":["Setting"],"title":"개인적인 Vim 설정"},{"categories":["DeepLearning"],"contents":"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Author: Jun-Yan Zhu∗, Taesung Park∗, Phillip Isola, Alexei A. Efros\nDate: Mar 30, 2017\nURL: https://arxiv.org/abs/1703.10593\nIntroduction  Image-to-image translation 은 paired data의 상황에서 많이 연구. 하지만 실제 환경에선 이런 paired data를 구하기 힘듦. 본 논문은 unpaired data 상황에서 Network가 image-to-image 를 잘 학습하는 것에 초점을 맞춤.    image     image   Formulation  \\(X, Y\\): X, Y 도메인의 데이터 \\(G, F\\): X to Y generator, Y to X generator \\(D_X, D_Y\\): X, Y 도메인에 대한 Discriminator  Adversarial Loss $$\\mathcal{L}_{GAN}(G, D_Y, X, Y) = \\mathbb{E}_{y\\sim p_{data}(y)}[\\log D_Y(y)] + \\mathbb{E}_{x\\sim p_{data}(x)}[\\log (1 - D_Y(G(x)))]$$\n$$\\mathcal{L}_{GAN}(F, D_X, Y, X) = \\mathbb{E}_{x\\sim p_{data}(x)}[\\log D_X(x)] + \\mathbb{E}_{y\\sim p_{data}(y)}[\\log (1 - D_x(F(y)))]$$\nCycle Consistency Loss  각각의 X, Y 데이터를 Y, X 데이터로 변환 후 다시 X, Y 데이터 복원.  $$\\mathcal{L}_{cyc}(G, F) = \\mathbb{E}_{x\\sim p_{data}(x)}[||F(G(x)) - x||_1 + \\mathbb{E}_{y\\sim p_{data}(y)}[||G(F(y)) - y||_1$$\nFull Objective $$\\mathcal{L}(G, F, D_X, D_Y) = \\mathcal{L}_{GAN}(G, D_Y, X, Y) + \\mathcal{L}_{GAN}(F, D_X, Y, X) + \\lambda\\mathcal{L}_{cyc}(G, F)$$\n$$G^*,F^* = argmin_{G, F}argmax_{D_X, D_Y}\\mathcal{L}(G, F, D_X, D_Y)$$\n  image   Implementation Network Architecture  Generator에서 Instance Normalization 사용. PixelGAN이 아닌 70x70 PatchGAN 사용.  Training detail  Loss 에서 \\(\\lambda\\) 는 10. Optimizer: Adam Learning rate: 0.002  Result   image     image     image   P.S  Cycle consistency가 매우 신박 Image translation 에선 Instance Normalization !  ","permalink":"https://jjerry-test.github.io/blog/cycle_gan/","tags":["Paper"],"title":"Review: Cycle GAN"},{"categories":["DeepLearning"],"contents":"Image-to-Image Translation with Conditional Adversarial Networks Author: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros Date: Nov 26, 2018 URL: https://arxiv.org/abs/1611.07004\nGAN을 이용한 Image translation 의 시초에 가까운 Pix2Pix를 알아보려고 합니다.\n자세하게 리뷰하진 않을 겁니다.\nIntroduction  이 논문의 main contribution은 다양한 문제에서 Conditional GAN이 합리적인 결과를 생성한다는 것을 입증하는 것. 이를 위해 Image-to-Image translation 으로 연구 진행.    Method  정말 간단한 구조 다음 사진은 Edge(Sketch) 영상을 Photo 영상으로 만드는 예시.    Objective $$\\mathcal{L}{cGAN}(G, D) = \\mathbb{E}{x, y}[\\log D(x, y)] + \\mathbb{E}_{x, z}[\\log (1-D(x, G(x, z))]$$\n$$\\mathcal{L}{L1}(G) = \\mathbb{E}{x, y, z}[||y-G(x,z)||_1]$$\n$$G^* = argmin_Gmax_D\\mathcal{L}_{cGAN}(G, D) + \\lambda\\mathcal{L}_{L1}(G)$$\nNetwork architectures  DCGAN 에서 제안한 방법으로 각 block을 구성.  Convolution → BatchNorm → ReLU    Generator with skips  U-Net 과 비슷하나 Concatenate를 Add로 변경.    Markovian discriminator (PatchGAN)  L1, L2 만 사용하면 blurry result가 만들어진다는 것은 많이 알려진 사항. Low frequency 부분에 대한 부분은 L1 loss로 High frequency는 GAN loss 가 담당. 좀 더 다양한 High frequency 에 적합하도록 하기 위해 local image patch 에 대해 discriminator를 적용. (PatchGAN) NxN patch 에 대해 각각 real/fake를 판별.    Optimization and Inference  D → G 와 같은 순서로 학습 진행 Optimizer: Adam  Learning rate: 0.0002 Momentum $\\beta_1$: 0.5 Momentum $\\beta_2$: 0.999   Batch size: 1~10  Experimants  Semantic labels ↔photo Architectural labels→photo Map↔aerial photo BW→color photos Edges→photo Sketch→photo Day→night Thermal→color photos Photo with missing pixels→*inpainted photo*  Evaluation metrics  Amazon Mechanical Turk (AMT) FCN-score  Analysis of the objective function     Analysis of the generator architecture  U-Net 기반의 구조로 했을 때가 훨씬 좋음.      From PixelGANs to PatchGANs to ImageGANs  Discriminator의 출력을 1x1, 16x16, 70x70, 286x286과 같이 차례로 키우면서 실험.      Perceptual validation  사람이 보기에도 L1 + cGAN이 좋음.     Colorization에서는 좀 떨어짐.    Semantic segmentation  해당 task 에서는 오히려 L1과 같은 reconstruction loss만을 이용한 구조가 적합해 보임.      Community-driven Research  Twitter 에 공개한 후 다른 연구자들의 실험.      P.S  장마철\u0026hellip; 물 조심하세요..  ","permalink":"https://jjerry-test.github.io/blog/pix2pix/","tags":["Paper"],"title":"Review: Pix2Pix"},{"categories":["DeepLearning"],"contents":"오늘은 neptune.ai 라는 툴을 소개시켜 드리려 합니다.\nJupyter는 아는데\u0026hellip;. Neptune은 또 뭐여\u0026hellip;. 암튼 이 분야는 참.. 태양계를 좋아하는 듯합니다.\n이는 NN 실험을 편하고 효율적으로 할 수 있게 도와주는 툴입니다.\n각 실험 세팅 별로 Hyper parameter, Metrics를 기록하고 시각화하여 비교할 수 있습니다.\n그리고 개인에겐 100GB의 저장소를 제공하기 때문에 타인과 공유도 가능해요!\n어찌보면 Weights \u0026amp; Biases 랑 비슷하죠.\n다음은 Neptune docs 에서 가져온 Neptune의 특징입니다.\n data exploration and analysis → decision science → machine learning and deep learning 와 같은 과정을 수행하는데 적합. Python, Jupyter Notebook, R 에서 동작. Keras, PyTorch Lightning, XGBoost, Matplotlib 와 같은 ML, DL에 사용되는 Python 라이브러리를 연동(통합). MLflow, TensorBoard , Sacred 와 같은 Tracking tool 과 연동(통합) 가능. AWS, GCP, Kubernetes, Azure 와 같은 Cloud 와도 원활히 작동.  그럼 실제로 한번 써보도록 하겠습니다.\n Sign up 은 넘어갑니다.\n 1. 기본 환경 구축 기존 환경에서 쓰실 분들은 쓰셔도 됩니다.\n저는 따로 환경을 만들어서 테스트를 했습니다.\nconda create -n neptune python=3.7 conda activate neptune conda install psutil matplotlib tensorflow-gpu # 혹은 tensorflow 2. Neptune 설정 Neptune을 설치합니다.\nconda를 쓰시는 분들께 보통 conda install 로 설치하라고 말씀을 드리지만.. 이번엔 특별히 pip install로 설치를 추천드립니다.\n  설치 후 자신이 사용하는 OS에 맞춰서 API token을 PC에 기입(?) 해주세요.\n  여기까지 하면 세팅은 끝납니다. 그 이후에 Run script, Result in UI 부분은 하고 싶으신 분만 해보세요!\n3. Mnist 를 이용한 실험 다음과 같은 코드를 작성합니다.\nimport hashlib import os import tempfile import matplotlib.pyplot as plt import neptune import numpy as np import tensorflow as tf from tensorflow.keras import models, layers, optimizers, callbacks, datasets # For Efficiency gpus = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(\u0026#39;GPU\u0026#39;) print(len(gpus), \u0026#34;Physical GPUs,\u0026#34;, len(logical_gpus), \u0026#34;Logical GPUs\u0026#34;) except RuntimeError as e: print(e) # select project neptune.init(\u0026#39;jjerry-k/mnist\u0026#39;) # Define parameters PARAMS = {\u0026#39;batch_size\u0026#39;: 64, \u0026#39;n_epochs\u0026#39;: 100, \u0026#39;shuffle\u0026#39;: True, \u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;early_stopping\u0026#39;: 10, \u0026#39;optimizer\u0026#39;: \u0026#39;Adam\u0026#39;, } # Create experiment neptune.create_experiment(name=\u0026#39;classification_example\u0026#39;, tags=[\u0026#39;classification\u0026#39;, \u0026#39;MNIST\u0026#39;], params=PARAMS) # Dataset mnist = datasets.mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images = train_images / 255.0 test_images = test_images / 255.0 class_names = [str(i) for i in range(10)] neptune.set_property(\u0026#39;class_names\u0026#39;, class_names) for j, class_name in enumerate(class_names): plt.figure(figsize=(10, 10)) label_ = np.where(train_labels == j) for i in range(9): plt.subplot(3, 3, i + 1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[label_[0][i]], cmap=plt.cm.binary) plt.xlabel(class_names[j]) neptune.log_image(\u0026#39;example_images\u0026#39;, plt.gcf()) def lr_scheduler(epoch): if epoch \u0026lt; 10: new_lr = PARAMS[\u0026#39;learning_rate\u0026#39;] else: new_lr = PARAMS[\u0026#39;learning_rate\u0026#39;] * np.exp(0.1 * ((epoch//50)*50 - epoch)) neptune.log_metric(\u0026#39;learning_rate\u0026#39;, new_lr) return new_lr # Model model = models.Sequential([ layers.Flatten(input_shape=(28, 28)), layers.Dense(128, activation=\u0026#39;relu\u0026#39;), layers.Dense(len(class_names), activation=\u0026#39;softmax\u0026#39;) ]) if PARAMS[\u0026#39;optimizer\u0026#39;] == \u0026#39;Adam\u0026#39;: optimizer = optimizers.Adam( learning_rate=PARAMS[\u0026#39;learning_rate\u0026#39;], ) elif PARAMS[\u0026#39;optimizer\u0026#39;] == \u0026#39;Nadam\u0026#39;: optimizer = optimizers.Nadam( learning_rate=PARAMS[\u0026#39;learning_rate\u0026#39;], ) elif PARAMS[\u0026#39;optimizer\u0026#39;] == \u0026#39;SGD\u0026#39;: optimizer = optimizers.SGD( learning_rate=PARAMS[\u0026#39;learning_rate\u0026#39;], ) model.compile(optimizer=optimizer, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) # Log model summary model.summary(print_fn=lambda x: neptune.log_text(\u0026#39;model_summary\u0026#39;, x)) def log_train_data(logs): neptune.log_metric(\u0026#39;epoch_acc\u0026#39;, logs[\u0026#39;accuracy\u0026#39;]) neptune.log_metric(\u0026#39;epoch_loss\u0026#39;, logs[\u0026#39;loss\u0026#39;]) def log_val_data(logs): # Evaluate model eval_metrics = model.evaluate(test_images, test_labels, verbose=0) for j, metric in enumerate(eval_metrics): neptune.log_metric(\u0026#39;eval_\u0026#39; + model.metrics_names[j], metric) # Training Network model.fit(train_images, train_labels, batch_size=PARAMS[\u0026#39;batch_size\u0026#39;], epochs=PARAMS[\u0026#39;n_epochs\u0026#39;], shuffle=PARAMS[\u0026#39;shuffle\u0026#39;], validation_data=[test_images, test_labels] callbacks=[callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_train_data(logs)), callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_valdata(logs)), callbacks.EarlyStopping(patience=PARAMS[\u0026#39;early_stopping\u0026#39;], monitor=\u0026#39;accuracy\u0026#39;, restore_best_weights=True), callbacks.LearningRateScheduler(lr_scheduler)] ) # Log model weights with tempfile.TemporaryDirectory(dir=\u0026#39;.\u0026#39;) as d: prefix = os.path.join(d, \u0026#39;model_weights\u0026#39;) model.save_weights(os.path.join(prefix, \u0026#39;model\u0026#39;)) for item in os.listdir(prefix): neptune.log_artifact(os.path.join(prefix, item), os.path.join(\u0026#39;model_weights\u0026#39;, item)) 4. 결과 확인 neptune에는 총 7개의 탭이 있습니다.\n각각 예시를 보시면 역할을 이해하실 수 있습니다.\nCharts 여기에는 neptune.log_metric()을 사용한 변수들이 그래프로 기록됩니다.\n  Log Charts에 기록된 변수의 값이 기록되고 추가적으로 neptune.log_image(), neptune.log_text() 를 사용한 변수 또한 기록됩니다.\n  Monitoring 리소스 모니터링과 standard error, standard output을 볼 수 있어요.\n  Artifacts neptune에 원하는 파일을 전송하여 저장할 수 있습니다.\n  Source code 생략\n  Parameters 코드 상단에 적은 PARAMS 가 기록이 됩니다.\n  Detail 뭐\u0026hellip;실험의 정보, 시간 등 기록이 되고 추가적으로 neptune.set_property를 이용하여 Properties 에 변수를 추가할 수 있어요!\n  wandb와 비슷한 역할을 하는 neptune.ai라는 tool에 대해 다루었습니다.\n음\u0026hellip;.솔직히 wandb가 더 편한 듯 하네요. (코드가 짧아서)\n뭐 좀 더 다양하게 사용하면 이게 더 나을\u0026hellip;수도?\n선택은 개인의 몫입니다.\nP.S.  점점\u0026hellip;포스팅 주제 고갈\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/neptune/","tags":["Tools"],"title":"Neptune...? 은 또 뭐여.."},{"categories":["DeepLearning"],"contents":"오늘은 netron 이라는 Tool를 드리려 합니다.\n 본 포스팅은 https://github.com/lutzroeder/netron 내용을 이용하여 작성하였습니다.\n 간단히 말씀드리면 Neural network viewer 입니다.\n굉장히 많은 Framework들을 지원하고 있습니다.\n ONNX (.onnx, .pb, .pbtxt) Keras (.h5, .keras) Core ML (.mlmodel) Caffe (.caffemodel, .prototxt) Caffe2 (predict_net.pb) Darknet (.cfg) MXNet (.model, -symbol.json) Barracuda (.nn) ncnn (.param) Tengine (.tmfile) TNN (.tnnproto) UFF (.uff) TensorFlow Lite (.tflite)  이 외에도 불안정하지만 다음과 같은 Framework 도 지원합니다.\n TorchScript (.pt, .pth) PyTorch (.pt, .pth) Torch (.t7) Arm NN (.armnn) BigDL (.bigdl, .model) Chainer (.npz, .h5) CNTK (.model, .cntk) Deeplearning4j (.zip) MediaPipe (.pbtxt) ML.NET (.zip) MNN (.mnn) PaddlePaddle (.zip, __model__) OpenVINO (.xml) scikit-learn (.pkl) TensorFlow.js (model.json, .pb) TensorFlow (.pb, .meta, .pbtxt, .ckpt, .index)  (솔직히 Torch, TensorFlow 종류는 왜 나눠서 설명하는지 잘 모르겠음\u0026hellip;..하나로 합쳐 놓으면 안되나..)\n설치 및 사용법 netron은 PC에 설치해서 사용하거나 설치없이 Browser 버전으로 사용할 수도 있습니다.\n macOS: Download the .dmg file or run brew cask install netron Linux: Download the .AppImage file or run snap install netron Windows: Download the .exe installer or run winget install netron Python Server: Run pip install netron and netron [FILE] or import netron; netron.start('[FILE]') Browser: Start the browser version.  한번 브라우저 버전을 실행시켜 보았습니다.\n  Open Model 을 클릭 하신 후 보고 싶은 Network 저장 파일을 선택합니다.\n  Network 구조가 차례로 보이네요.\n  다음은 keras로 작성된 MobileNetV2 를 netron으로 띄운 후 앞 부분을 잘라낸 사진입니다.\n  왼쪽에는 네트워크 구조를 볼 수 있고 만약 레이어를 선택하면 오른쪽에 Node(Layer)에 대한 세부 설정 값들을 확인할 수 있습니다.\n몇몇 프레임워크 별로 sample을 이렇게 제공해줍니다!\nopen을 누르면 browser를 이용하여 볼 수 있네요!\n ONNX: squeezenet [open] CoreML: exermote [open] Darknet: yolo [open] Keras: mobilenet [open] MXNet: inception_v3 [open] TensorFlow: chessbot [open] TensorFlow Lite: hair_segmentation [open] TorchScript: traced_online_pred_layer [open] Caffe: mobilenet_v2 [open]  P.S.  피곤\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/netron/","tags":["Tools"],"title":"Netron! Network 구조를 보여주세요!"},{"categories":["DeepLearning"],"contents":"지금까지 Docker(이하 도커) 세팅 까지 포스팅을 했었습니다.\nDocker 까진 좋은데\u0026hellip;연결을 항상 Terminal 혹은 CMD를 켜서 ssh로 해야하나..? 생각이 듭니다.\n그래서 이래저래 찾아봤습니다.\nDocker 공식 문서를 보니 Dockerize an SSH service 이런 글이 있더군요.\n이 글을 참고하여 image(이하 이미지)를 만들어 보기로 했습니다.\n그럼 빠르게 빠르게 진행하겠습니다.\n1. Dockerfile 생성 및 build FROMnvidia/cuda:10.1-cudnn7-runtime-ubuntu16.04LABEL maintainer \u0026#34;Jerry Kim \u0026lt;jaeyeol2931@gmail.com\u0026gt;\u0026#34;ARG PYTHON_VERSION=3.7RUN apt-get updateRUN apt-get install -y \\  build-essential \\  cmake \\  git \\  curl \\  wget \\  vim \\  unzip \\  ca-certificates \\  libjpeg-dev \\  libpng-dev \\  openssh-serverRUN apt-get update \u0026amp;\u0026amp; apt-get -y upgrade#RUN mkdir /var/run/sshdRUN echo \u0026#39;root:{개인 비밀번호}\u0026#39; | chpasswdRUN sed -i \u0026#39;s/PermitRootLogin prohibit-password/PermitRootLogin yes/\u0026#39; /etc/ssh/sshd_config# SSH login fix. Otherwise user is kicked off after loginRUN sed \u0026#39;s@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g\u0026#39; -i /etc/pam.d/sshdENV NOTVISIBLE \u0026#34;in users profile\u0026#34;RUN echo \u0026#34;export VISIBLE=now\u0026#34; \u0026gt;\u0026gt; /etc/profileEXPOSE22CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;]RUN rm -rf /var/lib/apt/lists/*RUN curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o ~/miniconda.shRUN chmod +x ~/miniconda.sh \u0026amp;\u0026amp; \\  ~/miniconda.sh -b -p /opt/conda \u0026amp;\u0026amp; \\  rm ~/miniconda.shRUN /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include ninja cython typing opencv matplotlib tqdm \u0026amp;\u0026amp; \\  /opt/conda/bin/conda install -y jupyter jupyterlab seaborn pillow pandas pylint scikit-learn scikit-image tensorflow-gpu \u0026amp;\u0026amp; \\  /opt/conda/bin/conda update -y --all \u0026amp;\u0026amp; \\  /opt/conda/bin/conda clean -ya중간에 RUN echo 'root:{개인 비밀번호}' | chpasswd 에서 ssh 접속시 사용할 비밀번호를 적어주세요!\n# Example RUN echo \u0026#39;root:test\u0026#39; | chpasswd 위 내용들을 vim이나 nano같은 에디터를 이용해서 Dockerfile을 만들어주세요!\n  그리고 다음과 같이 커맨드를 입력하여 도커 이미지를 build 합니다.\ndocker build -t {이미지 이름} .   뭐 이런 저런 Log 들이 촤라———라락 넘어갈겁니다. 계속 기다려 주세요\u0026hellip;.\ndocker images 를 입력하면 제대로 생성된 것을 볼 수 있습니다!\n  2. Container 실행 이미지를 실행하여 container(이하 컨테이너) 만들어 줍니다.\n세부 옵션은 구.글.링 아시죠?\ndocker run -d -P -v /home/ubuntu/jerry/:/jerry --name {컨테이너 이름} {이미지 이름} docker port {컨테이너 이름} 22   컨테이너를 만들고 해당 컨테이너의 22번 포트 (ssh 포트)가 nipa의 몇번 포트와 연결되어 있는지 출력해줍니다.\n3. VSCode를 이용하여 접속  vscode 에 Remote - SSH 플러그인이 설치가 되어 있다는 전제 하에 진행합니다.\n 새로운 호스트를 추가해줍니다.\n  ssh root@{NIPA IP} -p {포트 번호} 라고 입력하고 엔터!\n  그럼 우하단에 Host를 추가했습니다! 와 같이 알림이 뜹니다.\n추가를 했으니 이제 Host에 연결을 해봅니다.\n비밀번호는 맨 처음에 Dockerfile에서 썼던 비밀번호 입니다!\n  아마 처음에는 세팅이 오래 걸릴겁니다. 우하단에 알림이 없어질 때까지 기다려주세요 !\n기초 세팅이 다 완료 되면 컨테이너의 vscode-server에 플러그인을 설치해야 합니다.\n저는 아래 4개 플러그인을 설치했어요.\n  그 후에 테스트 코드를 돌려봅니다.\n  정상적으로 잘 작동하네요!\n후\u0026hellip;이제 Vim으로 코딩을 안할 수 있습니다.\n이번 포스팅은 정말 환경 구축이 처음이신 분들께는 불친절한 포스팅일 수도 있습니다.\n죄송합니다. 일단 제 일기처럼 쓰는 포스팅이라서요. 감안해서 읽어주세요.\nP.S.  근데 막상 내가 NIPA를 \u0026hellip;..주로 쓰지 않음\u0026hellip; 나에게 NIPA는 환경 구축 포스팅을 적는 용도\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/nipa_vscode/","tags":["Tools"],"title":"NIPA x VScode !"},{"categories":["DeepLearning"],"contents":" 본 포스팅은 Colab on steroids: free GPU instances with SSH access and Visual Studio Code Server 내용을 (많이) 참고하여 작성하였습니다.\n 많은 분들이 Google Colaboratory (코랩) 를 사용하실겁니다. (무료니까\u0026hellip;)\n무료로 고성능(?)의 하드웨어를 쓸 수 있다는 건 참 좋은 것입니다!\n하지만\u0026hellip;문제는 Jupyter Notebook 형식으로 써야한다는거\u0026hellip;아 물론 편하신 분들도 있겠죠!?\n전 개인적으로 별로 안좋아합니다. (물론 애초에 안쓰기도 함.)\n근데 SSH, VSCode Server를 이용해서 Colab에 접속을\u0026hellip;하는 포스팅이 있더군요.\n갑자기 실험쥐 정신이 튀어나와서 진행을 해봤습니다.\n뻘소리 그만하고 간단 간단 설명 시작하겠습니다.\n1. ngrok으로 token 만들기 ngrok은 대충 로컬 웹 서버를 SSH 접속이나 모바일 테스트 할 수 있도록 공공 URL로 접근 가능토록 해주는 것입니다. 가격은 걱정하지 마세요. 1개는 무료거든요.\n  어쨌든 가입을 하고 token을 생성 해줍니다.\n    2. Colab에 ngrok 설치 및 실행 노트북 설정을 먼저 해주세요. (ex) CPU, GPU, TPU\n  그리고 셀에 다음과 같이 코드를 실행합니다.\n아래 authtoken 에는 ngrok의 Authtoken을 적어주세요. (당연하지만 스트링으로)\n# Install useful stuff ! apt install --yes ssh screen nano htop ranger git \u0026gt; /dev/null # SSH setting ! echo \u0026#34;root:carbonara\u0026#34; | chpasswd ! echo \u0026#34;PasswordAuthentication yes\u0026#34; \u0026gt; /etc/ssh/sshd_config ! echo \u0026#34;PermitUserEnvironment yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config ! echo \u0026#34;PermitRootLogin yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config ! service ssh restart \u0026gt; /dev/null # Download ngrok ! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ! unzip -qq -n ngrok-stable-linux-amd64.zip # Run ngrok authtoken = **\u0026#34;PUT_YOUR_TOKEN_HERE\u0026#34;** get_ipython().system_raw(\u0026#39;./ngrok authtoken $authtoken \u0026amp;\u0026amp; ./ngrok tcp 22 \u0026amp;\u0026#39;) ! sleep 3 # Get the address for SSH import requests from re import sub r = requests.get(\u0026#39;http://localhost:4040/api/tunnels\u0026#39;) str_ssh = r.json()[\u0026#39;tunnels\u0026#39;][0][\u0026#39;public_url\u0026#39;] str_ssh = sub(\u0026#34;tcp://\u0026#34;, \u0026#34;\u0026#34;, str_ssh) str_ssh = sub(\u0026#34;:\u0026#34;, \u0026#34; -p \u0026#34;, str_ssh) str_ssh = \u0026#34;ssh root@\u0026#34; + str_ssh print(str_ssh)   출력으로 나온 커맨드를 이용하여 한번 터미널에서 테스트를 해봅니다. 비밀번호는 carbonara 입니다.\n  3. VSCode에서 실행하기 먼저 Colab 화면에서 다음 코드를 실행해서 Google Drive를 마운트 해줍니다.\n# Mount Google Drive and make some folders for vscode from google.colab import drive drive.mount(\u0026#39;/googledrive\u0026#39;) ! mkdir -p /googledrive/My\\ Drive/colabdrive ! mkdir -p /googledrive/My\\ Drive/colabdrive/root/.local/share/code-server ! ln -s /googledrive/My\\ Drive/colabdrive / ! ln -s /googledrive/My\\ Drive/colabdrive/root/.local/share/code-server /root/.local/share/   VSCode를 켜고 ssh로 접속을 해봅니다.\n  접속해서 Colab Notebooks 디렉토리에 접근한 화면입니다. Google Drive랑 동일한 것을 확인할 수 있죠!\n  4. 코드 작성 및 실행 코드 실행 여부를 테스트 해보겠습니다.\n일단 Colab 에는 VSCode 플러그인이 설치 되어 있지 않기 때문에 python이나 개인적으로 필요한 플러그인들을 설치해줍니다.\n저는 테스트이니 아래 세 가지만 설치했습니다.\n  그럼 한번 실행해보죠.\n  오\u0026hellip;..신기\u0026hellip;\n이렇게 연결해서 사용이 가능합니다.\nhtop를 이용해서 리소스 모니터링도 가능하죠.\n  여기까지 Colab x VSCode 포스팅입니다. 뭐\u0026hellip;.어떤 면에선 편할 것 같지만 막\u0026hellip;편할 것 같진 않네요.\n아직 이 상황을 겪지 않아서 모르겠지만 원래 코랩의 큰 문제가 있죠. Session timeout\u0026hellip;.\n일정 시간동안 동작이 없으면 연결이 끊겨서 다시 실행을 시켜야하는\u0026hellip;.상황이 오죠.\n  나한테 왜 그래\u0026hellip;\n그 상황이 왔을 때 느낌으로는 VSCode의 SSH 접속이 끊길 것 같네요. 흠\u0026hellip;. 실험 후에 추가로 적어 보겠습니다.\nP.S 끊겼습니다.\n  VSCode 를 봐도..\n  그렇습니다. 제 생각대로 Colab의 Session time이 끝나면\u0026hellip;끊기네요..ㅎㅎ 편하게 쓰기는 힘들 듯\u0026hellip;\n","permalink":"https://jjerry-test.github.io/blog/colab_vscode/","tags":["Tools"],"title":"Colab x VSCode !"},{"categories":["DeepLearning"],"contents":" 원래 NIPA GPU 서버를 대여받은 후에 포트 포워딩을 먼저 해줘야 합니다. 하지만 그 부분에 대해선 보안적인 부분이 있기 때문에 생략하겠습니다.\n 이번엔 NIPA 내 개인 환경 세팅에 대해 포스팅을 해보려 합니다.\n개인마다 원하는 환경이 다르기 때문에 정말 필요하죠.\n물론 기본적으로 설치된 Anaconda 환경으로도 충분할 수 있지만 살짝쿵 문제가 있습니다.\n문제에 대해 살펴보겠습니다.\nNIPA GPU 서버에 접속을 하면 다음과 같은 화면이 출력됩니다.\n  먼저 말씀드렸던 conda 환경으로 기본적으로 다양한 환경이 제공되네요.\n저의 경우 이번에 tf-nightly가 필요했습니다.\n그래서 TensorFlow2, python3.6 환경을 activate 한 후 설치를 시도했죠.\n     what\u0026hellip;?         음\u0026hellip;.conda 버전의 문제인가 싶어서 base conda를 update 하려 했습니다.\n     what\u0026hellip;?         뭐야..이건 또 왜 안되는거야\u0026hellip; 짜증이 났습니다.\n대충 너무 옛날 버전의 conda니까 최소 4.8로 재설치 해주세요. 라는 내용입니다.\n하....이건 좀 너무한데...그냥 Docker나 설치하자.. 라는 생각을 하게 되었습니다.\n그럼 Docker 설치에 대해 포스팅 해보겠습니다.\nDocker 에 대한 자세한 설명은 하지 않을 겁니다.\n홈페이지 혹은 Docker 에 대한 포스팅을 참고하시기 바랍니다.\n간단히 말씀드리면 OS 단계까지 가상환경을 만드는 겁니다.\n그럼 설치 방법에 대해 적겠습니다.\n# 2020.10.10 추가 사항 conda uninstall curl # SET UP THE REPOSITORY sudo apt-get update sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; # INSTALL DOCKER ENGINE sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # VERIFY THAT DOCKER ENGINE IS INSTALLED CORRECTLY sudo docker run hello-world 만약에 제대로 설치 되었다면 마지막에 다음과 같은 출력이 남습니다\n  여기까지 하시면 기본 Docker 설치는 끝났습니다.\n하지만 이것만 설치하면 GPU 는 사용하지 못합니다.\nGPU를 쓰기 위해선 nvidia-docker를 설치를 해야 합니다.\nnvidia-docker는 간단히 말하면 docker 에서 데스크탑의 GPU를 사용할 수 있도록 nvidia에서 만든(?)것입니다.\n설치법은 다음과 같습니다.\n# Ubuntu 16.04/18.04/20.04, Debian Jessie/Stretch/Buster # Add the package repositories distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker # Test nvidia-docker sudo docker run --gpus all nvidia/cuda:10.0-base nvidia-smi 이 또한 설치가 제대로 되었다면 마지막에 다음과 같이 nvidia-smi 출력이 나올 겁니다.\n     하\u0026hellip;.편안\u0026hellip;.         이번엔 NIPA에 Docker 설치하는 과정을 포스팅 해봤습니다.\n공짜로 빌려주는 건 좋으나 환경 구축은 역시나\u0026hellip;.해야 하네요.\n제가 쓰는 Docker image는 개인적인 도커 파일 에 있으니 참고하세요!\nP.S  2020.10.10 내용 추가  sudo usermod -aG docker {유저명} 을 수행하시면 sudo 명령어가 필요 없습니다!    ","permalink":"https://jjerry-test.github.io/blog/nipa_docker/","tags":["Tools"],"title":"NIPA x Docker !"},{"categories":["DeepLearning"],"contents":"저번 포스팅에 이어 이번엔 NIPA 컴퓨팅 자원 이용 신청에 관한 포스팅을 해보려 합니다!\n내용 출처: http://www.aihub.or.kr/node/254\n이용 절차   일반 사용자\n 신청 대상: AI 제품·서비스를 연구·개발하고자 하는 국내 중소·벤처 기업, 스타트업, 공공기관, 대학교(원), 일반 협·단체 신청 기간: 2020. 1. 6.(월) ∼ 2. 7.(금)※ 추가 모집 : 2020.5.25.(월) ~ 11.30.(월) 신청 절차: 이용 신청서 작성 → 온라인 신청 사이트에 제출 → 서면 심사 → 선정결과 발표\n※ 선정기준 : 지원 자격요건, AI 연구‧개발대상 여부, 사용계획서(목적, 연구‧개발분야, 사용내용, 필요성 등)를 중심으로 심사‧평가\n※ 모집 규모 대비 초과 신청 시 중소·벤처기업, 공공기관, 대학교(원)에 자원이 우선 할당될 수 있음    수시 사용자\n 신청 대상: 일반 사용자와 개발자, 학생 등 개인 신청 기간: 2020.4.10.(금) ~ 2020.12.21.(월) 신청 절차: 이용신청 작성‧제출 → AI 연구‧개발대상 여부 확인 → 여유 자원 확인 → 서비스 이용    각각의 신청서 양식은 다음과 같습니다.\n일반 사용자용 신청서  다운로드 링크    수시 사용자용 신청서  다운로드 링크    당연한거지만 일반 사용자가 수시 사용자에 비해 작성해야할 것이 많습니다.\n신청서 작성 방법은 예시가 너무 잘 적혀있기 때문에 따로 설명을 드리지 않겠습니다. (궁금하신게 있다면 따로 댓글 남겨주세요!)\n신청서를 작성하신 후엔 AIHub 사이트 로그인을 하신 후 이용신청 으로 넘어갑니다.\n그럼 다음과 같은 화면이 나오는데 (수시 사용자 기준) 1~8번 중 해당하는 부분 선택하고 개인 정보 채워주시면 됩니다!\n  다 채운 후 이용신청을 누르시면 한\u0026hellip;. 하루, 이틀? 정도 후에 메일과 핸드폰으로 문자 안내가 옵니다!\n메일에는 NIPA 서버 사용방법, 문자에는 관리 페이지 이용 안내가 옵니다.\n신청이 어렵지 않으니 많이 많이 신청하시고 즐거운 딥러닝 공부, 연구하세요! (홍보 아님)\n  P.S.  중요! ! ! 수시 사용자는 계속 사용하려면 10일마다 연장 신청 해야함. 꼭! 반드시! 위 내용을 지키지 않으면 서버 초기화되서 내용들 다 사라짐!!! 너무 대충 포스팅을\u0026hellip;하는걸까\u0026hellip; 2020.10.10 내용 추가  서식 링크 변경    ","permalink":"https://jjerry-test.github.io/blog/nipa_apply/","tags":["Tools"],"title":"NIPA 컴퓨팅 자원 신청 방법 !"},{"categories":["DeepLearning"],"contents":"요즘 커뮤니티(V-AIS) 에선 NIPA에 대한 얘기가 많이 나옵니다.\nNIPA는 정보통신산업진흥원 의 약자인데 근데 그래서 이게 왜..? 라는 의문을 가지실 겁니다.\n지금 AI Hub에서 지원하는 AI 컴퓨팅 자원 지원 사업을 하고 있는데요.\n그 사업에서 컴퓨팅 자원을 관리하는게 NIPA 입니다. 그래서 NIPA, NIPA 하죠..\n간단하게 그림으로 보여드리면 이렇습니다.\n  더 간단하게 설명드리면 GPU서버 지원해드림 ㅇㅇ 이겁니다.\n어느 정도 사양이냐\u0026hellip;\n  일반 사용자와 수시 사용자로 나뉘는데요. 그 차이는 사이트에서 확인하시길..\n이렇게 보면 GPU로 뭐 쓰는지 잘 모르시겠죠. 그래서 가져왔습니다.\n제가 포스팅, 환경구축과 같은 자료를 만들기 위해 신청한 서버입니다.\n  위 GPU 사진은 수시 사용자 기준입니다. 일반 사용자면 V100 두개겠네요.\n뭐 이런 서버를 공짜로 제공을 해줍니다.\n집에 서버가 따로 없는 분들이라면 이보다 좋은 건 없겠죠. (게다가 VRAM이 32기가임\u0026hellip;.)\n   헤헿 겁나 좋군         다음엔 NIPA 신청 방법에 대한 포스팅은 간단히 해보겠습니다.\n그럼 모두 즐거운 딥러닝 공부, 연구하세요.\nP.S.  수시 사용자의 경우 기본 10일 사용, 10일마다 사용 연장신청을 해야함. 당연하지만 신청해놓고 사용안하면 검열 후 강제 반납.  ","permalink":"https://jjerry-test.github.io/blog/nipa_intro/","tags":["Tools"],"title":"NIPA...NIPA가 뭐죠.."},{"categories":["DeepLearning"],"contents":"이번 포스팅은 Multi GPU 시스템에서 Google의 머신러닝 오픈 소스 플랫폼인 TensorFlow사용법에 관한 것입니다!\n거두절미하고 바로 코딩으로 들어가겠습니다!\nSingle GPU 예시  다음 코드는 Single GPU를 이용하여 mnist data를 분류하는 코드입니다.  # Import Package import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models, losses, optimizers, datasets, utils # Data Prepare (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.expand_dims(train_x/255., -1), np.expand_dims(test_x/255., -1) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # Build Network cnn = models.Sequential() cnn.add(layers.Conv2D(16, 3, activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1,))) cnn.add(layers.MaxPool2D()) cnn.add(layers.Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;)) cnn.add(layers.MaxPool2D()) cnn.add(layers.Flatten()) cnn.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) cnn.compile(optimizer=optimizers.Adam(), loss=losses.sparse_categorical_crossentropy, metrics=[\u0026#39;accuracy\u0026#39;]) print(\u0026#34;Network Built!\u0026#34;) # Training Network epochs=10 batch_size = 4096 history = cnn.fit(train_x, train_y, epochs=10, batch_size=batch_size, validation_data=(test_x, test_y)) Multi GPU 예시  다음 코드는 Multi GPU를 이용한 코드입니다.  # Import Package import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models, losses, optimizers, datasets, utils # Data Prepare (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.expand_dims(train_x/255., -1), np.expand_dims(test_x/255., -1) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # Build Network strategy = tf.distribute.MirroredStrategy() with strategy.scope(): cnn = models.Sequential() cnn.add(layers.Conv2D(16, 3, activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1,))) cnn.add(layers.MaxPool2D()) cnn.add(layers.Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;)) cnn.add(layers.MaxPool2D()) cnn.add(layers.Flatten()) cnn.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) cnn.compile(optimizer=optimizers.Adam(), loss=losses.sparse_categorical_crossentropy, metrics=[\u0026#39;accuracy\u0026#39;]) print(\u0026#34;Network Built!\u0026#34;) # Training Network epochs=10 batch_size_each_gpu = 4096 batch_size = batch_size_each_gpu*len(gpus) history = cnn.fit(train_x, train_y, epochs=10, batch_size=batch_size, validation_data=(test_x, test_y)) 어렵지 않습니다. Build Network 주석 부분과 Training Network 부분에 batch_size만 조금 수정해주시면 끝납니다!\n  하지만 이렇게 하면 무식하게 GPU의 모든 메모리를 할당합니다.\n그렇기 떄문에 다음과 같이 코드를 추가하여 필요한 만큼 할당하도록 합니다.\n필요한 만큼의 GPU 메모리만 사용하기 # Import Package import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models, losses, optimizers, datasets, utils # Data Prepare (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.expand_dims(train_x/255., -1), np.expand_dims(test_x/255., -1) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # For Efficiency gpus = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(\u0026#39;GPU\u0026#39;) print(len(gpus), \u0026#34;Physical GPUs,\u0026#34;, len(logical_gpus), \u0026#34;Logical GPUs\u0026#34;) except RuntimeError as e: print(e) # Build Network strategy = tf.distribute.MirroredStrategy() with strategy.scope(): cnn = models.Sequential() cnn.add(layers.Conv2D(16, 3, activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1,))) cnn.add(layers.MaxPool2D()) cnn.add(layers.Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;)) cnn.add(layers.MaxPool2D()) cnn.add(layers.Flatten()) cnn.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) cnn.compile(optimizer=optimizers.Adam(), loss=losses.sparse_categorical_crossentropy, metrics=[\u0026#39;accuracy\u0026#39;]) print(\u0026#34;Network Built!\u0026#34;) # Training Network epochs=10 batch_size_each_gpu = 4096 batch_size = batch_size_each_gpu*len(gpus) history = cnn.fit(train_x, train_y, epochs=10, batch_size=batch_size, validation_data=(test_x, test_y)) 기존 Multi GPU 코드와 달라진 점은\n For Efficiency라는 부분이 추가. strategy = tf.distribute.MirroredStrategy()을 Build Network에서 For Efficiency의 가장 첫번째 라인으로 이동.  이렇게 변경 후 실행 후 nvidia-smi와 같은 모니터링 툴을 확인해보시면 이전과는 다르게 GPU 메모리를 필요한 만큼만 사용하는걸 보실 수 있습니다!\nP.S  다음은 뭘로 포스팅하지\u0026hellip;  번외편 (Using gradient tape) # %% # Import Package import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models, losses, optimizers, datasets, utils gpus = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(\u0026#39;GPU\u0026#39;) print(len(gpus), \u0026#34;Physical GPUs,\u0026#34;, len(logical_gpus), \u0026#34;Logical GPUs\u0026#34;) except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) # %% # Data Prepare epochs=10 batch_size_each_gpu = 4096 batch_size = batch_size_each_gpu*len(gpus) (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.expand_dims(train_x/255., -1), np.expand_dims(test_x/255., -1) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # %% # Build Network class build_model(models.Model): def __init__(self): super(build_model, self).__init__() self.conv1 = layers.Conv2D(16, 3, activation=\u0026#39;relu\u0026#39;) self.pool1 = layers.MaxPool2D() self.conv2 = layers.Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;) self.pool2 = layers.MaxPool2D() self.flatten = layers.Flatten() self.dense = layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) def call(self, x): x = self.conv1(x) x = self.pool1(x) x = self.conv2(x) x = self.pool2(x) x = self.flatten(x) return self.dense(x) print(\u0026#34;Network Built!\u0026#34;) # Set mirrored Strategy strategy = tf.distribute.MirroredStrategy() with strategy.scope(): # Prepare dataset  train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(len(train_x)).batch(batch_size) train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset) test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size) test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset) # Make Network cnn = build_model() # Set Loss \u0026amp; Metric function loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE) def compute_loss(labels, predictions): per_example_loss = loss_object(labels, predictions) return tf.nn.compute_average_loss(per_example_loss, global_batch_size=batch_size) test_loss = tf.keras.metrics.Mean(name=\u0026#39;test_loss\u0026#39;) train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u0026#39;train_accuracy\u0026#39;) test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u0026#39;test_accuracy\u0026#39;) # Set optimizer optimizer = tf.keras.optimizers.Adam() # Define taining, test function def train_step(inputs): images, labels = inputs with tf.GradientTape() as tape: predictions = cnn(images, training=True) loss = compute_loss(labels, predictions) gradients = tape.gradient(loss, cnn.trainable_variables) optimizer.apply_gradients(zip(gradients, cnn.trainable_variables)) train_accuracy.update_state(labels, predictions) return loss def test_step(inputs): images, labels = inputs predictions = cnn(images, training=False) t_loss = loss_object(labels, predictions) test_loss.update_state(t_loss) test_accuracy.update_state(labels, predictions) # Define training, test function suitable for Mirrored Strategy  @tf.function def distributed_train_step(dataset_inputs): per_replica_losses = strategy.experimental_run_v2(train_step, args=(dataset_inputs,)) return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) @tf.function def distributed_test_step(dataset_inputs): return strategy.experimental_run_v2(test_step, args=(dataset_inputs,)) # Train Network for epoch in range(epochs): # Training Loop total_loss = 0.0 num_batches = 0 for x in train_dist_dataset: total_loss += distributed_train_step(x) num_batches += 1 train_loss = total_loss / num_batches # Test Loop for x in test_dist_dataset: distributed_test_step(x) template = (\u0026#34;에포크 {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}\u0026#34;) print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100)) test_loss.reset_states() train_accuracy.reset_states() test_accuracy.reset_states() ","permalink":"https://jjerry-test.github.io/blog/tf2_multi_gpu/","tags":["TensorFlow"],"title":"TensorFlow Multi GPU 사용법"},{"categories":["DeepLearning"],"contents":"End-to-End Object Detection with Transformers Author: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko Date: May 27, 2020 URL: https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers\nIntroduction  현재 Object detection model들은 Input부터 Output (bounding bos, category label) 까지 Direct 하지 못함. Post processing 이 영향을 끼치기 때문에\u0026hellip; 본 논문에선 Direct prediction approach 제안. 이전에도 몇몇 실험이 있었으나 그 당시에는 prior knowledge를 준다거나 성능이 별로 좋지 못했음. Transformer 를 사용. 새로운 Loss function 도입.    Related Works Set Prediction  현재까지 Direct로 set(box, class)을 prediction 하는 방법이 없음. Post processing 이 없는 model 제안. 이를 위해 Hungarian algorithm 기반의 loss 설계.  Transformers and Parallel Decoding  다른 RNN 계열보다 Long sequence 에 적합한 model auto-regressive model  Object detection Set-based loss  기존에 bipatite matching loss 를 사용했지만 NMS 를 사용해야 성능이 향상되었음. 그 후 Learnable NMS 를 사용한 방법이 제시 되었으나 hand-crafted context feature 를 사용 하기에 효율적이지 못함.  Recurrent detectors  이름에서 알 수 있듯 RNN 계열을 도입한 Object detection 기존 방법에선 Small dataset을 이용했고 RNN 계열을 이용했기에 parallel 구조를 가져가지 못했음.  The DETR model  DETR은 크게 두 개의 장점이 있음. → a set prediction loss, a architecture    Object detection set prediction loss $$\\hat{\\sigma} = {argmin}_{\\sigma \\in \\mathfrak{S}_N} \\sum^N_i \\mathcal{L}_{match}(y_i, \\hat{y}_{\\sigma(i)})$$\n$$\\mathcal{L}_{match}(y_i, \\hat{y}_{\\sigma(i)}) = -1_{c_i \\neq \\phi}\\hat{p}_{\\sigma(i)}(c_i) +1_{c_i \\neq \\phi}\\mathcal{L}_{box}(b_i, \\hat{b}_{\\sigma(i)})$$\n$$\\mathcal{L}_{Hungarian}(y, \\hat{y}) = \\sum^N_i[-\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i) +1_{c_i \\neq \\phi}\\mathcal{L}_{box}(b_i, \\hat{b}_{\\hat{\\sigma}(i)})]$$\nBounding box loss $$\\lambda_{iou}\\mathcal{L}_{iou}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{\\mathrm{L}1}|b_i - \\hat{b}_{\\sigma(i)}|_1$$\nDETR architecture Backbone  일반적인 Backbone 사용. 마지막 feataure map은 원본 사이즈 H, W 에 비해 32분의 1 downsampling, C는 2048  Transformer encoder  Attention is all you need의 Transformer의 encoder와 동일한 구조. Fixed positional encodings 으로 인하여 permutation-invariant 한 구조!  Transformer decoder  Attention is all you need의 Transformer의 decoder와 동일한 구조. 기존 Transformer와 차이는  Prediction feed-forward networks (FFNs)  ReLU를 사용하는 d dimension의 linear layer 3 개 사용. 한 branch 에서는 Normalized center coordinate, height, width 를 예측. 다른 하나의 branch는 class label을 softmax를 이용하여 예측. DETR은 항상 N개의 box에 대해 예측. 하지만 실제 object 수가 적을때는 나머지 box들을 no object 로 처리.  Auxiliary decoding losses  Transformer decoder에 Auxiliary loss 를 추가.  Experiment Comparison with Faster R-CNN   Ablations     Panoptic segmentation   P.S  내용 보충 예정. 신박한 컨셉.  ","permalink":"https://jjerry-test.github.io/blog/detr/","tags":["Paper"],"title":"Review: DETR"},{"categories":["Living"],"contents":"D-Link의 DIR-815L 을 잘 쓰고 있었습니다\u0026hellip;\n라즈베리파이와 연결하여 DDNS 도해놓고..\n근데 어느 날 다음과 같은 글이 메일, 공지로 올라오더군요.\n안녕하세요. D-Link Korea 입니다. 저희 dlink 제품군에서 제공되었던 무료 dlinkddns 서비스 종료에 관한 안내 말씀드립니다. 죄송합니다만, dlinkddns 무료 서비스는 기존에 오라클사의 dyn ddns 와의 계약만료일인 2020년 7월 2일 종료될 예정입니다. 이후에는 정상적인 무료 ddns서비스 이용이 불가하니 계속해서 dlinkddns 서비스를 이용하기를 원하실 경우에는 유료상품으로 전환하시기 바랍니다. ■ 유료전환 http://dlinkddns.com 사이트 접속시 유료전환은 안내 팝업과 함께 결재시 50% 할인 메뉴 클릭하여 진행합니다. 이외에 관련해서 궁금한 점이 있으신 분은 D-Link Korea 고객센터(1899-3540)로 문의하시기 바랍니다. 기존 무료로 지원되던 dlinkddns 서비스 종료에, 앞으로 더욱 나은 서비스로 보답하겠습니다. 감사합니다. What the\u0026hellip;?\n후\u0026hellip; 공유기를 새로 사던 DDNS 서비스를 이용하던 해야겠네요.\n디자인이랑 DDNS 때문에 샀는데.. 젠장.. 앞으론 그냥 Iptime 쓸래요.\n","permalink":"https://jjerry-test.github.io/blog/dlink_ddns/","tags":["Hardware"],"title":"D-Link ddns 서비스 종료"},{"categories":["DeepLearning"],"contents":"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design Author: Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun\nDate: Jul 30, 2018\nURL: https://arxiv.org/abs/1807.11164\nIntroduction  CNN 계열이 AlexNet 부터 정확도, 속도가 많이 발전하고 있음. 실제로는 제한된 컴퓨팅 파워에서(Mobile과 같은) 최고의 성능을 내는데 목표로 하고 있음. 이로 인해 Xception, MobileNet, ShuffleNet 등이 개발 되었음. 지금까지는 모델의 연산량을 이용하여 모델의 효율성을 판단하였으나 적합한 지표가 아님을 주장. FLOPs와 speed 간의 성능 비교가 옳지 않은 주요 이유가 두 가지.  memory access cost(MAC): 메모리 접근량(사용량) depending on the platform      Practical Guidelines for Ecient Network Design  본 연구는 GPU 하드웨어(1080ti), ARM 하드웨어(Snapdragon 810) 이 두 가지 환경에서 실험. 모델의 Runtime을 쪼개보면 다음과 같은 차트가 그려짐. FLOPs는 Convolution 에 대해 설명하기 떄문에 비교 지표로 적절하지 못함을 강조.     위 문제를 근거로 다음과 같이 여러 개의 가이드 라인을 제시.  G1) Equal channel width minimizes memory access cost (MAC)  근래에 많이 사용되는 depthwise separable convoltuion에서 연산량의 대부분은 pointwise convolution 이 차지. 1x1 convolution 이 차지하는 연산량은 다음과 같음.  $$h, w: \\text{the spatial size of the input feature map} $$ $$c_1, c_2: \\text{Number of channels about input and output }$$ $$B=hwc_1c_2, \\text{ FLOPs of the }1 \\times 1 \\text{ convolution}$$\n 현 상황에서 MAC의 수식은 다음과 같음.  $$MAC = hw(c_1+c_2) +c_1c_2 = hwc_1 + hwc_2 + c_1c_2$$ $$hwc_1: \\text{Number of input feature map\u0026rsquo;s element}$$ $$hwc_2: \\text{Number of output feature map\u0026rsquo;s element}$$ $$c_1c_2: \\text{Number of filter\u0026rsquo;s element}$$\n MAC의 lower bound 는 다음과 같음.  $$MAC \\ge 2\\sqrt{hwB} + \\frac{B}{hw} \\to 2hw\\sqrt{c_1c_2} + c_1c_2$$\n \\(c_1 = c_2\\) 이면 MAC가 최소. 전체 연산량은 고정해놓고 \\(c_1:c_2\\)의 비율을 바꿔가면서 runtime 비교. 1:1일때가 가장 빠른 성능을 보임.     It reaches the lower bound when the numbers of input and output channels are equal.\n G2) Excessive group convolution increases MAC  Group convolution 이 많은 네트워크의 핵심이지만 Groups가 커지면 MAC을 증가시킴. → 안쓸 수는 없으니 적당히 쓰는게 좋다. 그룹에 따라 연산량이 줄어들기 때문에 B는 다음과 같음.  $$B=\\frac{hwc_1c_2}{g}$$\n$$MAC = hw(c_1+c_2) + \\frac{c_1c_2}{g} $$ $$ = hwc_1 + hwc_2 + \\frac{c_1c_2}{g}$$ $$ = hwc_1 + \\frac{Bg}{c_1} + \\frac{B}{hw}$$\n Groups 에 따라 runtime 비교.     The group number should be carefully chosen based on the target platform and task. It is unwise to use a large group number simply because this may enable using more channels, because the benet of accuracy increase can easily be outweighed by the rapidly increasing computational cost.\n G3) Network fragmentation reduces degree of parallelism  Inception 과 같이 여러 branch를 parallel하게 구성할 경우 성능은 좋아졌지만 효율성은 감소시킴. → GPU 같은 자원에는 어울리지 않음. Fragmentation 에 따른 runtime 비교.       Fragmented structure has been shown benecial for accuracy, it could decrease eciency because it is unfriendly for devices with strong parallel computing powers like GPU. It also introduces extra overheads such as kernel launching and synchronization.\n G4) Element-wise operations are non-negligible   Activation, Add 와 같은 Element-wise operation들의 비율이 꽤 존재. Figure 2 참고\n  이 연산은 FLOPs는 적지만 상대적으로 MAC은 큼.\n  Depthwise convolution 또한 element-wise 여서 MAC/FLOPs 가 클 것이라 생각.\n  각 상황에 대한 runtime 비교.\n     We observe around 20% speedup is obtained on both GPU and ARM, after ReLU and shortcut are removed.\n Conclusion and Discussions  use \u0026ldquo;balanced convolutions (equal channel width); be aware of the cost of using group convolution; reduce the degree of fragmentation; reduce element-wise operations.   다른 네트워크들에 대한 고찰   ShuffleNet V1\n Heavily group convolutions → G2 Bottleneck-like building blocks → G1 Residual Block → G3 Element-wise operation→ G4    MobileNet V2\n Inverted bottleneck structure → G1    Depthwise convolution \u0026amp; ReLU\n Element-wise operation → G4    NAS\n Highly fragmentation → G3      ShueNet V2: an Ecient Architecture Review of ShueNet v1  G1, G2, G3, G4 모두 지키지 않음. 이를 해결한 구조가 ShuffleNet V2 의 유닛 (Fig 3 (c), Fig 3 (d))    Channel Split and ShueNet V2   Fig 3 (c)\n Input feature 을 절반으로 나눠 두개의 branch 생성. Left branch는 아무 연산도 진행 X. → G3 에 대한 회피법. Right branch는 동일한 Number of filter로 1x1 Conv → 3x3 DWConv → 1x1 Conv 수행. → G1에 대한 회피법. 1x1 Conv 는 Group 을 나누지 않음 → G2에 대한 회피법. Residual Block의 Add operation 을 Concatenate 로 변경 → G4에 대한 회피법.    Fig 3 (d)\n Downsampling block Input feature 그대로 두개의 branch 생성. Number of filter는 모두 동일    네트워크 구조\n    Analysis of Network Accuracy  ShuffleNet V2는 효율적이며 성능도 좋음.  더 많은 channel, 더 큰 network를 만들 수 있음. DenseNet 이나 CondenseNet 처럼 feature reuse 과 매우 유사함.   DenseNet의 feature reuse 패턴과 ShuffleNet V2의 feature reuse 패턴 비교. 붉을 수록 Source layer와 Target layer의 연결성이 강하다는 의미. DenseNet과 같이 ShuffleNet V2에서도 Target layer가 멀어질 수록 연결성이 약함.    Experiment  총 4개의 모델과 비교.  ShuffleNet V1 MobileNet V2 Xception DenseNet      Accuracy vs. FLOPs  연산량을 40 MFLOPs 로 고정시키고 Network 를 구성한 후 성능 비교. (Table 8 상단)  Inference Speed vs. FLOPs/Accuracy  연산량을 특정 값 범위로 고정시키고 runtime 비교. (Fig 1 참조)    Compared with MobileNet v1  MobileNet V1의 경우 Accuracy가 좋지 않으나 GPU runtime은 가장 빠름. 이는 위에서 제시한 가이드 라인을 어느 정도 가장 잘 만족하기 때문.  Compared automatic model search  NAS 는 매우 느리지만 제시한 가이드 라인을 만족하고 speed에 대한 metric을 사용한다면 충분히 좋은 성능을 보일 것.  Compatibility with other methods  Squeeze-and-excitation 과 같은 module과 같이 사용할 수 있음. 속도는 떨어지나 정확도는 상승. (Table 8 하단)  Generalization to Large Models  2GFLOPs 이상의 큰 모델을 생성할 수 있음. 50개의 레이어를 가진 모델을 생성해도 ResNet-50 과 비교하여 적은 연산량, 뛰어난 성능을 보임. (Table 6 상단) SE module, residual block을 사용하여 더욱 깊게 만들어도 상대적으로 연산량이 적으면서 뛰어난 성능을 보임. (Table 6 하단)    Object Detection  Light-Head RCNN 사용. Classification 에서 성능이 별로였던 Xception 이 Detection 에선 성능이 좋음. → Receptive Field가 크기 때문이라고 생각. 3x3 depthwise convolution 을 추가하여 Receptive Field를 키워보니 (ShuffleNet V2*) runtime은 늘었으나 성능이 증가함.    P.S  어려웠다. 항상 가이드 라인을 지키면서 모델을 설계할 수 있을까?  ","permalink":"https://jjerry-test.github.io/blog/shufflenetv2/","tags":["Paper"],"title":"Review: ShuffleNetV2"},{"categories":["DeepLearning"],"contents":"Aggregated Residual Transformations for Deep Neural Networks Author: Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He\nDate: Nov 16, 2016\nURL: https://arxiv.org/abs/1611.05431\nIntroduction  Network desine 에 고려해야할 hyper-parameter가 너무 많음. (Width, filter size, Height, \u0026hellip;.) VGG, ResNet은 비슷한 구조의 레이어를 계속 쌓는 방법을 사용. Inception 은 성능은 이전보다 뛰어나지만 이전 방법들과 다르게 복잡한 구조를 쌓는 방법 사용. 본 논문에서는 VGG/ResNet과 같이 비슷한 레이어를 반복하지만 AlexNet 에서 나온 처음 제안된 Group Convolution 을 적용하여 split-transform-merge stretegy 를 도입. 일반적인 Reidual Block과 ResNeXt의 Residual Block 비교. Cardinality = Number of Groups,    Method Template  전체적인 구조는 기존의 VGG/ResNet과 같이 일정 Block 을 반복하여 쌓는 구조. 반복되는 블럭은 동일한 hyper parameter 사용.    Revisiting Simple Neurons  가장 기본적인 뉴런의 구조  $$\\sum_{i=1}^Dw_ix_i$$\n   기본 뉴런 또한 split-transform-merge (Splitting, Transforming, Aggregating)의 구조를 가짐. Vector X 가 $x_i$로 Splitting, $x_iw_i$로 Transforming, $\\sum_{i=1}^D$ 로 Aggregating  Aggregated Transformations  Networt-in-Network와 다르게 Network-in-Neuron이라는 컨셉으로 차원 확장  $$\\mathcal{F}(x) = \\sum_{i=1}^C\\mathcal{T}_i(\\mathrm{x})$$\n 다른 방식이지만 동일하다는 것을 설명      Model Capacity  Complexity, Number of parameter 를 유지하면서 실험. 다른 Hyper parameter는 수정하고 싶지 않기 때문에 Residual Block의 Cardinality C와 bottleneck d를 수정 Cardinality와 bottleneck d의 관계    Result On ImageNet-1K  Cardinality를 1~32 씩 증가시키되 complexity 는 유지하도록 설정하고 실험.       Increasing Cardinality 와 Increasing depth or width 비교 Cardinality 의 성능이 더 좋음.     Residual connections 여부에 따른 비교     State-of-the-art model 과 비교    On ImageNet-5K  5000개 클래스에서도 잘 되더라.      On CIFAR     On COCO object detection  Faster RCNN에 적용.    P.S  AlexNet의 선견지명. 하긴 안좋으면 논문으로 쓸리가 없지.  ","permalink":"https://jjerry-test.github.io/blog/resnext/","tags":["Paper"],"title":"Review: ResNeXt"},{"categories":["DeepLearning"],"contents":"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices Author: Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun\nDate: Jul 04, 2017\nURL: https://arxiv.org/abs/1707.01083\nIntroduction  Visual recognition 에서 deeper, larger CNN을 설계하는 것이 트렌드. 하지만 이는 매우 많은 연산량을 필요로함. 본 논문은 정해놓은 범위의 연산량에서 최고로 효율적인 구조는 찾아내는 것을 목표로 함. Xception, ResNeXt 에서 1x1 Convolution 을 사용하는데 두 네트워크에서 대부분의 연산량이 1x1 Convolution 이 차지하고 있어 비효율적. 이를 보완하기 위해 AlexNet에서 처음 제안한 group convolution 적용. Group convolution 의 단점을 보완하기 위해 channel shuffle operation 또한 제안.  Method Channel Shuffle for Group Convolutions  상대적으로 연산량이 많은 1x1 Convolution을 ResNeXt 에서 사용한 Group Convolution 으로 적용. 하지만 Group으로 계속 진행하다보면 특정 채널에 편향된 결과를 보이는 문제가 생길 것이므로 channel을 shuffle 해줌.    ShuffleNet Unit  ShuffleNet에서 사용된 Bottle unit은 Xception과 MobileNet에서 사용된 Residual Block에서 1x1 Convolution을 Group Convolution으로 변경하고 Channel Shuffle을 추가한 것. Stride unit 에선 element-wise addition이 아닌 concatenation으로 대체.    Network Architecture   Result Ablation Study Pointwise Group Convolutions   Groups 에 따른 성능 비교.\n  ShuffleNet s\\(\\times\\) 에서 s는 필터 개수에 대한 scaling factor.\n  무조건 많이 나눈다고 좋은 것은 아님.\n    Channel Shuffle vs. No Shuffle  Channel Shuffle 여부에 따른 성능 비교. Shuffle 적용시 성능이 뚜렷하게 증가한 것을 확인.    Comparison with Other Sturcture Units  제한된 연산량 내에서 다른 Network 들과 성능 비교.     기존에 비슷한 성능의 Network들과 연산량 비교.    Comparison with MobileNets and Other Frameworks  Mobile devices에 특화된 MobileNet과 성능 비교    Generalization Ability  MS COCO Data를 사용하여 Object detection 성능 비교.    Actual Speedup Evaluation  Mobile device에서 Inference 속도 비교.    P.S  GPU가 부족해서 했다던 Group Convolution의 부활..?  ","permalink":"https://jjerry-test.github.io/blog/shufflenetv1/","tags":["Paper"],"title":"Review: ShuffleNetV1"},{"categories":["DeepLearning"],"contents":"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features Author: Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo\nDate: May 13, 2019\nURL: https://arxiv.org/abs/1905.04899\nIntroduction  CNN 은 computer vision 문제에 많이 사용되고 있음. 효율적이고 높은 성능을 위해 data augmentation, regularization 등 기법을 적용. 특정 부분에 overfitting(?) 되는 것을 방지하기 위해 dropout, regional dropout 과 같은 방법 사용. 그 외에도 일부분을 0으로 채운다거나 노이즈로 채우는 방법, 정보가 있는 부분의 pixel을 줄이는 방법 등이 성능 향상을 보였으나 CNN은 데이터가 많이 고픈데\u0026hellip;.데이터를 없앤다..? 라는 부분에서 의문을 가짐. 영상의 일부를 자르고 다른 영상으로 대체하는 CutMix 를 제안.    CutMix Algorithm  A, B 두개의 클래스만 존재.  $$(x, y): \\text{Training image, label}$$ $$(A, B): \\text{Training class}$$ $$(x_A, y_A), (x_B, y_B): \\text{Training sample}$$\n 어느 부분을 섞을 것인지 binary mask (M) 생성 생성된 mask를 통해 섞을 비율 lambda 추출. Label의 경우 비율에 One-hot encoding이 합친 후 영상에서의 각 클래스의 비율로 변경.  $$\\mathrm{M}: \\text{Binary mask where to drop out and fill}$$ $$\\lambda: \\text{Combination ratio}$$ $$\\tilde{x} = \\mathrm{M} \\bigodot x_A + (1 - \\mathrm{M}) \\bigodot x_B$$ $$\\tilde{y} = \\lambda{y_A} + (1 - \\lambda)y_B$$\n M에서 bounding box 좌표 (B) 추출. x, y 좌표는 Uniform distribution. $x_B$에서 B 를 매칭시켜서 crop 후 B에 매칭되는 $x_A$ 의 부분에 paste.  $$\\mathrm{B}: \\text{Bounding box coordinates } (r_x, r_y, r_w, r_h)$$ $$r_x \\sim \\text{Unif }(0, W), r_w = W\\sqrt{1-\\lambda},$$ $$r_y \\sim \\text{Unif } (0, H), r_h = H\\sqrt{1-\\lambda}$$\nDiscussion  CutMix를 이용했을 때 CNN이 어느 부분을 학습하는지 확인.     다른 method와 비교하여 CutMix의 주요 차이점.     Validation Error를 비교했을 때 기존의 모델에 비해 CutMix 적용시 Error가 낮음.    Experiments Image Classification ImageNet Classification  Baseline, 다른 augmentation method와 비교     두 Model에 CutMix를 적용하여 성능 비교.    CIFAR Classification  다른 Regularization 들과 비교.     가벼운 Model 에 적용하여 비교.     CIFAR-10에 적용한 결과.    Ablation Studies  CutMix 에서 alpha가 뭐지\u0026hellip;\u0026hellip;.     CutMix하는 방법을 다양하게 적용했을 때 성능 비교 Center Gaussian: Uniform distribution → Gaussian distribution Fixed-size: 16 x 16 ( \\(\\lambda = 0.75\\) )로 고정 Scheduled: 학습이 진행될 수록 CutMix 확률을 0부터 1까지 증가 One-hot: 패치 비율에 따라 Portion label이 아닌 One-hot encoding으로 적용 Complete-label: lambda 를 고려하지 않고 \\(y = 0.5y_A + 0.5y_B\\)로 적용    Weakly Supervised Object Localization  Localization 부분에 대해 다른 방법들과 비교. 학습 후 CAM을 이용해서 bounding box를 그린 것으로 보임.      Transfer Learning of Pretrained Model  Object detection, Image captioning 에 적용하여 성능 비교.    Robustness and Uncertainty  Adversarial attack 에 대해 Accuracy 비교. Fast Gradient Sign Method (FGSM)을 이용하여 adversarial perturbation 생성.     Occlusion 상황에 대해서 성능 비교. 가운데 부분 혹은 Boundary 에 0~224 크기 사이의 hole을 생성.     Uncertainty    CutMix Algorithm   P.S  Appendix에 내용이 더 있지만\u0026hellip; 간단히 정리하려니 넣기 좀 힘듦. 당연한 얘기지만 모든 데이터에 적용하기엔 어려움이 있을 것으로 보임.  ","permalink":"https://jjerry-test.github.io/blog/cutmix/","tags":["Paper"],"title":"Review: CutMix"},{"categories":["Ubuntu"],"contents":"지극히 개인이 사용하기 위한 Dockerfile   환경을 만들 때마다 추가될 예정입니다. 마음껏 편하신대로 Copy \u0026amp; Paste 하세요!  TensorFlow FROMnvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04LABEL maintainer \u0026#34;Jerry Kim \u0026lt;jaeyeol2931@gmail.com\u0026gt;\u0026#34;ARG PYTHON_VERSION=3.7RUN apt-get updateRUN apt-get install -y \\  build-essential \\  cmake \\  git \\  curl \\  wget \\  ca-certificates \\  libjpeg-dev \\  libpng-devRUN apt-get update \u0026amp;\u0026amp; apt-get -y upgrade# 2020.10.11 내용 추가# Docker container에 ssh로 접속하고 싶다면....# RUN mkdir /var/run/sshdRUN echo \u0026#39;root:{개인 비밀번호}\u0026#39; | chpasswdRUN sed -i \u0026#39;s/PermitRootLogin prohibit-password/PermitRootLogin yes/\u0026#39; /etc/ssh/sshd_config# SSH login fix. Otherwise user is kicked off after loginRUN sed \u0026#39;s@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g\u0026#39; -i /etc/pam.d/sshdENV NOTVISIBLE \u0026#34;in users profile\u0026#34;RUN echo \u0026#34;export VISIBLE=now\u0026#34; \u0026gt;\u0026gt; /etc/profileEXPOSE22CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;]RUN rm -rf /var/lib/apt/lists/*RUN curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o ~/miniconda.shRUN chmod +x ~/miniconda.sh \u0026amp;\u0026amp; \\  ~/miniconda.sh -b -p /opt/conda \u0026amp;\u0026amp; \\  rm ~/miniconda.shRUN /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include ninja cython typing opencv matplotlib tqdm \u0026amp;\u0026amp; \\  /opt/conda/bin/conda install -y jupyter jupyterlab seaborn pillow pandas pylint scikit-learn scikit-image tensorflow-gpu \u0026amp;\u0026amp; \\  /opt/conda/bin/conda update -y --all \u0026amp;\u0026amp; \\  /opt/conda/bin/conda clean -yaENV PATH /opt/conda/bin:$PATH","permalink":"https://jjerry-test.github.io/blog/dockerfile/","tags":["Docker"],"title":"개인적인 도커 파일"},{"categories":["DeepLearning"],"contents":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks Author: Mingxing Tan, Quoc V. Le\nDate: May 28, 2019\nURL: https://arxiv.org/abs/1905.11946\nIntroduction  ConvNet 의 성능을 높이는데 Depth, Width, Image size 중 하나를 증가 시키는게 일반적인 방법. 본 논문에서는 ConvNet의 성능과 효율성을 증가시키기 위한 원론적인 방법에 대한 연구. 실험의 결과로 Compound scaling method 를 제안.    image   Compound Model Scaling  Scaling 문제에 대한 정의, Approache 별 연구, 새로운 방법에 대한 내용 서술.  Problem Formulation  Model scaling은 Baseline 에서 Length(Depth), Width, Resolution 를 확장. 하지만 실제론 리소스에 제약이 있으니 이에 맞춰 문제를 새롭게, 단순하게 정의. Design space를 줄이기 위해 모든 레이어는 상수 값을 이용하여 규칙적으로 변화하도록 함. 최종 목적은 제한된 리소스에서 성능을 최대화하는 것.  $$\\mathcal{N}: \\text{ConvNet}$$ $$ \\mathcal{F}_i: \\text{Layer architecture}$$ $$L_i: \\text{Network length}$$ $$C_i: \\text{Width}$$ $$H_i, W_i: \\text{Input resolution}$$\n$$ {max}_{d, w, r} \\space\\space\\space\\space Accuracy(\\mathcal{N}(d, w, r)) $$ $$s.t. \\space\\space\\space\\space \\mathcal{N}(d, w, r) = \\bigodot_{i=1\u0026hellip;s}\\hat{\\mathcal{F}}_i^{d \\cdot \\hat{L}_i}(X_{\\langle r\\cdot \\hat{H}_i, r \\cdot \\hat{W}_i, w\\cdot \\hat{C}_i \\rangle} )$$ $$Memory(\\mathcal{N}) \\leq \\text{target memory}$$ $$FLOPS(\\mathcal{N}) \\leq \\text{target flops}$$\nScaling Dimensions  두번째 문제는 d, w, r 이 서로 dependent 하고 제한된 리소스에 따라 값이 변화. 그래서 기존에는 다음 세 개의 요소 중 하나를 변경함.  Depth (d)  VGGNet, GoogLeNet, ResNet 등등 레이어를 많이 많이 !  Width (w)  채널 수를 늘리고 깊이를 줄이는 방식. 하지만 higher level feature를 잡기 힘들 수 있음.  Resolution (r)  클수록 더 양질의 패턴을 찾을 수 있음.    image    Observation 1: 이를 통해서 어떤 요소를 증가시키던 성능이 오르는 것을 확인 하지만 Model이 무거워짐.  Compound Scaling  경험적으로 세 요소가 dependent 하다는 것을 이미 알고 있음. 다른 depth, resolution 을 이용하여 성능 비교.    image    Observation 2: 세 요소의 balance가 매우 중요.. 다음과 같은 compound scaling method 제안.  $$\\phi: \\text{Compound Coefficient} \\\\ depth: d = \\alpha^\\phi \\\\ width: w = \\beta^\\phi \\\\ resolution: r = \\gamma^\\phi \\\\ \\text{s.t. }\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\\\ \\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1$$\n 각 값은 small grid search로 결정된 상수 값.  EfficientNet Architecture   image    EfficientNet-B0 를 baseline network로 하여 Accuracy, FLOPS 둘 다 최적화하도록 multi-objective neural architecture search 적용.  Step 1  \\(\\phi\\) =1 로 고정 식 2, 3을 기반으로 하여 small grid search EfficientNet-B0에 가장 적합한 값을 \\(\\alpha=1.2, \\beta=1.1, \\gamma=1.15\\)   Step 2  \\(\\alpha, \\beta, \\gamma\\)를 고정하고 \\(\\phi\\)를 변경하여 실험. Result (ImageNet Result for EfficientNet) 참고      Result Scaling Up MobileNets and ResNets  Compound scale 을 증명하기 위해 MobileNet과 ResNet을 이용하여 비교. 기존의 방법들은 3개중 1개의 요소만 scaling.    image   ImageNet Result for EfficientNet   Training Setting\n Optimization  RMSProp Decay: 0.9 Momentum: 0.9   Batch normalization  Momentum: 0.99   Weight decay: 1e-5 Initial learning rate: 0.256  Decay: 0.97 (every 2.4 epochs)   Swish Activation AutoAugmentation: 뭔지 모르겠군 1 Stochastic depth: 뭔지 모르겠군 2  Drop connect ratio: 0.2   Dropout  0.2 ~ 0.5 (B0 ~ B7)      B0부터 B7 까지 성능 비교.\n  GPipe에 비해 8.4배 적고 좋은 성능.\n    image    CPU를 이용한 Inference 속도 비교.    image    모델별 FLOPS-Accuracy curve    image   Transfer Learning Result for EfficientNet  ImageNet pretrained model 을 이용하여 각종 Dataset을 Transfer learning 한 성능 비교 사용한 Dataset    image    Transfer learning 결과 전체적으로 모델이 가벼움.    image    기존의 모델들과 비교하여 가볍지만 뛰어난 성능을 보임.    image   Discussion  EfficientNet-B0 를 이용하여 각기 다른 scaling method를 이용하여 성능 비교    image     image     image   P.S  이런 연구는..NAS(Network Architecture Search)가 답..인건가 근데 이것도 하드웨어가 빵빵해야\u0026hellip;.. 크흡  ","permalink":"https://jjerry-test.github.io/blog/efficientnet/","tags":["Paper"],"title":"Review: EfficientNet"},{"categories":["DeepLearning"],"contents":"CBAM: Convolutional Block Attention Module Author: Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon\nDate: Jul 17, 2018\nURL: https://arxiv.org/abs/1807.06521\nIntroduction  BAM 에서 설명한 것처럼 최근 CNN 성능 향상에 주로 연구되는 요소는 depth, width, cardinality. 본 논문에선 Convolutional Block Attention Module(CBAM) 제안. Convolution을 이용하여 channel, spatial information 을 추출하고 섞어서 사용. channel, spatial attention module은 각각 \u0026ldquo;what\u0026rdquo;, \u0026ldquo;where\u0026quot;에 대한 정보를 학습할 수 있음.    Convolutional Block Attention Module  CBAM 의 구조는 다음 사진과 같음.  $$F: \\text{Input feature map} \\\\ F':\\text{Channel attention module feature map} \\\\ F'': \\text{Spatial attention module feature map} \\\\ F' = M_c(F)\\bigotimes F \\\\ F'' = M_s(F')\\bigotimes F'$$\n  Channel attention branch $$M_c(F) = \\sigma(MLP(AvgPool(F)) + MLP(MaxPool(F))) \\\\ = \\sigma(W1(W0(F^c_{avg})) + W1(W0(F^c_{max})))$$\nW0 의 output channel 크기: F의 채널 수 / reduction ratio(r)\nW1 의 output channel 크기: F의 채널 수\nSpatial attention branch $$M_s(F)=\\sigma(f^{7\\times7}([AvgPool(F); MaxPool(F)])) \\\\ = \\sigma(f^{7 \\times 7}([F^s_{avg};F^s_{max}]))$$\n 7x7 Convolution의 output channel 크기: 1  Arrangement of attention modules  두 branch의 순서를 어떻게 배열할지 고민. 실험적으로 Channel → Spatial 로 하기로 함.  Ablation study using ImageNet-1K Channel attention  Pooling 기법별 성능 비교    Spatial attention  Pooling, convolution kernel size 에 따른 성능 비교    Arrangement of the channel and spatial attention  Attention module 순서에 따른 성능 비교    Result Classification Result on ImageNet-1K       Object Detection on MS COCO and VOC 2007     Network Visualization with Grad-CAM   P.S  BAM과 동일하게 Original Code가 있지만\u0026hellip;.논문과 다른 부분이 매우 많음.  ","permalink":"https://jjerry-test.github.io/blog/cbam/","tags":["Paper"],"title":"Review: CBAM"},{"categories":["DeepLearning"],"contents":"BAM: Bottleneck Attention Module Author: Jongchan Park, Sanghyun Woo, Joon-Young Lee, In So Kweon\nDate: Jul 17, 2018\nURL: https://arxiv.org/abs/1807.06514\nIntroduction   DL은 Classification, Detection, Segmentation 등 많은 패턴 인식 분야에서 강력한 Tool로 사용.\n  성능을 올리기 위해서 좋은 backbone을 설계하는 것이 기본적인 접근법.\n  직관적인 방법은 더 깊게 설계하는 것.\n  VGGNet는 AlexNet 보다 두배 이상.\n  ResNet 은 VGGNet보다 22배 이상이면서 residual connections 사용하여 gradient flow 를 향상.\n  GoogLeNet 은 매우 깊고 같은 layer에서 다양한 feature를 사용하여 성능 향상.\n  DenseNet 이전 layer의 feature map 들을 concatenation 하여 사용.\n  WideResNet, PyramidNet layer의 channels 를 증가하여 성능 향상.\n  ResNeXt, Xception과 같은 backbone은 grouped convolutions을 이용하여 성능 향상.\n  본 논문에선 attention 의 효과를 보기 위해 기존의 architecture 에 사용하기 쉬운 가벼운 Bottle Attention Module(BAM) 제안\n    Bottleneck Attention Module  BAM 의 구조는 다음 사진과 같음.  $$F: \\text{Input feature map} \\\\ M(F): \\text{Attention map} \\\\ F' = F + F\\bigotimes M(F) \\\\ M(F) = \\sigma(M_c(F) + M_s(F))$$\n  Channel attention branch $$M_c(F) = BN(MLP(AvgPool(F))) \\\\ = BN(W_1(W_0AvgPool(F) + b_0)+b_1)$$\nW0 의 output channel 크기: F의 채널 수 / reduction ratio(r)\nW1 의 output channel 크기: F의 채널 수\nSpatial attention branch $$M_s(F)=BN(f_3^{1\\times1}(f_2^{3\\times3}(f_1^{3\\times3}(f_0^{1\\times1}(F)))))$$\n 모든 연산은 convolution 연산. 3x3 Convolution 연산 수행시엔 dilation convolution 사용. 첫번째~세번째 Convolution 의 output channel 크기: F의 채널 수 / reduction ratio(r) 마지막 Convolution 의 output channel 크기: 1  Combine two attention branches $$M(F) = \\sigma(M_c(F) + M_s(F))$$\n Channel attention branch 출력: 1x1xR Spatial attention branch 출력: HxWx1 두 attention branch를 합치는 방법으로 element-wise summation, multiplication, max operation 고려.  Ablation study using CIFAR-100 Dilation value and Reduction ratio  Dilation value와 Reduction ratio에 따른 성능 비교 Table 1 (a)  Separate or Combined branches \u0026amp; Combining methods  두 attention branch 사용 방법에 따른 성능 비교 Table 1 (b)  Comparison with placing original convblocks  BAM 사용 여부에 따른 성능 비교 Table 1 (c)    Bottleneck: The efficient point to place BAM  BAM 사용 위치에 따른 성능 비교.    Result Classification Result on CIFAR-100 and ImageNet-1K   Object Detection on MS COCO and VOC 2007   Comparison with Squeeze-and-Excitation   P.S  Original Code가 있지만\u0026hellip;.논문과 다른 부분이 매우 많음.  ","permalink":"https://jjerry-test.github.io/blog/bam/","tags":["Paper"],"title":"Review: BAM"},{"categories":["DeepLearning"],"contents":"Searching for MobileNetV3 Author: Andrew G. Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam\nDate: May 06, 2019\nURL: https://arxiv.org/abs/1905.02244\nIntroduction  Efficient neural network 는 low latency, higher accuracy 와 더불어 전력소모가 줄어들게 하기 때문에 배터리 수명 보존에도 기여. 이에 힘입어 다음 세대의 더 효율적인 네트워크 제안. 본 논문에서 중요한 것은 네 가지.  Complementary search techniques New efficient versions of non-linearities practical New efficient network design New efficient segmentation decoder    Efficient Mobile Building Blocks  MobileNetV2의 Inverted residual structure에 squeeze and excitation 을 추가.    Network Search Platform-Aware NAS for Block-wise Search  RNN-based controller, factorized hierarchical search space  NetAdapt for Layer-wise Search  platform-aware NAS로 찾은 Seed network architecture 로 시작. 매 스텝마다:\n(a) 이전의 proposal 에 비해 latency가 최소 a만큼 감소된 새로운 proposal 생성.\n(b) 각 proposal은 이전 스텝의 pre-trained model을 사용하여 새로 제안된 architecture를 채우고 누락된 weight에 대해선 적절한 값으로 채움. 각 proposal 은 T step 동안 finetuning하고 대략적인 accuracy 추출.\n(c) 몇몇 metric을 이용하여 최적의 proposal 을 선택. 목표로하는 latency에 도달할 때까지 반복.  Network Improvements  Network의 초반, 후반부의 expansive layer 구조 수정. 새로운 non-linearity fuction 제안.  h-swish: swish 의 변형 버전, 빠른 계산 속도, 경량화    Redesigning Expensive Layers   Nonlinearities   sigmoid → h-swish\n$$hard\\text{-}swish(x) = x\\frac{ReLU6(x + 3)}{6}$$\n  h-swish 를 deeper layer에서만 사용.\n      Large squeeze-and-excite  MnasNet: Platform-Aware Neural Architecture Search for Mobile 에서 Squeeze-and-Excite(SE) bottleneck 크기만큼 convolutional bottleneck 발생. 본 논문에선 expansion layer의 채널의 1/4로 고정. 파라미터 미약하게 증가하면서 정확도 증가.  Result     P.S  Batch size를 4096으로 테스트\u0026hellip;. 역시 하드웨어 깡패 구글\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/mobilenetv3/","tags":["Paper"],"title":"Review: MobileNet V3"},{"categories":["DeepLearning"],"contents":"MobileNetV2: Inverted Residuals and Linear Bottlenecks Author: Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen\nDate: Jan 13, 2018\nURL: https://arxiv.org/abs/1801.04381\nAbstract  새로운 mobile architecture! mobile에서 Object detection, Semantic segmentation 에 적용할 수 있음!  Introduction  MobileNet과 비슷한 얘기 새로운 모듈 제안! → Inverted residual block  Preliminaries, discussion and intuition  논문의 가장 큰 특징은 Depthwise Separable Convolution, Linear Bottlenecks, Inverted Residuals.  Depthwise Separable Convolutions  Xception 에서부터 제안된 Convolution Efficient network의 핵심 Block  Linear Bottlenecks  MobileNetV1에서 computation 과 accuracy의 trade-off 를 비교하기 위해 width multiplier parameter를 사용. 본 논문에선 1x1 Convolution 을 이용하여 dimension reduction 수행.  Inverted Residuals  기존의 Residual Block 과 비슷한 구조 (Bottleneck → Expansion \u0026amp; Skip connection). 본 논문에선 Expansion → Bottleneck \u0026amp; Skip connection 구조의 Inverted Residual Block 사용.      Model Architecture   Ablation study Inverted residual connections  Skip connection 을 bottleneck 후에 하는 것이 성능이 더 좋음. Figure 6 (b) 참고  Importance of linear bottleneck  Bottleneck 에서는 activation 을 사용하지 않는 것이 더 좋음. Figure 6 (a) 참고    ","permalink":"https://jjerry-test.github.io/blog/mobilenetv2/","tags":["Paper"],"title":"Review: MobileNet V2"},{"categories":["Living"],"contents":"After MacOS Install 1. Install applications  Kakao Talk Between Snap  Add Terminal shortcut   Speedtest Magnet MenuBar Stats  Add MenuBar Stats Plugins Manager https://www.seense.com/menubarstats/plugins3/   Buddy for Youtube Bandizip  https://kr.bandisoft.com/bandizip/x/   Notion  https://www.notion.so/desktop    2. Install brew /usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#34; 3. Install oh-my-zsh sh -c \u0026#34;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\u0026#34; 4. Install pyenv Python Installation for mac\n5. Edit .zshrc # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # Path to your oh-my-zsh installation. export ZSH=\u0026#34;/Users/jerry/.oh-my-zsh\u0026#34; # Set name of the theme to load --- if set to \u0026#34;random\u0026#34;, it will # load a random theme each time oh-my-zsh is loaded, in which case, # to know which specific one was loaded, run: echo $RANDOM_THEME # See https://github.com/robbyrussell/oh-my-zsh/wiki/Themes ZSH_THEME=\u0026#34;jreese\u0026#34; # Set list of themes to pick from when loading at random # Setting this variable when ZSH_THEME=random will cause zsh to load # a theme from this variable instead of looking in ~/.oh-my-zsh/themes/ # If set to an empty array, this variable will have no effect. # ZSH_THEME_RANDOM_CANDIDATES=( \u0026#34;robbyrussell\u0026#34; \u0026#34;agnoster\u0026#34; ) # Uncomment the following line to use case-sensitive completion. # CASE_SENSITIVE=\u0026#34;true\u0026#34; # Uncomment the following line to use hyphen-insensitive completion. # Case-sensitive completion must be off. _ and - will be interchangeable. # HYPHEN_INSENSITIVE=\u0026#34;true\u0026#34; # Uncomment the following line to disable bi-weekly auto-update checks. # DISABLE_AUTO_UPDATE=\u0026#34;true\u0026#34; # Uncomment the following line to automatically update without prompting. # DISABLE_UPDATE_PROMPT=\u0026#34;true\u0026#34; # Uncomment the following line to change how often to auto-update (in days). # export UPDATE_ZSH_DAYS=13 # Uncomment the following line if pasting URLs and other text is messed up. # DISABLE_MAGIC_FUNCTIONS=true # Uncomment the following line to disable colors in ls. # DISABLE_LS_COLORS=\u0026#34;true\u0026#34; # Uncomment the following line to disable auto-setting terminal title. # DISABLE_AUTO_TITLE=\u0026#34;true\u0026#34; # Uncomment the following line to enable command auto-correction. # ENABLE_CORRECTION=\u0026#34;true\u0026#34; # Uncomment the following line to display red dots whilst waiting for completion. # COMPLETION_WAITING_DOTS=\u0026#34;true\u0026#34; # Uncomment the following line if you want to disable marking untracked files # under VCS as dirty. This makes repository status check for large repositories # much, much faster. # DISABLE_UNTRACKED_FILES_DIRTY=\u0026#34;true\u0026#34; # Uncomment the following line if you want to change the command execution time # stamp shown in the history command output. # You can set one of the optional three formats: # \u0026#34;mm/dd/yyyy\u0026#34;|\u0026#34;dd.mm.yyyy\u0026#34;|\u0026#34;yyyy-mm-dd\u0026#34; # or set a custom format using the strftime function format specifications, # see \u0026#39;man strftime\u0026#39; for details. # HIST_STAMPS=\u0026#34;mm/dd/yyyy\u0026#34; # Would you like to use another custom folder than $ZSH/custom? # ZSH_CUSTOM=/path/to/new-custom-folder # Which plugins would you like to load? # Standard plugins can be found in ~/.oh-my-zsh/plugins/* # Custom plugins may be added to ~/.oh-my-zsh/custom/plugins/ # Example format: plugins=(rails git textmate ruby lighthouse) # Add wisely, as too many plugins slow down shell startup. plugins=(git) source $ZSH/oh-my-zsh.sh # User configuration # export MANPATH=\u0026#34;/usr/local/man:$MANPATH\u0026#34; # You may need to manually set your language environment # export LANG=en_US.UTF-8 # Preferred editor for local and remote sessions # if [[ -n $SSH_CONNECTION ]]; then # export EDITOR=\u0026#39;vim\u0026#39; # else # export EDITOR=\u0026#39;mvim\u0026#39; # fi # Compilation flags # export ARCHFLAGS=\u0026#34;-arch x86_64\u0026#34; # Set personal aliases, overriding those provided by oh-my-zsh libs, # plugins, and themes. Aliases can be placed here, though oh-my-zsh # users are encouraged to define aliases within the ZSH_CUSTOM folder. # For a full list of active aliases, run `alias`. # # Example aliases # alias zshconfig=\u0026#34;mate ~/.zshrc\u0026#34; # alias ohmyzsh=\u0026#34;mate ~/.oh-my-zsh\u0026#34; # Set pyenv export PYENV_ROOT=\u0026#34;${HOME}/.pyenv\u0026#34; export PATH=\u0026#34;${PYENV_ROOT}bin:$PATH\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; # \u0026gt;\u0026gt;\u0026gt; conda initialize \u0026gt;\u0026gt;\u0026gt; # !! Contents within this block are managed by \u0026#39;conda init\u0026#39; !! __conda_setup=\u0026#34;$(\u0026#39;/Users/jerry/.pyenv/versions/miniconda3-latest/bin/conda\u0026#39; \u0026#39;shell.zsh\u0026#39; \u0026#39;hook\u0026#39; 2\u0026gt; /dev/null)\u0026#34; if [ $? -eq 0 ]; then eval \u0026#34;$__conda_setup\u0026#34; else if [ -f \u0026#34;/Users/jerry/.pyenv/versions/miniconda3-latest/etc/profile.d/conda.sh\u0026#34; ]; then . \u0026#34;/Users/jerry/.pyenv/versions/miniconda3-latest/etc/profile.d/conda.sh\u0026#34; else export PATH=\u0026#34;/Users/jerry/.pyenv/versions/miniconda3-latest/bin:$PATH\u0026#34; fi fi unset __conda_setup # \u0026lt;\u0026lt;\u0026lt; conda initialize \u0026lt;\u0026lt;\u0026lt; 6. Create conda environment #!/bin/bash  conda install -y ipykernel jupyter jupyterlab pylint conda update --all -y conda create -y -n tf python=3.7 conda create -y -n tc python=3.7 source ~/.pyenv/versions/miniconda3-latest/etc/profile.d/conda.sh # Create TensorFlow Environment conda activate tf conda install -y numpy scipy pandas matplotlib pylint conda install -y seaborn pillow scikit-image opencv scikit-learn tqdm ipython ipykernel ipywidgets conda install -y tensorflow conda update --all -y python -m ipykernel install --user --name tf --display-name TensorFlow conda deactivate # Create PyTorch Environment conda activate tc conda install -y numpy scipy pandas matplotlib pylint conda install -y seaborn pillow scikit-image opencv scikit-learn tqdm ipython ipykernel ipywidgets conda install -y -c pytorch pytorch torchvision conda update --all -y python -m ipykernel install --user --name tc --display-name PyTorch conda deactivate 7. Install VSCode  https://code.visualstudio.com/docs/?dv=osx Install plugin  python pylance Rainbow Brackets indent-rainbow Remote - SSH kite  https://kite.com/download/   c/c++ markdown-all-in-one leetcode  brew install node Sign in using github      ","permalink":"https://jjerry-test.github.io/blog/macbook-setup/","tags":["Macbook"],"title":"개인적인 맥북 세팅법"},{"categories":["Living"],"contents":"Raspberry pi 4 \u0026amp; Transmission Container  1. Docker Install curl -fsSL get.docker.com -o get-docker.sh sudo bash get-docker.sh sudo usermod -aG docker pi 그리고 재부팅\n2. Pull Transmission Image docker pull linuxserver/transmission 3. Create Transmission Container docker create \\  --name=transmission \\  -e PUID=1000 \\  -e PGID=1000 \\  -e TZ=Asia/Seoul \\  -e USER={username} \\  -e PASS={password} \\  -p 9091:9091 \\  -p 51413:51413 \\  -p 51413:51413/udp \\  -v {path to data}:/config \\  -v {path to downloads}:/downloads \\  -v {path to watch folder}:/watch \\  --restart unless-stopped \\  linuxserver/transmission 4. Run Container docker start transmission ","permalink":"https://jjerry-test.github.io/blog/rpi_transmission/","tags":["Hardware"],"title":"Raspberry pi 4 에 Transmission 세팅하기"},{"categories":["DeepLearning"],"contents":"Raspberry pi 4 \u0026amp; Google Coral USB Accelerator 평소에 라즈베리파이 4를 NAS로 사용하고 있었습니다. 이런 느낌으로..\n하지만\u0026hellip;항상 마음 속에는 \u0026ldquo;흠\u0026hellip;.라즈베리파이로 딥러닝을 돌려보고 싶다\u0026hellip;\u0026rdquo; 라는 생각을 하고 있었죠.\n평소와 같이 평화로운 중고나라 를 탐색하고 있었습니다. (모니터가 사고 싶어서 \u0026hellip;.)\n근데 . . 갑자기 . . ? 왜 인지 모르겠지만 Google Coral USB Accelerator 를 검색하고 싶더군요.\n그래서 바로 검색을 했고 7마넌(나름 저렴)에 올라와있길래 일요일에 주문을 했습니다.\n그리고 오늘 집에 도착을 했죠.\n이제 사용을 해보려고 합니다.\n준비물을 소개하도록 하죠.\n준비물  라즈베리 파이 Coral Accelerator Webcam    Step 1. 연결     모두 연결 후 라즈베리파이를 켜면 위 사진과 같이 USB에 흰색 불이 들어옵니다.\nStep 2. 라즈베리파이 세팅 #!/bin/bash echo \u0026quot;deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo apt-get update sudo apt-get install libedgetpu1-std # Check Your Platform # https://www.tensorflow.org/lite/guide/python sudo pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl #!/bin/bash git clone https://github.com/google-coral/examples-camera.git cd examples-camera sh download_models.sh  download_models.sh를 실행하시면 다음과 같이 all_models라는 디렉토리 안에 각 데이터셋 별 labelmap.txt와 학습된 모델의 tflite 파일이 있습니다.\n여기서 밑줄 친 파일을 이용해서 object detection 을 해볼겁니다! 일단 그 다음 세팅으로 넘어가죠.\n  #!/bin/bash cd opencv sh install_requirements.sh  opencv를 이용한 스크립트를 사용하기 위해 필요한 것들을 설치합니다.\nStep 3. 스크립트 실행 이제 opencv 디렉토리 안에 detect.py를 실행 시켜줄건데요. 옵션이 몇개 있습니다. 한번 살펴보도록 하죠.\nusage: detect.py [-h] [--model MODEL] [--labels LABELS] [--top_k TOP_K] [--camera_idx CAMERA_IDX] [--threshold THRESHOLD] optional arguments: -h, --help show this help message and exit --model MODEL .tflite model path --labels LABELS label file path --top_k TOP_K number of categories with highest score to display --camera_idx CAMERA_IDX Index of which video source to use. --threshold THRESHOLD classifier score threshold  한번 다음과 같이 실행을 해보겠습니다.\npython detect.py \\ --model ../all_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite \\ --labels ../all_models/coco_labels.txt \\ --top_k 3 \\ --threshold 0.7    막 빠를 줄 알았는데 Webcam 의 한계라 그런지\u0026hellip; FPS가 낮네요..\n추후에 카메라 모듈을 이용해서 해봐야겠습니다.\n그럼 간단한 이용기를 마치겠습니다.\n","permalink":"https://jjerry-test.github.io/blog/coral/","tags":["Tools"],"title":"Google Coral USB 사용기"},{"categories":["DeepLearning"],"contents":"StarGAN v2: Diverse Image Synthessis for Multiple Domains Author: Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha\nDate: Dec 04, 2019\nURL: https://arxiv.org/abs/1912.01865\nAbstract   Image translation 을 잘 하는 Model을 학습하려면 다음 사항을 만족해야함\n Diversity of generated images Scalability over multiple domains    기존의 방법들은 limited diversity, multiple models(networks)를 다룸.\n  StarGAN v2는 두 조건 모두 만족.\n  Introduction  Domain: 시각적으로 구별되는 범주 Style: 각 영상이 가지는 독특한 외관적 특성    StarGAN v2 Proposed framework     4개의 Network 로 구성.\n  Generator (G)\n Image x와 Style code s를 입력으로 받아 새로운 영상을 생성. adaptive instance normalization (AdaIN) 사용.    Mapping network (F)\n Latent code z와 Domain code y를 입력으로 받아 Style code s생성. Multi Layer Perceptron 구조.    Style encoder (E)\n Image x와 Domain code y를 입력으로 받아 x에서 Style code s를 추출.    Discriminator (D)\n Image x를 입력으로 받아 Domain code y와 Real/Fake 분류.    Training objectives  Adversarial objective  GAN 에서 기본적으로 사용되는 Loss    $$\\mathcal{L}_{adv}=\\mathbb{E}_{\\mathrm{x},y}[\\log{D_y}(\\mathrm{x})] + \\mathbb{E}_{\\mathrm{x}, \\tilde{y}, \\mathrm{z}}[\\log{(1-D_{\\tilde{y}}(G(\\mathrm{x}, \\tilde{\\mathrm{s}})))}$$\n Style reconstruction  G(x, s) 를 Style encoder E 에 넣어 s 추출 후 입력 s와 비교    $$\\mathcal{L}_{sty}=\\mathbb{E}_{\\mathrm{x},\\tilde{y}, \\mathrm{z}}[\\parallel\\tilde{\\mathrm{s}}-E_{\\tilde{y}}(G(\\mathrm{x}, \\tilde{\\mathrm{s}}))\\parallel_1]$$\n Style diversification  G가 다양한 Image를 생성할 수 있도록 Regularization 하는 역할. z1, z2 가 F에 의해 생성된 s1, s2와 입력 x를 G의 입력으로 새로운 영상 생성. L1 Norm 계산.    $$\\mathcal{L}_{ds}=\\mathbb{E}_{\\mathrm{x},\\tilde{y}, \\mathrm{z}_1, \\mathrm{z}_2}[\\parallel G(\\mathrm{x}, \\tilde{\\mathrm{s}}_1) - G(\\mathrm{x}, \\tilde{\\mathrm{s}}_2) \\parallel_1]$$\n Preserving source characteristics  Cycle GAN 의 cycle consistency loss. target domain의 style 을 적용한 영상을 다시 E(x)로 추출된 s를 이용하여 x'로 reconstruction 한 후 L1 Norm 계산.    $$\\mathcal{L}_{cyc}=\\mathbb{E}_{\\mathrm{x}, y, \\tilde{y}, \\mathrm{z}}[\\parallel \\mathrm{x} - G(G(\\mathrm{x}, \\tilde{\\mathrm{s}}), \\hat{\\mathrm{s}})\\parallel_1]$$\n  Full objective\n$$\\mathcal{L}_D = -\\mathcal{L}_{adv} \\ \\mathcal{L}_{F, G, E}=\\mathcal{L}_{adv} + \\lambda_{sty} \\mathcal{L}_{sty} - \\lambda_{ds} \\mathcal{L}_{ds} + \\lambda_{cyc} \\mathcal{L}_{cyc}$$\n About \\(\\lambda\\)       Dataset sty ds cyc     CelebA-HQ 1 1 1   AFHQ 0.3 1 0.1    Experiments   Baselines\n MUNIT DRIT MSGAN StarGAN    Datasets\n CelebA-HQ AFHQ    Evaluation metrics\n Frechet inception distance (FID) Learned perceptual image patch similarity (LPIPS)    Analysis of individual components  StarGAN에서 본 연구에서 제안하는 방법들을 하나하나 넣어가면서 성능 실험. 정량적 평가를 보면 추가할 때마다 좋아지는 것을 볼 수 있음.     각 단계별 생성한 영상의 결과    Comparison on diverse image synthesis  다른 방법들과 비교 Latent-guided synthesis ( Latent code 만을 이용하여 생성 )       Referenc-guided synthesis ( Style code 를 이용한 생성 )       Human evaluation  방법 별로 100개의 sample 생성 후 사람이 판단.      Result  겁나\u0026hellip;.잘 생성함\u0026hellip;      ","permalink":"https://jjerry-test.github.io/blog/starganv2/","tags":["Paper"],"title":"Review: StarGAN v2"},{"categories":["DeepLearning"],"contents":"StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation Author: Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha\nDate: Dec 21, 2019\nURL: https://arxiv.org/abs/1711.09020\nIntroduction  Multi domain image translation 이라고 하면 다음 사진과 같이 머리 색, 성별, 연령대 등과 같이 여러 condition 에 대응하는 영상을 생성.     기존의 image-to-image translation 은 n개의 domain 을 적용할 시에 n*(n-1) 개의 generator가 필요했음. StarGAN은 한 1개의 generator로 n개의 domain 을 적용.    Star Generative Adversarial Networks   Loss functions  Adversarial Loss  $$\\mathcal{L}_{adv} = \\mathbb{E}_x [\\log D_{src}(x)] + \\mathbb{E}_{x, c} [\\log (1-D_{src}(G(x,c)))] - \\lambda_{gp}\\mathbb{E}_{\\hat{x}}[(\\Vert \\nabla_{\\hat{x}}D_{src}(\\hat{x})\\Vert_2 -1)^2]$$\n$$\\lambda_{gp} = 10$$\n Domain Classification Loss  $$\\mathcal{L}^r_{cls} = \\mathbb{E}_{x, c'}[-\\log D_{cls}(c'|x)]$$\n$$\\mathcal{L}^f_{cls} = \\mathbb{E}_{x, c'}[-\\log D_{cls}(c|G(x, c)]$$\n Reconstruction Loss  $$\\mathcal{L}_{rec} = \\mathbb{E}_{x, c, c'}[\\Vert x-G(G(x, c), c')\\Vert_1]$$\n Full Objective  $$\\mathcal{L}_D = -\\mathcal{L}_{adv} + \\lambda_{cls}\\mathcal{L}^r_{cls}$$\n$$\\mathcal{L}_G = \\mathcal{L}_{adv} + \\lambda_{cls}\\mathcal{L}^f_{cls}+\\lambda_{rec}\\mathcal{L}_{rec}$$\n$$\\lambda_{cls} = 1, \\lambda_{rec} = 10$$\nResult     Training Detail     ","permalink":"https://jjerry-test.github.io/blog/starganv1/","tags":["Paper"],"title":"Review: StarGAN v1"},{"categories":["DeepLearning"],"contents":"이번 포스팅에선 Weights \u0026amp; Biases 라는 Tool 을 소개드리려 합니다.\n다음과 같은 특징을 강조하네요.\n Store hyper-parameters used in a training run Search, compare, and visualize training runs Analyze system usage metrics alongside runs Collaborate with team members Replicate historic results Run parameter sweeps Keep records of experiments available forever  그럼 간단 사용법에 대해서 설명드리겠습니다.\nSign Up 을 했다는 가정하에 진행합니다. 패키지 설치 wandb 패키지를 먼저 설치해줍니다.\npip install wandb wandb 로그인 설치 후 Terminal (or 명령 프롬프트) 에 다음과 같이 입력합니다.\nwandb login 그러면 아래와 같은 화면이 나오면서 웹 브라우저가 켜집니다.\n    Browser 에서 API키를 복사해주해서 Terminal (or 명령 프롬프트) 에 붙여넣기합니다.\n  그럼 wandb 에 로그인 완료!\n  Code 실행 간단한 MLP를 이용한 MNIST Classification 문제입니다.\n# wandb initialization import wandb wandb.init(project=\u0026#34;test_project\u0026#34;) import os import numpy as np from matplotlib import pyplot as plt import tensorflow as tf from tensorflow.keras import models, layers, optimizers, losses, utils, datasets # Import callback function from wandb.keras import WandbCallback print(\u0026#34;Packge Loaded!\u0026#34;) # Data Loading (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.reshape(train_x/255., [-1, 784]), np.reshape(test_x/255., [-1, 784]) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # Network Building ## Using Sequential mlp = models.Sequential() mlp.add(layers.Dense(256, activation=\u0026#39;relu\u0026#39;, input_shape=(784,))) mlp.add(layers.Dense(128, activation=\u0026#39;relu\u0026#39;)) mlp.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) print(\u0026#34;Network Built!\u0026#34;) mlp.compile(optimizer=optimizers.Adam(), loss=losses.sparse_categorical_crossentropy, metrics=[\u0026#39;accuracy\u0026#39;]) history = mlp.fit(train_x, train_y, epochs=10, batch_size=16, validation_data=(test_x, test_y), callbacks=[WandbCallback()]) # callbacks 에 Wandbcallback 추가 Check training 해당 코드를 실행시키고 W\u0026amp;B 에 접속을 하면 다음과 같은 화면이 나옵니다.\n위에서 test_project 라는 프로젝트에 세팅을 했기 때문에 확인을 해봅니다.\n하나의 프로젝트에서 한번 실행하면 뭔지 모를 조합의 Name으로 실행 상태를 보여줍니다. 저는 한번만 했기 때문에 lyric-dream-3 라는 이름으로 하나만 생겨있습니다.\n  lyric-dream-3 같이 이름을 클릭해서 무엇을 볼 수 있는지 살펴보겠습니다.\nChart 탭 Chart 탭에선 loss, epoch, metric 등과 같은 학습 관련 지표를 볼 수 있는 CHARTS와 모델 학습에 사용된 시스템을 모니터링 할 수 있는 SYSTEM이 있습니다.\n          System 탭 System 탭에서는 Chart 탭에서 SYSTEM 과 동일한 그래프를 보여주네요.   Model 탭 이름 그대로 학습하는 Model의 graph 구조를 볼 수 있습니다.   Logs 탭 여기선 학습 로그들이 저장됩니다.   Files 탭 솔직히 보여주기만 해도 좋지만 저장도 해주면 좋겠죠.\nFiles 탭에는 현재 모델을 돌릴때 사용된 정보, 가장 성능이 좋았을 epoch 의 model, 돌리는 환경에 설치된 package 리스트 등의 정보가 저장됩니다.\n물론 다운로드 가능!\nmodel-best.h5는 graph 정보도 저장되어 있기 때문에 다운로드 후 load해서 바로 Inference 가능합니다!   Weights \u0026amp; Biases 에 대해 정~~~말 간단히 알아봤습니다.\n딱히 이런저런 기능은 추가를 하지 않고 기본적인 기능만 봤습니다.\n추후에는 좀 더 다양한 기능과 프레임워크에 적용해보도록 하겠습니다.\n 200401 PyTorch 예제 추가 # wandb initialization import wandb wandb.init(project=\u0026#34;test_project\u0026#34;) # Importing Modules import numpy as np from matplotlib import pyplot as plt import torch from torch import nn, optim import torch.nn.functional as F from torchvision import utils, datasets, transforms # Loading Data # MNIST dataset mnist_train = datasets.MNIST(root=\u0026#39;./\u0026#39;, train=True, transform=transforms.ToTensor(), download=True) print(\u0026#34;Downloading Train Data Done ! \u0026#34;) mnist_test = datasets.MNIST(root=\u0026#39;./\u0026#39;, train=False, transform=transforms.ToTensor(), download=True) print(\u0026#34;Downloading Test Data Done ! \u0026#34;) # Defining Model # our model class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.linear1 = nn.Linear(784, 256) self.linear2 = nn.Linear(256, 10) def forward(self, X): X = F.relu((self.linear1(X))) X = self.linear2(X) return X model = Model() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Logging model, Gradient, parameters on dashboard wandb.watch(model) # Training Phase batch_size = 100 data_iter = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True, num_workers=1) print(\u0026#34;Iteration maker Done !\u0026#34;) # Training loop for epoch in range(10): avg_loss = 0 total_batch = len(mnist_train) // batch_size for i, (batch_img, batch_lab) in enumerate(data_iter): X = batch_img.view(-1, 28*28) Y = batch_lab y_pred = model.forward(X) loss = criterion(y_pred, Y) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() avg_loss += loss if (i+1)%100 == 0 : print(\u0026#34;Epoch : \u0026#34;, epoch+1, \u0026#34;Iteration : \u0026#34;, i+1, \u0026#34; Loss : \u0026#34;, avg_loss.data.numpy()/(i+1)) # Logging metrics  wandb.log({\u0026#39;epoch\u0026#39;: epoch+1, \u0026#34;loss\u0026#34;: avg_loss.data.numpy()/(i+1)}) print(\u0026#34;Epoch : \u0026#34;, epoch+1, \u0026#34; Loss : \u0026#34;, avg_loss.data.numpy()/total_batch) print(\u0026#34;Training Done !\u0026#34;) # Save final model torch.save(model.state_dict(), \u0026#34;model.h5\u0026#34;) wandb.save(\u0026#39;model.h5\u0026#39;) # Evaluation test_img = mnist_test.data.view(-1, 28*28).type(torch.FloatTensor) test_lab = mnist_test.targets outputs = model.forward(test_img) pred_val, pred_idx = torch.max(outputs.data, 1) correct = (pred_idx == test_lab).sum() print(\u0026#39;Accuracy : \u0026#39;, correct.data.numpy()/len(test_img)*100) # Testing r = np.random.randint(0, len(mnist_test)-1) X_single_data = mnist_test.data[r:r + 1].view(-1,28*28).float() Y_single_data = mnist_test.targets[r:r + 1] single_prediction = model(X_single_data) plt.imshow(X_single_data.data.view(28,28).numpy(), cmap=\u0026#39;gray\u0026#39;) print(\u0026#39;Label : \u0026#39;, Y_single_data.data.numpy()[0]) print(\u0026#39;Prediction : \u0026#39;, torch.max(single_prediction.data, 1)[1].numpy()[0]) ","permalink":"https://jjerry-test.github.io/blog/wandb/","tags":["Tools"],"title":"Weights \u0026 Biases 가 뭐지??"},{"categories":["DeepLearning"],"contents":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications Author: Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam\nDate: Apr 17, 2017\nURL: https://arxiv.org/abs/1704.04861\nAbstract - 모바일, 임베디드에 적용할 수 있는 네트워크.\n  1. Introduction  CNN 유명세 짱짱맨 좋은 성능(정확도)를 위해 연구가 계속 진행되었지만 속도와 메모리 측면에서 비효율적. 경량화, 효율화 필요!!  2. Prior Work  최근 경량화, 효율적인 네트워크 개발에 초점. Depthwse separable convolution,  3. MobileNet Architecture  핵심적인 레이어 설명.  3.1 Depthwise Separable Convolution  Depthwise Convolution 수행 후 Pointwise Convolution 수행.       MobileNet 에서는 3 x 3 의 Depthwise saparable convolution 사용. 연산량이 약 8~9 배로 크게 감소.  3.2 Network Structure and Training  총 28개의 레이어     MobileNet의 Convolution Block 구조     MobileNet의 대부분 파라미터, 연산 시간은 1x1 Conv     RMSprop, asynchronous gradient descent 사용. Tensorflow 로 구현.  3.3 Width Multiplier: Thinner Models  MobileNet의 기본 구조는 작고 빠르지만 더 작고 빠른 모델을 필요로 함. Width multiplier 라는 파라미터 제안.  3.4 Resolution Multiplier: Reduced Representation   4. Experiments  효율적이다 라는 내용이 전부..  4.1 Model Choices     4.2 Model Shrinking Hyperparemeters             4.3 Fine Grained Recognition   4.4 Large Scale Geolocalization   4.5 Face Attributed     4.6 Object Detection     4.7 Face Embeddings   5. Conclusion  연산량이 적고 파라미터 수도 적음! 다양한 task 에 사용가능!  ","permalink":"https://jjerry-test.github.io/blog/mobilenetv1/","tags":["Paper"],"title":"Review: MobileNet V1"},{"categories":["DeepLearning"],"contents":"Xception: Deep Learning with Depthwise Separable Convolution Author: Franc¸ois Chollet\nDate: Dec 19, 2016\nURL: https://arxiv.org/abs/1610.02357\n Abstract  CNN 에서 많이 사용되는 Inception 은 Regular Convolution 과 Depthwise Separable Convolution 의 중간 단계. Depthwise Separable Convolution 은 최대 개수의 타워를 가진 Inception module (?) Inception module을 Depthwise Separable Convolution 으로 대체한 새로운 Deep Convolutional Neural Network 제안. Xception 이라고 칭함. ImageNet 으로 학습된 Inception V3 보다 살짝 성능이 좋고 350,000,000개의 이미지와 17,000개의 클래스로 구성된 Larger image dataset 에선 Inception V3 보다 월등히 뛰어난 성능을 보임. Inception V3와 Xception 이 동일한 parameter 수를 가짐. 그러므로 연산량, 메모리의 증가가 아닌 모델 parameter를 효과적으로 사용해서 성능 향상이 된 것.  1. Introduction   Convolutional Neural Network (이하 CNN) 은 Computer Vision 에서 가장 주요한 알고리즘이 되었고, 이를 설계하는 방법에 대해 개발하는데 많은 관심을 가지게 됨.\n  Lenet -\u0026gt; AlexNet (2012) -\u0026gt; ZFNet (2013) -\u0026gt; VGG (2014) -\u0026gt; Inception 종류 \u0026hellip; -\u0026gt; Inception-ResNet (2015)\n  Inception 스타일의 기본 구성 요소는 Inception module.\n  Figure 1은 Inception V3 의 표준 Inception Module.\n  Inception 모델은 위와 같은 모듈을 Stack 한 것. VGG-Style 네트워크는 단순히 Convolution layer를 Stack.\n  실험적으로 Inception-style 이 VGG-style보다 적은 parameter로 다양한, 많은 feature를 학습 할 수 있다는 것을 보임.\n  1.1 The Inception hypothesis   Convolution layer는 3차원 공간에서 filter를 학습하려고 함.\n  Single Convolution kernel 은 채널의 correlation과 공간의 correlation 을 동시에 mapping 함.\n  Inception 모듈은 채널, 공간의 correlation 을 독립적으로 나타낼 수 있도록 연산을 분해. -\u0026gt; 쉽고 효율적인 프로세스를 만듦.\n  Inception의 가설은 채널 채널, 공간의 correlation 이 분리되어 있으므로 동시에 매핑하는 것은 좋지 않다는 것.\n  Figure 2는 3x3 conv와 1x1 conv만 사용한 단순화한 Inception 모듈.\n  Figure 3은 Figure 2의 Inception 모듈에서 하나의 큰 1x1 Convolution과 3x3 Convolution들로 재구성한 것.\n  이 방법이 Inception 의 가설보다 뛰어난 가설을 만드는 것이 합리적인 것일지, 채널과 공간을 독립적으로 매핑할 수 있는지 의문.\n  1.2 The continuum between convolutions and separable convolutions   Figure 4 처럼 Inception 모듈 구성.\n  1x1 Convolution 적용하여 채널의 correlation 매핑, 그 후 각각의 channel별로 공간의 correlation 매핑\n  이를 An “extreme” version of an Inception module 이라고 칭함.\n  TensorFlow 프레임워크에 Depthwise Separable Convolution 연산과 거의 동일함.\n  TensorFlow나 Keras 프레임워크에 있는 Depthwise Separable Convolution (Separable Convolution 라고도 불림.) 은 각 channel 별로 3x3 Convolution 적용 후 채널간의 1x1 Convolution 적용.\n  영상처리 분야에서 사용하는 Separable Convolution 과 혼동하면 안됨, 이 연산은 공간적 분리를 하는 Convolution.\n     비교 Extream Depthwise Separable     연산순서 pointwise\u0026ndash;\u0026gt;channelwise channelwise\u0026ndash;\u0026gt;pointwise   비선형성 Presence Absence    2. Prior work  VGG-16 과 같은 구조가 xception 과 유사. Inception 구조는 가지치기의 이점을 보여줌. Depthwise separable convolution는 경량화에도 적합. TensorFlow에는 이미 구현되어있음. Residual connection 을 광범위하게 사용.  3. The Xception architecture    Figure 5 와 같은 구조 제안. 처음과 마지막을 제외하곤 linear residual module 사용. 총 36개의 convolution layer로 구성. 매우 단순한 구조.  4. Experimental evaluation  Xception과 Inceotion V3 비교. Parameters가 비슷. 네트워크 규모에 대한 차이를 없애기 위함. ImageNet과 JFT dataset 이용.  4.1 The JFT dataset  그냥\u0026hellip;JFT 데이터 설명\u0026hellip; Google 데이터 중 하나 JFT 로 학습, FastEval14k dataset으로 성능 비교.  4.2 Optimization configuration  각 방법에 대해서 다음과 같은 설정으로 Xception, Inception V3 모두 학습 On ImageNet  Optimizer: SGD Momentum: 0.9 Initial learning rate: 0.045 Learning rate decay: 0.94 (every 2 epochs)   On JFT  Optimizer: RMSprop Momentum: 0.9 Initial learning rate: 0.001 Learning rate decay: 0.9 (every 3,000,000 samples)    4.3 Regularization configuration  Weight decay Dropout Auxiliary loss tower  4.4 Training infrastructure  80개의 NVIDIA K80 GPU \u0026hellip;. ImageNet 학습시 synchronous gradient descent을 적용하여 data parallelism 이용. → 3일 소요 JFT 학습시 asynchronous gradient descent을 적용하여 data parallelism 이용. →한달 소요  4.5 Comparison with Inception V3 4.5.1 Classification performance   두 데이터 모두 Xception이 좋은 성능을 보임.\n  4.5.2 Size and speed   Parameter가 늘지 않으면서 성능 향상을 보이기에 Xception이 효율적인 모델.\n  4.6 Effect of the residual connections  Residual connection에 대한 ablation study 진행. Residual connection의 중요성을 보여줌.    4.7 Effect of an intermediate activation after point wise convolutions   Depthwise separable convolution은 depthwise → pointwise convolution으로 구성되어있음.\n  그 중간에 activation function에 대한 ablation study 진행.\n  Inception module에 대한 연구와 반대되는 결과 도출.\n  5. Future directions  Depthwise separble convolution 이 만능이라는 보장은 없음.  ","permalink":"https://jjerry-test.github.io/blog/xception/","tags":["Paper"],"title":"Review: Xception"},{"categories":["Python"],"contents":"파이썬, 딥러닝을 하시는 분들 중 Jupyter 부류를 이용하시는 분들이 꽤 많습니다. Jupyter Notebook, Jupyter Lab,...\n저는 둘 다 사용해보긴 하지만 주로 Jupyter Lab 을 사용합니다.\n이유는 그냥\u0026hellip;좀 더 보기 편해서..?\n두 환경의 차이는 다음 링크에서 확인해주세요.\n Jupyter Notebook Jupyter Lab  참고사항 https://github.com/jupyterlab/jupyterlab/issues/7122\n며칠 전 이슈에 올라온 상황입니다.\njupyter lab 1.0.2 버전에 extension을 설치하면 sidebar 부분이 깨지는 버그가 있네요\u0026hellip;\n1.1.1 에선 고쳐졌다고 합니다.\n자, 다시 본론으로 돌아가봅니다.\nJupyter Lab 에서 단축키로 설정할 수 있는 기능들은 다음과 같습니다.\n\u0026lsquo;notebook:create-new\u0026rsquo;\n\u0026lsquo;notebook:interrupt-kernel\u0026rsquo;\n\u0026lsquo;notebook:restart-kernel\u0026rsquo;\n\u0026lsquo;notebook:restart-clear-output\u0026rsquo;\n\u0026lsquo;notebook:restart-run-all\u0026rsquo;\n\u0026lsquo;notebook:reconnect-to-kernel\u0026rsquo;\n\u0026lsquo;notebook:change-kernel\u0026rsquo;\n\u0026hellip; (너무 많다\u0026hellip;.)\n\u0026lsquo;notebook:hide-all-cell-outputs\u0026rsquo;\n\u0026lsquo;notebook:show-all-cell-outputs\u0026rsquo;\n\u0026lsquo;notebook:enable-output-scrolling\u0026rsquo;\n\u0026lsquo;notebook:disable-output-scrolling\u0026rsquo;\n\u0026lsquo;notebook:save-with-view\u0026rsquo;\n너무 많은 관계로 링크 참고해주세요.\n이번 포스팅에선 예시로 Restart and Clear 에 대해서 shortcut을 설정해보겠습니다.\n설정 방법 먼저 Jupyter Lab 을 실행시켜주세요.\n  Advanced Setting Editor 를 클릭해주세요.\n  좌측에 Keyboard Shortuts 를 선택해주세요.\n  여기서 왼쪽의 창은 default setting에 대한 설명이고 오른쪽을 Custom 을 위해 기입하는 부분입니다.\n그럼 예시대로 Restart and Clear 에 대해서 shortcut을 설정해보겠습니다.\n위에 기능에 대해서 링크 올려드렸었죠? 그 깃헙에서 링크에 대한 내용을 먼저 찾아봅니다.\n  거기서 \u0026lsquo;{기능 이름}\u0026rsquo; 부분을 복사해주세요.\n그리고 다음과 같이 기입해줍니다.\n  그리고 우측 상단에 저장버튼을 누르고 노트북에서 Shift + Command + C 를 눌렀을때 Restart and Clear 이 작동하게 됩니다.\n사진만 보고 따라 적으시기 귀찮으실테니 code block 으로 남깁니다.\n\u0026quot;shortcuts\u0026quot;: [ { \u0026quot;command\u0026quot;: \u0026quot;kernelmenu:restart-and-clear\u0026quot;, \u0026quot;keys\u0026quot;: [\u0026quot;Shift + Ctrl + C\u0026quot;], \u0026quot;selector\u0026quot;: \u0026quot;[data-jp-kernel-user]:focus\u0026quot; }] 과정을 요약해드리면 (Jupyter Lab 켜기 ~ Editor 열기 까지는 생략)\n 기능에 대한 링크에서 자신이 원하는 기능 찾기 위에 code block 에서 \u0026ldquo;command\u0026rdquo;: 부분에 \u0026lsquo;{기능 이름}\u0026rsquo; 기입 \u0026ldquo;keys\u0026rdquo;: 에 원하는 커맨드 넣기 (복수개 가능) \u0026ldquo;selector\u0026rdquo;: 에 .jp-Notebook:focus\u0026quot; 혹은 \u0026ldquo;[data-jp-kernel-user]:focus\u0026rdquo; 기입 저장 후 즐겁게 사용!  shortcut 설정은 여기까지 입니다.\n혹시 이외에 다른 설정에 대해 궁금한거 있으시면 comment 남겨주세요!\n참고사항  만약 A 기능은 Shift + Alt + C 이고 B 기능은 Alt + Shift + C 일때\u0026hellip; 전체적으로 보면 같은 키 입력이지만 순서가 다르기 때문에 별개의 shortcut으로 작동합니다.\n ","permalink":"https://jjerry-test.github.io/blog/jupyter_shortcut/","tags":["Setting"],"title":"jupyter Lab 에서 단축키 설정하는 방법!"},{"categories":["Python"],"contents":"Anaconda 를 사용하다보면 여러 가상환경을 만들게 됩니다. (아닐수도 있구요\u0026hellip;)\n그 후에 jupyter 를 사용하시는 분들이라면 대부분 이렇게 사용하실 겁니다.\nconda activate 환경이름 jupyter notebook conda activate 환경이름 이라는걸 무.조.건 써줘야하죠..\n이게 매우 귀찮았습니다\u0026hellip;\n \u0026ldquo;activate 없이 base에서 jupyter notebook을 실행해도 가상환경을 잡을 수 있는 방법이 없나..\u0026rdquo;\n 이런 생각 많이들 하실 것 같습니다.\n당연히 방법이 있습니다!\n그 방법에 대해 알려드리겠습니다.\nconda activate {환경이름} python -m ipykernel install --user --name {환경이름} --display-name {Jupyter에 표시될 이름} 이렇게 하시면 됩니다. 예시를 직접 보여드리겠습니다.\n  저는 base 환경만 썼습니다. 가상환경을 가볍게 하나 만들도록 할게요!\n  test라는 가상환경을 만들었습니다. 먼저 kernel을 추가하지 않고 jupyter notebook을 실행해보겠습니다.\n    첫번째 사진을 보시면 Python 3 만 나오는걸 보실 수 있습니다. 저 Python 3 는 base의 Python을 가리킵니다.\nbase에는 제가 tensorflow를 설치해놨기 때문에 import 가 잘 작동하는군요..\n그럼 kernel을 추가해보겠습니다. (일단 test 환경에 ipython 이 안깔려있어서 설치함..)\n    커널을 추가하고 나면 Installed ~~~~ 라는 메세지를 보실 수 있어요!\n그럼 다시 base로 돌아가서 jupyter notebook을 실행해보겠습니다.\n    첫번째 사진을 보시면 추가하기 전과는 다르게 test 라는 항목이 추가되었습니다!\n실행을 해봐도 tensorflow 모듈이 없다고 나오는걸 확인하실 수 있습니다!\n앞으로는 conda activate ~~를 안하셔도 되요! (뿌듯)\n이번 포스팅은 여기까지 입니다.\n많은 분들께 도움이 되었으면 좋겠네요!\nPS. jupyter notebook 우측 상단쪽에 어떤 커널로 실행하고 있는지 표시가 됩니다. (Logout 아래)\n","permalink":"https://jjerry-test.github.io/blog/multiple_kernel/","tags":["Setting"],"title":"jupyter (ipython) 여러 커널 사용하기!"},{"categories":["Python"],"contents":"예~~전에 NIfTI 파일을 load 하는 방법을 올렸었습니다!\n이번에는 Python에서 DICOM 포맷의 데이터를 load 하는 방법에 대한 포스팅을 해보려고 합니다.\n가장 먼저 관련 패키지인 Pydicom 을 설치를 해줍니다.\nconda install -c conda-forge pydicom 저번과 똑같이 단순한 설치방법!\n이제 코딩으로 읽어보겠습니다.\n예시 데이터로 다음 링크에 있는 영상을 이용하였습니다.\nimport pydicom as di from matplotlib import pyplot as plt data = di.read_file(\u0026#34;.dcm 경로\u0026#34;) #data = di.dcmread(\u0026#34;.dcm 경로\u0026#34;) # 편한거 쓰시면 됩니다. img = data.pixel_array plt.imshow(img) # 슬라이스 1장일 경우 #plt.imshow(img[:,:,\u0026#34;slice 번호\u0026#34;]) # 슬라이스가 여러 장일 경우 plt.show() 이런 식으로 작성하시면 됩니다.\n예시를 보여드리면\n  예시 영상이 좀 작네요\u0026hellip;\n어쨌든 이런 식으로 읽습니다.\n흠\u0026hellip;.NIfTI와 DICOM을 했으니\u0026hellip;다음엔 Insight Meta-Image 를 해보겠습니다!\n","permalink":"https://jjerry-test.github.io/blog/dicom/","tags":["Usage"],"title":"Python으로 DICOM 영상을 읽어보자!"},{"categories":["DeepLearning"],"contents":"Deep Generative Adversarial Networks for Thin-Section Infant MR Image Reconstruction  Jiaqi Gu1, Zezu Li1, YuanYuan Wans1, 3, Haowei Yang2, Zhongwei Qiao2, and Jinhua Yu1, 3 1School of Information Science and Technology, Fudan University, Shanghai 200433, China\n2The Children\u0026rsquo;s Hospital of Fudan University, Shanghai 201102, China\n3Key Laboratory of Medical Imaging Computing and Computer Assisted Intervention of Shanghai, Department of Electronic Engineering, Institute of Functional and Molecular Medical Imaging, Fudan University, Shanghai 200433, China   Abstract  Thin section magnetic resonance images (Thin MRI) 는 뇌수술, 뇌 구조 분석에 좋은 영상. 하지만 Thick section magnetic resonance images (Thick MRI) 에 비해 imaging cost가 많이 들기 때문에 잘 사용되지 않음. Thick MRI 2 Thin MRI 제안. Two stage( GAN -\u0026gt; CNN )로 구성하였고 Thick MRI의 Axial, Sigittal plane을 이용하여 Thin MRI의 Axial reconstruction. 3D-Y-Net-GAN 은 Axial, Sagittal Thick MRI 를 이용하여 Fusion. 3D-Dense U-Net은 Sagittal plane에 대해 세부적인 calibrations, structual correction 제공. Loss function 은 structual detail을 Network가 capture 할 수 있도록 제안. bicubic, sparse representation, 3D-SRU-Net 과 비교. 35번의 Cross-validation, 114개를 이용하여 두개의 testset 구성.  PSNR : 23.5 % 증가. SSIM : 90.5 % 증가. MMI : 21.5 % 증가.    Introduction  Thin MRI 는 slice thickness가 1mm이고 sapcing gap이 0mm. 하지만 항상 Thin MRI를 사용할 수 없음. 일반적으로 사용하는 Thick MRI는 slice thickness가 4~6mm 이고 sapcing gap이 0.4~1mm.  해상도 : Thin MRI \u0026gt; Thick MRI   인간의 뇌 발달에 대한 insight를 주기 때문에 유아의 brain MR image는 어른의 brain MR image 보다 연구에 가치가 있음 하지만 유아의 MR image를 얻는게 쉽지 않음. 그래서 Thick to Thin 제안. 기존 traditional interpolation algorithm  시각적으로는 성능이 좋아보임. 하지만 성인의 brain 에 초점을 맞춤. Interpolation-based super-resolution reconstruction: effects of slice thickness Evaluation of interpolation effects on upsampling and accuracy of cost functions-based optimized automatic image registration   Frame interpolation 방법과 같이 적용 가능.  Slice Interpolation in MRI Using a Decomposition-reconstruction Method   Super-resolution 문제로 적용할 수도 있음.  Image super-resolution via sparse representation   CNN, GAN 이 발전하면서 super-resolution 이 탄력을 받음.  [Context-Sensitive Super-Resolution for Fast Fetal Magnetic Resonance Imaging] [Deep Generative Adversarial Neural Networks for Compressive Sensing MRI]   이전에 성인의 Thick MRI를 Thin MRI 로 reconstruction 하는 3D-SRGAN 제안했으나 axial plane만 고려했음. [Reconstruction of Thin-Slice Medical Images Using Generative Adversarial Network] Deep Learning 이 reconstruction performance 뿐 아니라 reconstruction time 감소에도 매우 효과적인걸 보임.  Proposed Method A. Overview  CNN은 기존에도 super-resolution에서 많이 사용됨. 하지만 최근까지 제안된 Network는 대부분 2D image에 대한 upscaling. 몇몇 Network는 3D image로 확장했지만 그렇게 효과를 보지 못했음. 이 논문의 Flow       B. Network Architecture  First stage는 3D-Y-Net-GAN 으로 Thick MRI를 Thin MRI로 생성 후 3D-DenseU-Net으로 recalibration.  3D-Y-Net-GAN  Input : Axial, Sagittal Thick MRI Output : Thin MRI r : Upscaling Factor ( r = 8 일 경우의 예시 )      Feature Extraction Branches\n 각 input에 대한 feature 추출. Maxpooling layer에서 [1, 2, 1], [2, 1, 1]의 다른 strides factor 적용. 3D convolutional layer 는 Convolution + Batch Normalization + Swish 로 구성.  Swish는 Activation 의 종류로 ReLU로 인해 생기는 Dead neuron을 극복할 수 있음. -\u0026gt; 근데 굳이 왜 swish일까\u0026hellip;   layers 를 거친 후 shape 의 변화  Axial : [H, W, S, 1] -\u0026gt; [H/2, W/2, S, 32] Sagittal : [H, W, r*S, 1] -\u0026gt; [H/2, W/2, S, 32]   Axial과 Sagittal의 shape이 다르기 때문에 Sagittal 에 대해서 preprocessing으로 3개의 3d convolution layer 적용.    Feature Fusion Branch\n 두 feature를 channel 방향으로 Concatanation. W 방향으로 Upsampling 후 H 방향으로 Downsampling feature 를 Concatanation. H 방향으로 Upsampling 후 첫번째 Block의 Feature map을 Concatanation U-Net 에서 아이디어를 얻었고 structual alignment, gradient-vanishing 등을 완화.    Reconstruction Branch\n Figure 3 (b) 와 같은 구조. Upsampling layer 3개를 연속으로 붙여서 8배 확장하는 구조 대신에 Multipath upscaling strategy 적용. -\u0026gt; Artifact 완화 효과\u0026hellip;?    Discriminator    Axial Image, Saggital Image, Combination Image 가 Real Pair인지 Fake Pair인지 감별. Input : \\((I^A, I^Y, I^S), (I^A, I^{GT}, I^S)\\) Output : Real, Fake    3D-DenseU-Net    전체적인 구조는 U-Net이지만 2개의 Enhanced residual block 을 적용하여 detail recalibration. Input : \\(I^Y, I^S, I^{YA}\\) -\u0026gt; 어떻게 3개가 input으로\u0026hellip;? Output : Thin MR Image \\(I^A\\) 를 \\(I^Y\\) 의 해당 위치에 insertion 하여 \\(I^{YA}\\) 생성. -\u0026gt; 아직 이해 X..  Axial Information 을 이용하여 정확한 axial 을 만들기 위해\u0026hellip; \\(I^S\\) 를 \\(I^Y\\) 에 insertion하게 되면 Sagittal 에 대한 information 이 과해지기 때문에 Reconstrtion Axial Image의 Quality 가 안좋아 질 것!   End-to-End 가 아니라 각각 따로따로 학습. -\u0026gt; Faster RCNN 과 같은 방식으로 할런지\u0026hellip;.?  Loss Function  \\(G\\) 는 generator 라는 의미. Self-Adaptive Charbonnier Loss  일반적으로 많이 사용되는 \\(\\ell2\\) 전반적으로 Smoothing 하게 만들어지고 \\(\\ell1\\) 은 GT와 Prediction 의 차이로 indiscriminate 하게 학습. Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution에 따르면 Charbonnier loss(미분가능한 $\\ell1$의 분산)가 \\(\\ell1, \\ell2\\) 보다 성능이 뛰어남. Deep Learning for Isotropic Super-Resolution from Non-Isotropic 3D Electron Microscopy 에 따르면 Cubic-weighted mean square error 가 Generated 영상과 Ground truth 간의 차이가 큰 \u0026ldquo;어려운\u0026rdquo; 부분의 성능을 강조. 다음과 같은 Loss 제안. \\(\\epsilon\\) 은 default로 \\(10^{-6}\\)    $$L^G_{SC} = \\frac{1}{rLWH}\\sum_{x,y,z=1,1,1}^{L,W,rH}\\sqrt{(I^{GT}_{x,y,z}-I^Y_{z,y,z})^2+\\epsilon}\\cdot\\bigg(\\frac{1}{2}+\\frac{(I^{GT}_{x,y,z}-I^Y_{z,y,z})^2}{2max((I^{GT}-I^Y)^2)}\\bigg)$$\n 3-D Gradient Correction Loss  Charbonnier Loss는 Pixelwise difference에 대한 Loss, Gradient에 대한 손실을 줄 수 있음. 다음과 같이 각 axis에 대한 Gradient 를 이용하여 Loss 제안.    $$L^G_{GC} = \\mathbb{E}[(\\nabla_{x}I^{GT}_{x,y,z} - \\nabla_{x}I^Y_{x,y,z})^2] \\\\ + \\mathbb{E}[(\\nabla_{y}I^{GT}_{x,y,z} - \\nabla_{y}I^Y_{x,y,z})]^2 \\\\ + \\mathbb{E}[(\\nabla_{z}I^{GT}_{x,y,z} - \\nabla_{z}I^Y_{x,y,z})^2]$$\n Adversarial Loss  LSGAN Loss 사용.    $$L^D=\\frac{1}{2}\\mathbb{E}[(D(I^{GT}, I^A, I^S)-1)^2+D(I^Y, I^A, I^S)^2]$$\n$$L^G_{AD}=\\mathbb{E}[(D(I^Y, I^A, I^S)-1)^2]$$\n \\(\\ell_2\\) Weight Regularization Loss  (Loss는 아니지만\u0026hellip;) Overfitting을 방지하기 위해 사용.    $$L^G_{WR} = \\sum\\Vert W_G\\Vert^2_2$$\n  3D-Y-Net-GAN Loss\n \\(L_G = L^G_{SC} + \\lambda_1L^G_{GC} + \\lambda_2L^G_{AD} + \\lambda_3L^G_{WR}\\)    3D-DenseU-Net Loss\n \\(L = L_{SC} + \\lambda_1L_{GC} + \\lambda_3L_{WR}\\)    Experimental Result   Multiplanar 의 효율성을 검증하기 위해 다음과 같이 세 가지 경우로 나눔.\n Axial, Sagittal 둘 다 이용. Axial 만 이용. Saigittal 만 이용.    Loss function을 검증하기 위해 네 가지 경우로 나눔.\n \\(\\ell1norm + L_{GC} + L_{AD} + L_{WR}\\) (pixelwise loss를 \\(\\ell1norm\\)으로 대체.) \\(L_{SC} + L_{GC} + L_{WR}\\) \\(L_{SC} + L_{AD} + L_{WR}\\) \\(L_{SC} + L_{GC} + L_{AD} + L_{WR}\\)    Evalutaion Method 로는 아래와 같이 네 가지 기법과 자신들의 Network\n Bicubic interpolation Sparse representation 3D-SRU-Net 3D-Y-Net-GAN 3D-Y-Net-GAN + 3D-DenseU-Net    Metrics으로는 다음 세 가지 사용.\n PSNR(Peak Signal-to-Noise Ratio)    $$ \\begin{alignedat}{2} MAX_I = 255 \\\\ PSNR = 20\\cdot\\log_{10}\\Bigg(\\frac{MAX_I}{\\sqrt{\\frac{1}{rLWH}\\sum_{x, y, z}(I^R_{x,y,z}-I^{GT}_{x,y,z})^2}}\\Bigg) \\end{alignedat} $$\n SSIM(Structural SIMilarity)  $$L=255(\\text{dynamic\\ range})$$ $$\\mu:\\text{Variance} $$ $$\\mu_{ab}:\\text{Covariance} $$ $$c_1=(k_1L)^2 $$ $$c_2=(k_2L)^2 $$ $$SSIM=\\frac{(2\\mu_a\\mu_b+c_1)(2\\sigma_{ab}+c_2)}{(\\mu_a^2+\\mu_b^2+c_1)(\\sigma_a^2+\\sigma_b^2+c_2)}$$\n NMI(Normalized Mutual Information)  $$H(X) = -\\sum_{x_i}\\in{X} p(x_i)\\log {p(x_i)}$$ $$H(X, Y) = -\\sum_{y_i\\in{Y}} \\sum_{x_i\\in{X}}p(x_i, y_i)\\log{p(x_i, y_i)}$$ $$NMI(X, Y) = 2\\frac{H(X) + H(Y) - H(X, Y)}{H(X)+H(Y)}$$\n pixel 값을 [-1, 1]로 clipping -\u0026gt; 다시 8-bit gray scale로 변환. Generated MR images 와 Ground truth가 비슷할 수록 높은 값을 가짐.  A. Data and Preprocessing  총 154 samples의 2~5세 유아 Axial, Sagittal Thick MRI, Axial Thin MRI     Table 1. 과 같은 parameter 사용. Dataset 분할  Cross Validation Dataset : 40 samples Test 1 Dataset : 65 samples Test 2 Dataset : 49 samples   Preprocessing  각 영상별로 다른 parameter를 가지고 있고 intensities 도 다양하기 때문에 spatial misalignment, intensity imblance를 발견. Registration을 위해 SPM12 를 이용하여 unified spatial normalization 수행.  DICOM to NIfTI Segment gray matter, white matter, cerebrospinal fluid, skull, scalp, and air mask. Nonlinear deformation field ICBM Asian brain template in affine regularization   Grayscale Normalization  MRI 는 16 bit.. 단순 linear transformation 으로 [-1, 1]로 mapping.   Histogram Matching  고정된 샘플을 reference로 histogram matching 적용. histogram imbalance 제거.     Data Augmentation  Radial Transformation  Image Augmentation using Radial Transform for Training Deep Neural Networks   Mirror Reflection    B. Experimental Settings   5-fold cross-validation 적용.\n  35 개중 랜덤으로 28:7로 training:validation . -\u0026gt; 앞에선 40개라더니..?\n  Training 3D-Y-Net-GAN\n Batch Size : 16 Epochs : 200 Adam Optimizer Parameter  \\(\\beta_1\\): 0.9 Learning rate schedule  Initial value : 5*10-4 Decay Step : 252 Decay rate : 0.989     $\\lambda_1, \\lambda_2, \\lambda_3$ : 0.2, 0.02, 0.1 He initializer    Training 3D-DenseU-Net\n Batch Size : 12 Epochs : 300 Adam Optimizer Parameter  \\(\\beta_1\\): 0.9 Learning rate schedule  Initial value : 5*10-4 Decay Step : 373 Decay rate : 0.989     \\(\\lambda_1, \\lambda_3\\) : 1, 0.001 He initializer    SR Parameter\n Dictionary size = 512 Patch number = 100,000 Patch size = 13 x 13 Sparsity Regularization = 0.15 Overlap = 12.    Training 3D-SRU-Net\n Batch Size : 32 Epochs : 300 Adam Optimizer Parameter  \\(\\beta_1\\): 0.9 Initial value : 5*10-4      C. Ablation Experiment On Input Data  Input을 변경하면서 실험 진행.      Axial 과 Sagittal 을 같이 사용했을 때가 좀 더 세부적인 구조, 적은 왜곡을 보임.  두 축의 영상이 서로 조합하여 reconstruction task를 향상.   Quantitive evaluation 에서도 더 높은 지표를 산출.  D. Ablation Experiment On Loss Function  Loss를 변경하면서 실험 진행.      Self-Adaptive Charbonnier Loss에 비해 \\(\\ell1 norm\\) 이 흐린 영상을 생성. Without Gradient Correction Loss  덜 선명한 영상을 생성.   Without Adversarial Loss  덜 realistic 영상을 생성. -\u0026gt; ?????그냥 쓴 말인가..   Table3 \u0026hellip;지표 좀 이상..  E. Comparison With Other Methods  다른 Method들과 비교.      제안한 method로 생성된 image가 가장 Realistic하고 Ground truth 와 가장 비슷하다고 함. 대부분 Quantitative evaluation 에서 제안한 method가 다른 것들을 다 뛰어넘음.  Conclusion  제안한 Method 에선 Data preprocessing이 매우 중요하다\u0026hellip;\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/thin/","tags":["Paper"],"title":"Review: MRI interpolation using Deep Learning"},{"categories":["Python"],"contents":"이번 포스팅은 제가 많이 쓰는 패키지들에 대해서 적어보려고 합니다.\n추후에 포맷하고 다시 세팅할 수도 있으니까\u0026hellip;.\n저는 Anaconda가 아닌 Miniconda를 사용하기 때문에 어지간하면 하나 하나 설치를 해줘야해요. 물론 머리로는 기억하고 있지만 커맨드 쓰기가 귀찮으니\u0026hellip;적어놓으려고 합니다!\n각각 설명은\u0026hellip;생략할거에요. 보시는 분들 구글링 실력을 믿습니다.\nScientific uses  numpy, scipy, pandas conda install numpy scipy pandas  Visualization  matplotlib, seaborn conda install matplotlib seaborn  Image Processing  pillow, scikit-image, opencv conda install pillow scikit-image opencv  ML \u0026amp; DL  scikit-learn, tensorflow, pytorch (keras도 썼었는데\u0026hellip;이번에 뺐어요..) conda install scikit-learn tensorflow conda install -c pytorch pytorch torchvision  Medical Image Processing  pydicom, nibabel, simpleitk conda install -c conda-forge pydicom nibabel conda install -c SimpleITK SimpleITK  Tools  tqdm, jupyter, jupyter-lab conda install tqdm jupyter jupyterlab  이\u0026hellip;.정도네요!\n솔직히 제가 이렇게 포스팅을 하는 이유는\u0026hellip;.\n이미 제 노트북을\u0026hellip;한번 갈아엎었습니다\u0026hellip;\nconda 를 4.7.5 로 올리는 순간\u0026hellip;모든게 날아갔거든요.\n하\u0026hellip;얼른 세팅하러 갑니다..\nP.S conda 를 조심하세요\u0026hellip;\n","permalink":"https://jjerry-test.github.io/blog/conda_package/","tags":["Setting"],"title":"(개인적으로) 무.조.건 설치하는 Anaconda 패키지"},{"categories":["Python"],"contents":"Index  Index  A. Visualizing statistical relationships  Relating variables with scatter plots Emphasizing continuity with line plots  Aggregation and representing uncertainty Plotting subsets of data with semantic mappings Plotting with date data   Showing multiple relationships with facets      A. Visualizing statistical relationships Relationships 시각화에선 relplot() 이라는걸 주로 사용합니다. relplot() 에는 kind라는 옵션으로 scatter, line 을 그릴 수 있습니다.\n일단! 다음과 같이 기본적인 패키지를 import 하고 진행하겠습니다!\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026#34;darkgrid\u0026#34;) # 추후 Part 3 에서 다룰 내용! Relating variables with scatter plots Scatter plot을 해볼겁니다!\nscatterplot()을 사용해도 되지만 relplot()을 사용해서 그려보겠습니다.\n가장 먼저 \u0026hellip; 사용할 데이터를 읽어오겠습니다.\ntips = sns.load_dataset(\u0026#34;tips\u0026#34;) tips.head(5) 위를 실행하면 다음 사진과 같이 출력이 나옵니다.\n  정확히 무슨 데이터인지는 모르겠지만\u0026hellip; 어떤 가게의 가계부\u0026hellip;? 같습니다. 총 금액, 팁, 성별, 흡연 여부, 요일, 시간대, 크기(\u0026hellip;?) 등의 카테고리가 있네요.\n그럼 한번 plot 해보겠습니다.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips);   다음과 같은 figure가 나오네요!\nx, y, data에 들어간 의미를 알아볼까요? x= 는 x축을 어떤 데이터로 할지 정하는 부분입니다. y= 는 당연히 y축이겠죠? data= 는 어떤 data를 plot에 사용할지 적는 부분입니다.\n그래서 plot하는 코드를 풀어서 얘기해보자면\ntips라는 DataFrame에서 total_bill을 x축으로 tip을 y축으로 골라서 scatter plot 하라는 얘기입니다.\n좀 더 해볼까요?\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, data=tips);   x, y, data는 위랑 다를게 없는데\u0026hellip; hue라는 옵션이 추가가 되었습니다.\nhue='smoke'라고 흡연 여부에 대해 색상으로 표시를 해줍니다!\n하나 또 있습니다.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, style=\u0026#34;smoker\u0026#34;, data=tips);   style 옵션이 추가되었어요! style='smoker'를 추가해주면서 plot 스타일이 추가되었네요!\n다음 사진은 style=\u0026quot;time\u0026quot; 옵션을 준거에요!\n  색으로는 흡연여부를 표시하고 plot 스타일로는 시간대를 표시해주네요!\n자, hue 옵션의 또 다른 예시입니다.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;size\u0026#34;, data=tips);   이전 예시에선 Yes or No 였죠.\n이번 수치에 대해 색별로 의미를 줬습니다!\n다음과 같이 pallette 옵션을 줘서 색을 바꿀 수도 있어요!\npallette에 대한 자세한건 Part 3 에서 다루겠습니다.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;size\u0026#34;, palette=\u0026#34;ch:r=-.5,l=.75\u0026#34;, data=tips);   오\u0026hellip;scatter plot의 끝이 보여요\u0026hellip;\nsize 옵션입니다.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, size=\u0026#34;size\u0026#34;, data=tips);   size=\u0026quot;size\u0026quot;라고 옵션을 줬어요!\nplot의 크기에 따라 의미가 나뉘어졌습니다!\n다음고 같이 sizes를 이용해서 크기 범위를 정해줄수도 있네요!\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, size=\u0026#34;size\u0026#34;, sizes=(15, 200), data=tips);   Emphasizing continuity with line plots 이 파트는 연속적인 값을 가진 데이터에 대한 plot입니다.\n다음과 같이 DataFrame을 하나 만들어 봅시다!\ndf = pd.DataFrame(dict(time=np.arange(500), value=np.random.randn(500).cumsum())) df.head(5)   결과는 사진과 다를 수 있어요!\ntime, value 를 가지고 있다는 거에 초점을 맞추시면 됩니다!\n일별로 뭔가 값이 들어가 있네요.\n이를 relplot() 을 이용하여 그려보겠습니다. kind 옵션에 \u0026quot;line\u0026quot; 라고 주면 됩니다!\nautofmt_xdata() 라는건 이름에서 유추할 수 있듯이 x 축 레이블을 데이터에 맞게 format을 자동으로 맞춰주는거에요!\n지금은 x 축이 날짜니까 날짜에 대해선 x 축 레이블을 살짝쿵 기울인거에요!\n이걸 쓰지 않으면 날짜가 겹쳐질거에요\u0026hellip;(한번 해보세요..)\ng = sns.relplot(x=\u0026quot;time\u0026quot;, y=\u0026quot;value\u0026quot;, kind=\u0026quot;line\u0026quot;, data=df) g.fig.autofmt_xdate()   위 예시는 x 축이 연속적이지만 시계열 데이터였습니다!\n만약 연속적이지만 이동경로와 같이 x, y 축 좌표만 가진 데이터라면?\n예시를 들어볼게요! 다음과 같이 x, y 에 대한 데이터를 만듭니다.\ndf = pd.DataFrame(np.random.randn(500, 2).cumsum(axis=0), columns=[\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;]) df.head(5)   이런 위치에 대한 데이터는 sort = False 옵션을 줘야합니다!\n그렇지 않으면 \u0026hellip;. x 데이터를 자동으로 sorting 해서 plot 해버립니다..\nsns.relplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, sort=False, kind=\u0026#34;line\u0026#34;, data=df);   Aggregation and representing uncertainty 이 파트는 집계 및 신뢰구간 표시에 대한 설명입니다.\nseaborn은 기본 값으로 95% 신뢰구간을 표시해준다고 하녜요!\n다음과 같이 데이터를 load 해줍니다!\nfmri = sns.load_dataset(\u0026#34;fmri\u0026#34;) fmri.head(5)   fmri data 군요! 일단 무슨 의미인지 잘 모르겠습니다!\n각 환자별로 timepoint 를 가지고 signal을 가지는건 알겠네요..\n이 데이터를 이용해 다음과 같이 plot을 해봅시다!\n뭔가 선이랑 범위(?)로 보이는 불투명한 부분이 생겼습니다!\n선은 해당 timepoint 에서 signal 의 mean 값이고 불투명한 부분은 신뢰구간을 의미합니다!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, kind=\u0026#34;line\u0026#34;, data=fmri);     \u0026ldquo;그럼\u0026hellip;이런 데이터는 항상 신뢰구간을 같이 봐야하는가?\u0026rdquo;\n  아닙니다. ci(Confidence Intervals) 옵션을 바꿔주면 됩니다!\nci=None 이라고 옵션을 주고 plot을 하면 신뢰구간이 표현이 안됩니다!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, ci=None, kind=\u0026#34;line\u0026#34;, data=fmri);   또 다른 옵션이 있습니다.\n데이터가 매우 큰 경우! ci 옵션에 표준 편차를 넣어주면 각 timepoint 에서 분산을 표현해줍니다!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, kind=\u0026#34;line\u0026#34;, ci=\u0026#34;sd\u0026#34;, data=fmri);   마지막으로\u0026hellip; 이런 집계말고 정말 원본 그대로 plot 해보고 싶다면!\nestimator=None이라고 주면 됩니다.\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, estimator=None, kind=\u0026#34;line\u0026#34;, data=fmri);   Plotting subsets of data with semantic mappings 이 파트에선 relplot() 을 이용해서 scatter plot, line plot을 동시에 표현합니다.\n데이터는 위 파트에서 썼던 fmri 데이터를 사용합니다!\n그리고 위 파트와 같이 relplot(kind=\u0026quot;line\u0026quot;)을 쓰지만 hue=\u0026quot;event\u0026quot;라는 옵션을 주도록합니다!\n한번에 scatter plot과 line plot이 표현되네요!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;event\u0026#34;, kind=\u0026#34;line\u0026#34;, data=fmri);   그럼\u0026hellip;style도 추가해보도록 하죠!\nhue=\u0026quot;region\u0026quot;, style=\u0026quot;event\u0026quot;라고 옵션을 줍니다!\n점점 표현해주는게 많아지죠?\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;region\u0026#34;, style=\u0026#34;event\u0026#34;, kind=\u0026#34;line\u0026#34;, data=fmri);   또한 선 스타일과 마커 옵션을 줄 수도 있어요!\ndashes=False, markers=True를 추가해보세요!\n점선이 모두 실선이 되었고 마커가 추가되었습니다!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;region\u0026#34;, style=\u0026#34;event\u0026#34;, dashes=False, markers=True, kind=\u0026#34;line\u0026#34;, data=fmri);   괜히 정보가 많아지면\u0026hellip;해석이 겁나 어려워질 수 있습니다..\n다음과 같이 단순화하는게 좋을 때도 있습니다!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;event\u0026#34;, style=\u0026#34;event\u0026#34;, kind=\u0026#34;line\u0026#34;, data=fmri);   필요에 따라 모든 샘플 각각을 plot 해야하기도 하죠..\n다음 예시는 event가 stim 인 데이터들을 subject 별로 plot을 합니다!\n(저 plot이 되는 방법은 추후에 좀 더 자세히 설명을 적을게요!)\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;region\u0026#34;, units=\u0026#34;subject\u0026#34;, estimator=None, kind=\u0026#34;line\u0026#34;, data=fmri.query(\u0026#34;event == \u0026#39;stim\u0026#39;\u0026#34;));   여기서는 Colormap을 조금씩 다뤄보는 구간입니다.\n다음과 같이 데이터를 load 합니다!\n(뭔지는 모르겠습니다\u0026hellip;)\ndots = sns.load_dataset(\u0026#34;dots\u0026#34;).query(\u0026#34;align == \u0026#39;dots\u0026#39;\u0026#34;) dots.head(5)   지금까지 해왔던거 처럼 plot을 해봅니다.\nsns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, hue=\u0026#34;coherence\u0026#34;, style=\u0026#34;choice\u0026#34;, kind=\u0026#34;line\u0026#34;, data=dots);   음\u0026hellip;.근데 위에 사진이 뭔가 부족하다고 느껴집니다.\n자세히 보시면 그려지는 실선, 점선은 각각 6개인데 표현된 색상은 4개네요\u0026hellip;\n다음과 같이 작성하면 pallette를 custom하여 색상을 6개로 늘릴 수 있습니다!\npalette = sns.cubehelix_palette(light=.8, n_colors=6) sns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, hue=\u0026#34;coherence\u0026#34;, style=\u0026#34;choice\u0026#34;, palette=palette, kind=\u0026#34;line\u0026#34;, data=dots);   또는 다음과 같이 Colormap을 Normalization 할 수도 있죠!\nfrom matplotlib.colors import LogNorm palette = sns.cubehelix_palette(light=.7, n_colors=6) sns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, hue=\u0026#34;coherence\u0026#34;, style=\u0026#34;choice\u0026#34;, hue_norm=LogNorm(), kind=\u0026#34;line\u0026#34;, data=dots);   아니면\u0026hellip; 다음과 같이 size에 의미를 부여할 수도 있겠죠.\nsns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, size=\u0026#34;coherence\u0026#34;, style=\u0026#34;choice\u0026#34;, kind=\u0026#34;line\u0026#34;, data=dots);   여기선 Colormap 변경이 주요 내용이였습니다.\n그럼 다음과 같이 Colormap도 주면서\u0026hellip;size를 이용해서 의미를 부여하는 것도 가능하겠죠!?\nsns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, hue=\u0026#34;coherence\u0026#34;, size=\u0026#34;choice\u0026#34;, palette=palette, kind=\u0026#34;line\u0026#34;, data=dots);   Plotting with date data 이전에 날짜 혹은 시간과 연관된 데이터를 사용할 경우 autofmt_xdata()라는 부분이 있었습니다.\n이는 x축 레이블의 format을 자동으로 맞춰주는 부분이라고 했었는데요.\n가령 포맷을 변경해야하는 상황이 발생한다면..?\nmatplotlib 문서를 참고하여 변경하셔야합니다..\n왜냐하면 seaborn의 뼈대가 matplotlib 이니까요!\ndf = pd.DataFrame(dict(time=pd.date_range(\u0026#34;2017-1-1\u0026#34;, periods=500), value=np.random.randn(500).cumsum())) df.head(5)   g = sns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;value\u0026#34;, kind=\u0026#34;line\u0026#34;, data=df) g.fig.autofmt_xdate()   Showing multiple relationships with facets 지금까지는 단 1개의 figure 만 plot 했습니다!\n하지만 보통..다수의 figure가 필요하죠..\n이번엔 그 부분에 대한 내용입니다.\nrelplot()은 FacetGrid 기반이기 때문에 쉽게 가능합니다.\n\u0026ldquo;facet\u0026rdquo; 이라는 단어가 나올텐데요.\n이는 전체 figure 하나를 facet이라고 칭합니다. 다음과 같이 데이터를 load 하고 plot 해주세요! col=\u0026quot;time\u0026quot; 이라는 옵션을 추가함으로써 Lunch에 대한 figure와 Dinner에 대한 figure 두 개가 plot 됩니다!\n여기선 1x2 facet 이라고 할 수 있어요!\ntips = sns.load_dataset(\u0026#34;tips\u0026#34;) tips.head(5) sns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, col=\u0026#34;time\u0026#34;, data=tips);   또한 다음과 같이 두 변수에 대해 figure를 나눌 수도 있습니다.\ncol=\u0026quot;region\u0026quot;, row=\u0026quot;event\u0026quot;라고 하면 행은 **\u0026ldquo;event\u0026rdquo;**를 기준으로 열은 **\u0026ldquo;region\u0026rdquo;**으로 총 4개가 plot 됩니다!\n이건 2x2 facet!\nfmri = sns.load_dataset(\u0026#34;fmri\u0026#34;) fmri.head(5) sns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;subject\u0026#34;, col=\u0026#34;region\u0026#34;, row=\u0026#34;event\u0026#34;, height=3, kind=\u0026#34;line\u0026#34;, estimator=None, data=fmri);   fmri 데이터의 경우 여러 subject가 있었습니다.\n그 각각 subject 별로 figure를 만들고 싶을땐\u0026hellip;?\n다음과 같이 작성합니다.\ncol=\u0026quot;subject\u0026quot;, col_wrap=5 는 열을 subject를 기준으로 5개씩 끊어서 plot 하겠다는 의미입니다.\nheight는 facet의 높이(인치)를 정합니다.\naspect는 height에 맞춰 비율을 정하구요.\n그럼 총 14개의 figure가 plot되고 3x5 facet이 생성됩니다!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;event\u0026#34;, style=\u0026#34;event\u0026#34;, col=\u0026#34;subject\u0026#34;, col_wrap=5, height=3, aspect=.75, linewidth=2.5, kind=\u0026#34;line\u0026#34;, data=fmri.query(\u0026#34;region == \u0026#39;frontal\u0026#39;\u0026#34;));   여기까지 Part 1의 A 내용이였습니다.\n그럼 \u0026ldquo;어떤 내용이 부족해요!\u0026rdquo;, \u0026ldquo;이 내용 잘 모르겠어요!\u0026rdquo; 하는 부분을 남겨주시면 추가하도록 할게요!\n감사합니다!\n","permalink":"https://jjerry-test.github.io/blog/part1a/","tags":["Usage"],"title":"Seaborn Tutorial Part 1-A"},{"categories":["Living"],"contents":"안녕하세요! Jerry 입니다!\n오늘은 .. Atom 에디터를 완벽히 지우는 법에 대해 간단 포스팅을 하려고 합니다.\n Applications 디렉토리에서 Atom을 지운다. home 디렉토리에서 .atom 디렉토리를 지운다. /usr/local/bin/atom 을 지운다. /usr/local/bin/apm 을 지운다. ~/Library/Preferences/com.github.atom.plist 을 지운다. ~/Library/Preferences/com.github.atom.helper.plist 을 지운다.  끝입니다..\nAtom 에 대한 모~~든걸 지운거에요!\n감사합니다!\n","permalink":"https://jjerry-test.github.io/blog/macbook_auto_start/","tags":["Macbook"],"title":"맥북 Atom 완벽 제거"},{"categories":["DeepLearning"],"contents":"You Only Look Once: Unified, Real-Time, Object detection   편의상 bounding box -\u0026gt; bbox\n Abstract  Object detection을 bboxes, class probabilities regression 문제로 접근. Single Neural Network 로 bboxes와 class probabilities 둘 다 예측. Localization Error가 다소 높지만 background error가 낮음. End-to-End, Extremely fast  1. Introduction  Current Detection 시스템  DPM : Sliding Window 방식 R-CNN 계열 : potential bboxes 추출 -\u0026gt; image 에서 bboxes 부분 다시 classify -\u0026gt; Post-processing   Slow, Hard to optimize(각 요소별로 따로따로 학습을 해야함. RPN -\u0026gt; Classifier -\u0026gt; RPN -\u0026gt; \u0026hellip;)      복잡하지 않은 pipeline과 빠른 inferenc time.\n 45 fps on a Titan X .    이미지 전체를 이용한 prediction\n Fast R-CNN 보다 적은 background error    Object의 general한 representations 학습.\n natural image로 학습하고 art works로 test 했을 때 기존의 DPM 이나 R-CNN 보다 성능이 좋았다. 새로운 도메인이나 뭔가 모를 입력에 대해 일반화 할 수 있다.    2. Unfied Detection  Single Neural Network로 통합. 이미지 전체를 이용해서 각각의 bbox 예측.      입력 이미지를 S x S grid로 나눔.\n Feature map이 S x S 라고 이해하면 됨.    각 grid cell은 B개의 bbox의 정보(x, y, w, h, confidence score), 해당 grid cell의 Class probabilities 정보를 가짐.\n  bbox의 정보\n x, y : bbox의 center 좌표 w, h : 이미지 크기에 대비한 상대적인 값. confidence score : bbox가 object를 가졌는지 box가 얼마나 정확히 예측했는지에 대한 score Confidence Score는 다음과 같이 정의. $${Confidence; Score}= Pr(Object) * IOU^{truth}_{pred}$$ No object의 경우  Confidence Score는 0 이어야함.   Confidence Score가 IOU와 같아지길 원함.    Class Probabilities 정보\n  C개의 class에 대한 conditional class probabilities, \\(Pr(Class_i \\mid Object)\\)\n  Test 시에는 Conditional class probabilities와 individual box confidence score를 곱했다고 함.\n\\(Pr(Class_i|Object) = Pr(Object) * IOU^{truth}_{pred} = Pr(Class_i) * IOU^{truth}_{pred}\\)\n  bbox 별로 class confidence score를 알 수 있음.\n      PASCAL VOC 로 평가. S = 7, B = 2, C = 20\n  최종 출력은 7 x 7 x 30 의 tensor.\n  2.1 Design  24개의 Convolution layer, 2개의 Fully Connected layer. GoogLeNet의 inception 모듈 대신에 1 x 1 Convolution layer를 이용하여 reduction.   Tiny model은 9개의 Convolution layer, 2개의 Fully Connected layer.  2.2 Training   앞 단의 20개의 Convolution layer(Feature Extractor)를 ImageNet 1000-class competition 데이터(224 x 224)로 Pretrain.\n  20번째 Convolution layer 뒤에 Average Pooling, Fully Connected Layer.\n  ImageNet 2012 validation set으로 top-5 accuracy 88% 정도..     Pretrain 후 Detector 부분 추가 후 입력 크기를 448 x 448 로 높여서 학습 진행.\n  Bounding Box의 width, height 값은 이미지의 width, height로 normalize 하여 0 ~ 1 사이 값을 같도록 함.\n  Bounding Box의 x, y 값은 특정 grid cell의 left top으로부터 offset 값. 0 ~ 1 사이 값을 같도록 함.\n  마지막 layer는 linear activation function 사용.\n  다른 layer는 leaky ReLU 사용. $$\\phi(x)=\\begin{cases}x,\u0026amp;if;x \u0026gt;0\\\\ {0.1}x, \u0026amp; otherwise\\end{cases}$$\n  Optimization이 쉬운 Sum-Squared Error 를 사용.\n  이미지의 대부분 grid cell이 object 를 가지고 있지 않기 때문에 Confidence Score가 0 에 수렴.\n  이 상황에선 object를 가지고 있는 grid cell의 gradient를 압도할 수 있음.\n  이를 해결하기 위해 Bbox coordinate loss와 No object의 confidence loss 에 대해 weight 를 부여.\n\\(\\lambda_{coord} = 5\\) and \\(\\lambda_{noobj} = 0.5\\).\n  Sum-Squared Error는 large boxes와 small boxes 를 동일하게 평가.\n  large boxes 에 대해서 중요성을 반영하기 위해 width, height 는 square root 사용.\n  $$\\lambda_{coord}\\sum^{S^2}_{i=0}\\sum^B_{j=0}\\mathbb{I}^{obj}_{ij}(x_i-\\hat{x}_i)^2+(y_i-\\hat{y}_i)^2$$ $$+\\lambda_{coord}\\sum^{S^2}_{i=0}\\sum^B_{j=0}\\mathbb{I}^{obj}_{ij}(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2$$ $$+ \\sum^{S^2}_{i=0}\\sum^B_{j=0}\\mathbb{I}^{obj}_{ij}(C_i - \\hat{C}_i)^2$$ $$ + \\lambda_{noobj}\\sum^{S^2}_{i=0}\\sum^B_{j=0}\\mathbb{I}^{noobj}_{ij}(C_i - \\hat{C}_i)^2 $$ $$ + \\sum^{S^2}_{i=0}\\mathbb{I}^{obj}_{i}\\sum^B_{c\\in{classes}}(p_i(c) - \\hat{p}_i(c))^2 $$\n  \\(\\mathbb{I}^{obj}_{i}\\) : Object가 존재하는 Grid Cell i.\n  \\(\\mathbb{I}^{obj}_{ij}\\) : Object가 존재하는 Grid Cell i의 Bounding Box j.\n  Train 관련 Parameter\n Batch Szie : 64 Momentum : 0.9 Decay : 0.0005 Learning rate  \\(10^{-3}\\) 부터 \\(10^{-2}\\) 까지 천천히 증가. \\(10^{-2}\\) 로 75 epochs 학습. \\(10^{-3}\\) 로 줄여서 30 epochs 학습. \\(10^{-4}\\) 로 줄여서 30 epochs 학습.   Dropout : 0.5 Data Augmentation  Random Scaling, Translation of up to 20% of the original image size. Random adjustment exposure and saturation      2.3 Inference  Large Object 나 여러 grid cell에 걸쳐있는 object는 여러 셀에 predict 될 수 있음. Non-maximal suppression 사용하여 해결.  2.4 Limitations of YOLO  각 Grid Cell은 하나의 클래스만 가질 수 있기 때문에 Grid Cell 하나에 작은 object가 여러 개 있을때 제대로 예측하지 못할 수 있다. 예상치 못한 aspect ratio나 configuration을 가진 객체를 일반화 하는데 어려움. Large box와 Small box를 동일하게 처리. Large box의 small error보다 Small box의 small error가 IOU에 훨씬 큰 영향을 끼침.  3. Comparison to Other Detection Systems  생략  4. Experiments 4.1. Comparison to Other RealTime Systems  Fast Yolo는 가장 빠른 속도를 보여줌. YOLO 는 real-time 성능을 보여주면서 mAP도 뛰어난걸 확인할 수 있음.     4.2. VOC 2007 Error Analysis  Object Localization은 Fast R-CNN이 더 뛰어남. But, Background Error(False Positive)가 훨씬 높음.     4.3. Combining Fast RCNN and YOLO  Fast R-CNN과 YOLO를 앙상블 한 모델이 성능이 가장 좋음.     4.4. VOC 2012 Results   4.5 Generalizability  새로운 도메인, 예상치 못한 입력이 들어왔을때 일반화 성능이 뛰어남. Picasso Dataset 과 People-Art Dataset을 이용하여 다른 모델들과 일반화 성능 비교. YOLO가 가장 성능이 좋은것을 보여줌.       ","permalink":"https://jjerry-test.github.io/blog/yolo/","tags":["Paper"],"title":"Review: YOLO"},{"categories":["Living"],"contents":"늦었지만 키보드 자랑\u0026hellip;\n바밀로 저소음흑축 108키 입니다.\n넘나 영롱하다\u0026hellip;\n    ","permalink":"https://jjerry-test.github.io/blog/keyboard/","tags":["Hardware"],"title":"첫 기계식 키보드 !"},{"categories":["Python"],"contents":"거의 한달 반\u0026hellip;만에 글을 씁니다..!\n이번에는 Python에서 NIfTI 포맷의 데이터를 load 하는 방법에 대한 포스팅을 해보려고 합니다.\n저는 Anaconda를 사용중이기에..제 사용환경에 맞게 설명을 하겠습니다.\n가장 먼저 관련 패키지인 Nibabel 을 설치를 해줍니다.\nconda install -c conda-forge nibabel 이러면 설치는 끝입니다.\nimport nibabel as nib from matplotlib import pyplot as plt data = nib.load(\u0026#34;.nii 경로\u0026#34;) img = data.get_data() #plt.imshow(img)# 슬라이스 1장일 경우 plt.imshow(img[:,:,\u0026#34;slice 번호\u0026#34;]) plt.show() 이런 식으로 작성하시면 됩니다.\n예시를 보여드리면\n  이렇습니다!\n추후엔 DICOM 다루는 법에 대해서 업로드 해보겠습니다!\n","permalink":"https://jjerry-test.github.io/blog/nifti/","tags":["Usage"],"title":"Python으로 NIfTI 영상을 읽어보자!"},{"categories":["Ubuntu"],"contents":"대부분이 그렇겠지만 딥러닝, 머신러닝 하시는 분들은 우분투 서버를 이용합니다.\n물론 저도 그렇구요. 가끔 파일을 로컬에 복사할 일이 있습니다.\nmobaXterm 이라던가 xshell 같은 프로그램으로 옮길 수도 있지만 제 경험상 대용량이라던가 파일이 많다면 끊기더구요..\n그래서 CLI 환경에서 하는 방법을 찾아보다가 scp 라는 명령어가 있더군요.\n방법은 간단합니다.\nscp -r (계정이름)@(IP):(복사할 파일 경로) (저장할 경로) 이렇게 하면 끝나더군요. scp에 대한 자세한 설명은 추후에 추가하겠습니다.\n","permalink":"https://jjerry-test.github.io/blog/scp/","tags":["Command"],"title":"scp를 이용해서 파일 or 디렉토리 복사하기!"},{"categories":["DeepLearning"],"contents":"안녕하세요!\n오늘은 TensorFlow 설치 방법을 간단하게 알려드리려고 합니다.\nCPU와 GPU 중에서도 GPU 버전 설치에 대해 알려드릴거에요!\n매우 간단하니 놀라지마시기 바랍니다.\n Anaconda 를 설치한다. 터미널 or CMD에 conda install python=3.6 라고 입력한다.   CMD에서 안될경우 Anaconda prompt에서 실행하세요.  conda install tensorflow-gpu 라고 입력한다. 자신이 원하는 IDE로 코딩을 한다.  이렇게 끝입니다.\nAnaconda 이용시 로컬에 쿠다를 설치 안해줘도 됩니다.\n참 쉽죠\u0026hellip;.?\n문제가 있다면 댓글 남겨주시기 바랍니다!\n","permalink":"https://jjerry-test.github.io/blog/anaconda_setting/","tags":["TensorFlow"],"title":"Anaconda를 이용하여 TensorFlow 설치하기!"},{"categories":["Living"],"contents":"저는 터미널을 마음대로 바꾸는걸 좋아합니다.\n기본 터미널을 대체할 앱이 있긴하지만 추가적으로 설치하는걸 안좋아해서..ㅎㅎ\n그래서 Customizing 하는 법을 포스팅하려고 합니다.\nMac 터미널에서 환경변수 설정은 .bash_profile 에서 합니다. Ubuntu 에서 .bashrc와 같다고 생각하시면 되요.\n# terminal customize # 컬러 적용 여부 export CLICOLOR=1 # 디렉토리의 생상 export LSCOLORS=GxFxCxDxBxegedabagaced # 유저 및 호스트의 색상 export PS1=\u0026#39;\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;35m\\]\\w\\[\\033[00m\\]\\$\u0026#39; 첫번째 옵션은 1이면 Customizing응 한다는 의미입니다.\n두번째 옵션이 좀 문제인데요..\n알아보기 매우 힘듭니다..\n저도 기억하기 힘들어서 포스팅을 하는거죠.\n자..두번째 옵션을 두 부분으로 나눠서 설명드리겠습니다.\nGx Fx Cx Dx Bx 1x 2x 3x 4x 5x 1x : 디렉토리 색상 2x : symbolic link 색상 3x : socket 색상 4x : pipe 색 5x : 실행파일 색상 eg ed ab ag ac ed 1a 2b 3c 4d 5e 6f 1a : block special 색상 2b : char special 색상 3c : exe_setuid 색상 4d : ext_setgid 색상 5e : a-dir_writeothers_sticky 색상 6f : b-dir_writeothers_NOsticky 색상 솔직히 socket, pipe, 1a ~ 6f 까지는 뭔지 잘 모르겠습니다..\n제대로 배우지 않아서\u0026hellip;ㅎㅎ\n색상 설명\na black b red c green d brown e blue f magenta g cyan h light grey A bold black B bold red C bold green D bold brown (거의 노란색) E bold blue F bold magenta G bold cyan H bold light grey (거의 흰색) x default foreground or background 모든 색상설정은 알파벳 두개로 구성이 됩니다.\n앞에 알파벳은 글자의 색상이고 뒤에 알파벳은 배경의 색상입니다.\n세번째 옵션을 살펴보겠습니다.\n이 옵션 또한 복잡하게 써있네요..\n'\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;35m\\]\\w\\[\\033[00m\\]\\$' 이건 터미널을 켰을때 유저와 PC 이름을 어떻게 보여줄 것인가를 정합니다.\n'\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\] 에서 {USER_NAME}@{PC_NAME}을 초록색 볼드체로 하겠다라는 의미입니다. (01 : 볼드체, 32m : 초록색)\n\\[\\033[01;35m\\]\\w\\[\\033[00m\\] 에서 ~을 자주색 볼드체로 하겠다라는 의미입니다. (01 : 볼드체, 35m : 자주색)\n자세한 정보는 추후에 더 추가하겠습니다!\n","permalink":"https://jjerry-test.github.io/blog/terminal/","tags":["Macbook"],"title":"Mac Terminal Customizing"},{"categories":["Blog"],"contents":"안녕하세요! Jerry 입니다.\n요즘 깃헙으로 블로그를 하려고 하시는 분이 많은데요!\n한번 만드는 법을 포스팅해보려고 합니다!\n깃헙 블로그는 보통 Jekyll 이라는 정적 사이트 생성기를 이용합니다.\n자세한 설명은 여기서 확인 하시면 됩니다.\n블로그 만드는 방법은 두 가지가 있는데요.\n  테마를 자기가 만드는 방법.\n  공개된 테마를 가져와서 수정하는 방법.\n  전 2번 방법 으로 했습니다.\n왜냐하면 전 웹 프로그래밍 언어를 모르니까요..ㅎㅎ\n그래서 2번 방법 에 대해 포스팅하려고 합니다.\n1. 원하는 테마 다운받기. 템플릿 모음1 템플릿 모음2\n위 링크는 템플릿에 대한 정보를 모아놓은 사이트입니다.\n링크에 자신이 원하는 템플릿을 들어가보면 대부분 깃헙 repository 로 연결됩니다.\n예시로 템플릿 모음1 에 있는 Prologue 라는 테마를 적용해보겠습니다.\n  Download를 바로 누르셔도 되고 Homepage에 들어가서 git clone, Download ZIP 하셔도 상관없습니다.\n다운로드 받은 후에 알집을 풀어주세요!\n전 Home에 풀었구요.\n    그 폴더 안에는 이렇게 구성이 되어 있습니다.\n2. 루비 젬 설치. 템플릿을 만들면서 바로 바로 수정되는 사항을 볼 수 있다면 좋겠죠?\n그렇기 때문에 루비 젬 이라는 것을 설치 해야합니다.\nOS 별 설치법은 여기서 확인하시면 됩니다.\n저는 Mac OS 를 기준으로 작성하겠습니다.\ngem install jekyll bundler 까지 진행하세요.\n3. localhost 서버 열기. 다음과 같이 터미널을 열고 압축푼 경로로 이동해줍니다.\n  그리고 bundle install 이라고 입력합니다.\n  그러면 뭐 이것 저것 설치가 될거에요.\n  서버를 열 준비는 끝났습니다.\n이제 터미널에 bundle exec jekyll serve 라고 입력해주세요.\n  사진과 같이 나올거에요!\n밑에 http://127.0.0.1:4000/jekyll-theme-prologue라고 나와있네요!\n그럼 인터넷 브라우저를 켜주시고 주소창에 입력해주세요!\n그럼 다음과 같은 창이 열립니다.\n  이제 블로그를 할 준비는 완료했습니다!\n다음엔 테마를 토대로 커스터마이징 하는 포스팅을 준비해보겠습니다!\n","permalink":"https://jjerry-test.github.io/blog/gitblog/","tags":["Jekyll"],"title":"Jekyll Blog"},{"categories":["Python"],"contents":"저번 Python 설치 포스팅에서 pyenv로 설치 하는 법을 포스팅 했었습니다.\n이번에는 제가 pyenv 로 어떤 버전을 설치했고 무슨 패키지를 설치했는지 포스팅 하려고 합니다.\n먼저 설치할 버전의 이름을 정확히 알아야하므로 설치 가능한 버전들을 봅니다.\npyenv install --list 이렇게 터미널에 입력을 하면\nAvailable versions: 2.1.3 2.2.3 2.3.7 ... 중략 ... stackless-3.4.2 stackless-3.4.7 stackless-3.5.4 이런식으로 굉~~~장히 많은 버전이 있습니다. (약 340개?)\n그중에서 저는 anaconda3-5.2.0을 설치했습니다.\npyenv install anaconda3-5.2.0 pyenv global anaconda3-5.2.0 아나콘다 환경 설치 끝..\n이제 제가 사용하는 파이썬 패키지를 설치해야겠죠.\n보통 python 이라면 pip를 쓰겠지만 저는 아나콘다를 설치했죠.\n그래서 conda를 이용해서 설치했습니다.\n제가 설치 할 패키지는 pytorch, tensorflow, keras, tqdm 입니다.\nconda install tqdm # progress bar 패키지 conda install keras tensorflow # machine learning 패키지 conda install pytorch torchvision -c pytorch # machine learning 패키지 이렇게 하면 끝입니다.\nnumpy, scipy, matplotlib, ...등 패키지는 anaconda를 설치하면서 자동으로 설치가 됩니다 ㅎㅎ\n만약 저 환경을 삭제하고 싶다?\npyenv uninstall anaconda3-5.2.0 이라고 하시면 바로 삭제됩니다.\n많은 분들께 도움이 되었으면 좋겠습니다!\n감사합니다~!\n","permalink":"https://jjerry-test.github.io/blog/anaconda/","tags":["Setting"],"title":"Anaconda 간단하게 사용하기!"},{"categories":["Blog"],"contents":"Markdown은 2004년 존그루버에 의해 만들어졌으며 쉽게 쓰고 읽을 수 있으며 HTML로 변환이 가능한 텍스트 기반의 마크업언어입니다. 특수기호와 문자를 이용한 매우 간단한 구조의 문법을 사용하여 웹에서도 빠르게 컨텐츠를 작성하고 직관적으로 인식할 수 있습니다. Github을 사용하는 사람이라면 가장 먼저 만나게 되는 파일이 README.md 파일인데요. 이 파일도 Markdown으로 작성된 파일입니다. 마크다운을 통해서 설치방법, 소스코드 설명, 이슈 등을 간단하게 기록하고 가독성을 높일 수 있어서 많은 사람들이 사용하게 되었습니다!\nTable of contents  기본 사용법  Headings   h1 Heading  h2 Heading  h3 Heading  h4 Heading  h5 Heading  h6 Heading         Paragraphs Newline Horizontal Line Emphasis  Bold Italics   Blockquotes Lists  Unordered Ordered Time-saving Tip   Code  Inline code \u0026ldquo;Fenced\u0026rdquo; code block Indented code Syntax highlighting   Links  Autolinks Inline links   Images Raw HTML Escaping with backslashes   그 외 사용법  Strikethrough Tables  Aligning cells      기본 사용법 Headings Heading 은 h1 부터 h6 까지 있고 # 의 개수로 단계가 정해집니다.\n사용법 :\n# h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading 적용 후 :\nh1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading  Paragraphs 문단을 나누는 법입니다. 문단 사이에는 하나 이상의 빈 줄로 구분됩니다.\n사용법 :\n\u0026lt;p\u0026gt;이 문장은 첫번째 문단의 첫번째 문장입니다.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;이 문장은 두번째 문단의 첫번째 문장입니다. 이 문장은 두번째 문단의 두번째 문장입니다.\u0026lt;/p\u0026gt; 적용 후 :\n Newline 공백 두칸 후 엔터, \u0026lt;br\u0026gt;을 이용하여 개행을 할 수 있습니다.\n(\u0026lt;br\u0026gt;은 굳이\u0026hellip;.)\n사용법 :\ntest1`공백 두칸 후 엔터` test2 test1\u0026lt;br\u0026gt;test2 적용 후 :\ntest1\ntest2\ntest1test2\nHorizontal Line Markdown 문서에 수평선을 추가 할 수 있습니다.\n사용법 :\n`___`: 밑줄(_) 3개 `---`: 대쉬(-) 3개 `***`: 별표(*) 3개 적용 후 :\n    Emphasis Bold 볼드체 적용도 가능합니다.\n**이렇게 하면 볼드체!** 적용 후 :\n이렇게 하면 볼드체!\nItalics 이텔릭체 적용도 가능합니다.\n_이렇게 하면 이텔릭체!_ 적용 후 :\n이렇게 하면 이텔릭체!\n Blockquotes 블럭인용은 \u0026gt; 으로 합니다.\nHeading과 처럼 \u0026gt; 개수로 인용 안에 인용을 추가 할 수 있습니다.\n사용법 :\n\u0026gt; 이것은 인용구에요. 적용 후 :\n 이것은 인용구에요.\n 다중 인용 사용법 :\n\u0026gt; 이것은 첫번째 인용이에요. \u0026gt;\u0026gt; 이것은 두번째 인용이에요. \u0026gt;\u0026gt;\u0026gt; 이것은 세번째 인용이에요. 적용 후 :\n 이것은 첫번째 인용이에요.\n 이것은 두번째 인용이에요.\n 이것은 세번째 인용이에요.\n    Lists 목록에는 순서가 없는 목록, 순서가 있는 목록이 있습니다.\nUnordered 순서가 없는 목록은 *, -, + 로 사용할 수 있습니다.\n사용법 :\n+ 음식 + 가구 + 운동 - 농구 - 축구 - 야구 적용 후 :\n 음식 가구 운동  농구 축구 야구    또한 다음과 같이 사용할 수 있습니다.\n사용법 :\n- 신체 - 머리 - 상체 - 하체 적용 후:\n  사람\n 머리  눈 코 입   상체 하체    개미\n 머리 가슴 배    Ordered 순서가 있는 목록을 만들 때는 번호를 매기면 됩니다.\n1. 노트북을 켠다. 2. Atom 에디터를 켠다. 3. 프로젝트 폴더를 연다. 4. 코딩을 한다. 적용 후 :\n 노트북을 켠다. Atom 에디터를 켠다. 프로젝트 폴더를 연다. 코딩을 한다.  Time-saving Tip 순서가 있는 목록에서 시간을 아끼는 방법입니다.\n1., 2.,\u0026hellip; 라고 적을 필요 없이 1. 만 적으면 자동으로 번호를 매깁니다.\n1. 노트북을 켠다. 1. Atom 에디터를 켠다. 1. 프로젝트 폴더를 연다. 1. 코딩을 한다. 적용 후 :\n 노트북을 켠다. Atom 에디터를 켠다. 프로젝트 폴더를 연다. 코딩을 한다.   Code Inline code 문장 중간에  를 이용하여 code를 넣을 수 있습니다.\n파이썬에서 모듈을 쓰려면 `import '모듈명'` 이라고 하면 됩니다. 적용 후 :\n파이썬에서 모듈을 쓰려면 import '모듈명' 이라고 하면 됩니다.\n\u0026ldquo;Fenced\u0026rdquo; code block code block을 만들어서 code를 넣을 수 있습니다.\n적용 후 :\nimport os import sys Indented code 들여쓰기(공백 4개)를 이용하여 code를 쓸 수 있습니다.\n하지만 별로 추천하는 방법은 아닙니다.\n왜냐하면 syntax highlighting이 안됩니다.\n import sys impot os 적용 후 :\nimport sys impot os  Syntax highlighting \u0026ldquo;Fenced\u0026rdquo; code block 은 어떤 프로그래밍 언어의 code block 을 만들건지 적어서 사용을 합니다.\n그러면 그 언어에 맞게 Syntax highlighting을 합니다.\nWhich renders to:\nimport os import sys a = 2 b = 5 print(\u0026#34;Hello, World!\u0026#34;) print(a + b)  Links 링크를 거는 방법은 Autolinks, Inline links, Link titles, \u0026lsquo;Named Anchors\u0026rsquo; 이렇게 네 가지가 있습니다.\n이 포스팅에선 Autolinks, Inline links 이 두 가지를 설명하겠습니다.\nAutolinks \u0026lt;, \u0026gt; 사이에 링크를 적으면 자동으로 링크가 생성됩니다.\n\u0026lt;https://jjerry-k.github.io\u0026gt; 적용 후 :\nhttps://jjerry-k.github.io\nInline links 문장 안에 링크를 생성할 수 있습니다.\n[Jerry's Blog](https://jjerry-k.github.io) 적용 후 :\nJerry\u0026rsquo;s Blog\n Images 이미지를 올리는 방법입니다.\n첫번째 방법 ![Minion](http://octodex.github.com/images/minion.png) 적용 후 :\n두번째 방법 ![Alt text](http://octodex.github.com/images/stormtroopocat.jpg \u0026quot;The Stormtroopocat\u0026quot;) 적용 후 :\n위 두 방법은 이미지 사이즈 조절을 할 수 없습니다.\n만약 사이즈 조절이 필요하다면 \u0026lt;img src=\u0026quot;\u0026quot; height=\u0026quot;\u0026quot; width=\u0026quot;\u0026quot;\u0026gt;를 이용하면 됩니다.\n\u0026lt;img src=\u0026quot;http://octodex.github.com/images/dojocat.jpg\u0026quot; height=\u0026quot;100\u0026quot; width=\u0026quot;100\u0026quot;\u0026gt; 적용 후 :  Raw HTML 이미지에서 HTML 문법을 쓸 수 있다는걸 확인했다시피 Markdown 에서 HTML 문법을 사용할 수 있습니다.\n여기가 **\u0026lt;a href=\u0026quot;https://jjerry-k.github.io\u0026quot;\u0026gt;Jerry의 블로그\u0026lt;/a\u0026gt;**입니다. 여기가 **[Jerry의 블로그](https://jjerry-k.github.io)**입니다. 적용 후 :\n여기가 Jerry의 블로그입니다.\n여기가 Jerry의 블로그입니다.\n Escaping with backslashes \\ 를 이용하여 문장안에 기호를 사용할 수 있습니다.\n\\*를 문장에 쓰고 싶다! 적용 후 :\n*를 문장에 쓰고 싶다!\n 그 외 사용법 Strikethrough 문장에 줄을 그을 수 있습니다.\n~~블로그....귀찮다....~~ 적용 후 :\n블로그\u0026hellip;.귀찮다\u0026hellip;.\n Tables 표를 만들 수 있습니다. |, -를 이용하여 작성합니다.\n| 이름 | 학점 | | --- | --- | | 제리 | B+ | | 톰 | C+ | |스파이크| B0 | 적용 후 :\n   이름 학점     제리 B+   톰 C+   스파이크 B0    Aligning cells 셀 정렬은 :를 이용합니다.\n왼쪽 정렬은 :을 사용하지 않으면 됩니다.\nCenter text in a column\n중앙 정렬은 다음과 같이 합니다.\n| 이름 | 학점 | | :-: | :-: | | 제리 | B+ | | 톰 | C+ | |스파이크| B0 |    이름 학점     제리 B+   톰 C+   스파이크 B0    Right-align the text in a column\n오른쪽 정렬은 다음과 같이 합니다.\n| 이름 | 학점 | | --: | --: | | 제리 | B+ | | 톰 | C+ | |스파이크| B0 | 적용 후 :\n   이름 학점     제리 B+   톰 C+   스파이크 B0    Markdown 설명에 대한 포스팅\u0026hellip; 끝!\n","permalink":"https://jjerry-test.github.io/blog/markdown/","tags":["Markdown"],"title":"Markdown 사용법"},{"categories":["DeepLearning"],"contents":"안녕하세요! Jerry 입니다!\n어제 pip 로 설치하지 말라구요!? 라는 포스팅을 했죠?\n포스팅을 하고 갑자기 확 꽂혀서 anaconda 환경을 지웠습니다.\nSetting 을 다시 하기 위해서요! 핳핳핳\n그래서 지우고 다시 구축했죠.\n제가 사용하는 방법은 Python Installation for mac 에 적혀있습니다!\n아나콘다 환경 설치 후에 이제 python package를 설치해야겠죠?\npip 가 아닌 conda로 전부 설치했습니다.\nconda install tensorflow conda install pytorch torchvision -c pytorch conda install keras 그 후에 테스트를 하고자 터미널에서 tensorflow를 import 해봤습니다.\nimport tensorflow 그 후에 나온 warning 입니다.\n/Users/jerry/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\n라고 나오더군요.\n이게 굉장히 거슬리는 분들 계실겁니다..\n보통 h5py 라는 python 패키지가 2.8.0 미만이면 이 warning이 발생해요!\n해결책은 간단합니다!\npip install --upgrade h5py 이것만 해주시면 됩니다!\n그럼 즐거운 코딩!\n","permalink":"https://jjerry-test.github.io/blog/warning/","tags":["TensorFlow"],"title":"Python 3.6 에서 이 경고 보기 싫어요.."},{"categories":["DeepLearning"],"contents":"오늘 갑자기 당황스러운 포스팅을 봤습니다.\n출처 : https://www.anaconda.com/blog/developer-blog/tensorflow-in-anaconda/\n내용을 보니 pip 로 설치하는것 보다 conda를 이용하면 좋은 점이 두 가지가 있다고 합니다.\n  CPU 성능이 더 빨라졌다.     GPU 버전 설치가 쉽다.\n솔직히 이건 잘 모르겠네요..\n둘 다 똑같이 그래픽 드라이버 설치하고 CUDA 설치하고 해야하는데 뭐가 쉬워진다는건지..\n뭐.. 어떤 쿠다 버전을 설치했던 알아서 잡아서 TensorFlow 를 설치해준다면 편해진건 맞겠군요.\n  그리고 CPU 성능 증가는 Intel CPU에 한해서 빨라지는 것 같습니다.\n  사진을 보시면 the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) 을 사용해서 올렸다는거죠. (AMD 사용자 쥬륵..)\n뭐 아무튼\u0026hellip; 인텔에 텐플쓰시는 분들은 콘다로 넘어가심이 좋을 듯합니다.\n전 요즘 pytorch 를 쓰고 있어서 ㅎㅎ..\n읽어주셔서 감사합니다 ㅎㅎ\n","permalink":"https://jjerry-test.github.io/blog/condavspip/","tags":["TensorFlow"],"title":"pip 로 설치하지 말라구요!?"},{"categories":["Ubuntu"],"contents":"현재 저는 하나의 GPU 서버를 가지고 여러 명이 Deep Learning 을 돌려야 합니다.\n그러면 GPU 각각을 분배하거나 메모리를 분산해야겠죠.\n제 연구실 같은 경우엔 전자를 택했습니다.\n그래서 이번엔 특정 GPU만 사용하는 방법을 간 ! 단 ! 하 ! 게 포스팅하려고 합니다. (Just 명령어만 쓸꺼임.)\nCUDA_VISIBLE_DEVICES=0 python ~~~.py # 0번 GPU만 사용. #CUDA_VISIBLE_DEVICES=0,3 python ~~~.py # 0, 3번 GPU 사용. 이렇게 실행하시면 ~~~.py 를 실행해서 GPU 를 사용할 경우 0번 GPU만 사용해서 스크립트가 실행됩니다! 많은 분들이 TensorFlow를 사용하실텐데 이것으로 예를 들어보겠습니다.\nGPU가 여러 대인 상황에서 아무 옵션도 주지 않고 (스크립트에서도 안줬다는 가정하에..) tf.Session() 을 실행하게 되면 모든 GPU의 메모리를 혼자서 다 잡고 있는걸 보셨을 겁니다.\n하지만 만약 위에 코드 처럼 실행한다면 특정 번호에 해당하는 GPU의 메모리만 잡고 있는걸 보실 수 있을 겁니다!\n","permalink":"https://jjerry-test.github.io/blog/gpu_masking/","tags":["Usage"],"title":"GPU를 골라서 써보자!"},{"categories":["Python"],"contents":"# brew 설치. /usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#34; # wget 설치. brew install wget #pyenv 설치. brew install pyenv # bash_profile 에 경로 추가. echo \u0026#39;export PYENV_ROOT=\u0026#34;${HOME}/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=\u0026#34;${PYENV_ROOT}bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;eval \u0026#34;$(pyenv init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc exec $SHELL # 설치할 수 있는 환경 보여줌. pyenv install --list # 위에서 환경 확인 후 원하는 버전 입력. pyenv install \u0026lt;버전 이름\u0026gt; # system 과 \u0026lt;버전 이름\u0026gt; 두 환경이 존재. pyenv versions # \u0026lt;버전 이름\u0026gt;으로 default version 변경. pyenv global \u0026lt;버전 이름\u0026gt; pyenv versions # tmp 폴더 에선 system 이라는 버전으로 사용. mkdir tmp cd tmp pyenv local system pyenv 를 사용하면 특정 폴더에서는 python2 로 작동하고 그 외에는 python3 로 작동하게 할 수 있습니다!\n감사합니다!\n","permalink":"https://jjerry-test.github.io/blog/python4mac/","tags":["Setting"],"title":"Python Installation for mac"},{"categories":["DeepLearning"],"contents":"이번에 V.ais에서 TensorFlow Object Detection API 사용 매뉴얼을 작성해봤습니다.\nhttps://github.com/V-AIS/tensorflow\n아직 완벽하게 정리된건 아니지만\u0026hellip;\nTensorFlow Object Detection API를 처음 접하시는 분이라면 쉽게 따라할 수 있도록 적어봤습니다.\n수정됬으면 하는 부분이 있으면 댓글 남겨주세요!\n","permalink":"https://jjerry-test.github.io/blog/tfod/","tags":["TensorFlow"],"title":"Object Detection API"},{"categories":["Living"],"contents":"약 두 달정도 달려온 챌린지가 끝나고 처음으로 휴가를 얻었습니다.\n일단은 이번 주 쉬고 다음 주부터 다시 연구 시작..!\n쉬면서 TensorFlow Object Detection API 설명서나 써보렵니다.\n","permalink":"https://jjerry-test.github.io/blog/vacation/","tags":["Daily"],"title":"휴가"},{"categories":["DeepLearning"],"contents":"TensorFlow LSTM 예제 코드 여기저기서 TensorFlow LSTM 코드를 찾다가 예제를 제가 보기 편하게 작성햇습니다.\n추후에 새로운 코드로 업데이트 할 예정입니다. import tensorflow as tf from tensorflow.keras import models, layers, optimizers, losses from tensorflow.keras.utills import to_categorical # Setting hyper parameter learning_rate = 0.001 total_epoch = 30 batch_size = 128 n_input = 28 n_step = 28 n_hidden1 = 128 n_class = 10 # Loading Mnist Data mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train/255., x_test/255. y_train, y_test = to_categorical(y_train, n_class), to_categorical(y_test, n_class) tf.set_random_seed(777) X = tf.placeholder(tf.float32, [None, n_step, n_input]) Y = tf.placeholder(tf.float32, [None, n_class]) W = tf.Variable(tf.random_normal([n_hidden1, n_class])) b = tf.Variable(tf.random_normal([n_class])) # LSTM cell 선언. # RNN을 쓰고 싶으면 BasicRNNCell로 바꾸면 됨. # Stacked LSTM을 하고 싶으면 cell2 선언. cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden1) #cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden1) # 선언된 LSTM cell, X를 이용하여 네트워크 생성. outputs_1, states_1 = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32,scope=\u0026#34;LSTM1\u0026#34;) # Stacked LSTM을 하고 싶으면 다음과 같이 선언. #outputs_2, states_2 = tf.nn.dynamic_rnn(cell2, outputs_1, dtype=tf.float32, scope=\u0026#34;LSTM2\u0026#34;) # LSTM -\u0026gt; Fully Connected Layer -\u0026gt; Classification # outputs_1 : [ ? , num_step, num_hidden # -\u0026gt; [num_step, ? , num_hidden] outputs = tf.transpose(outputs_1, [1, 0, 2]) # Sequence의 마지막 출력값 outputs = outputs[-1] model = tf.matmul(outputs, W) + b cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y)) optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) # GPU 메모리 할당. config = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config) sess.run(tf.global_variables_initializer()) total_batch = int(mnist.train.num_examples/batch_size) for epoch in range(total_epoch): total_cost = 0 for i in range(total_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) batch_xs = batch_xs.reshape((batch_size, n_step, n_input)) _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys}) total_cost += cost_val print(\u0026#39;Epoch : %04d\u0026#39;% (epoch + 1),\u0026#39;Avg cost : {:f}\u0026#39;.format(total_cost / total_batch)) print(\u0026#39;Optimization Done\u0026#39;) is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1)) accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) test_batch_size = len(mnist.test.images) test_xs = mnist.test.images.reshape(test_batch_size, n_step, n_input) test_ys = mnist.test.labels print(\u0026#39;Test Accuracy : \u0026#39;, sess.run(accuracy, feed_dict={X: test_xs, Y: test_ys})) ","permalink":"https://jjerry-test.github.io/blog/lstm/","tags":["TensorFlow"],"title":"LSTM 예제 코드"},{"categories":["Living"],"contents":"근래에 라즈베리파이로 미니 PC를 만들었습니다!\n그래서 만드는 과정에 대해 글을 써보려고 합니다!\n제가 사용한 보드는\u0026hellip;. https://www.raspberrypi.org/products/raspberry-pi-3-model-b-plus/\n올해 출시한 라즈베리파이 3 B+ 입니다!(개인적으로 라즈베리를 처음 써봅니다\u0026hellip;)\n방열판도 붙여줬지요!\n  아래 사진은 제가 HDD를 장착 할 것이기 때문에\u0026hellip;.SATA 확장보드\u0026hellip;그리고 케이스, 어댑터 구매 내역입니다!\n  솔직히\u0026hellip;부품들 뚝딱 뚝딱 제작(조립이 맞을 듯\u0026hellip;)하는 것 보다 알리발 상품들 기다리는게 더 힘들었어요\u0026hellip;. 쥬륵\u0026hellip;.\n7일 정도 기다린것 같네요\u0026hellip;..\n박스 오자마자 정말 광속으로 뜯었습니다\u0026hellip;..\n케이스   SATA 확장보드   전용 어댑터 (5V 4A)   넘나\u0026hellip;감동\u0026hellip;\n금방 조립했습니다..\n  조립하는데 집중해서 사진을 잘 안찍었어요\u0026hellip;쥬륵\u0026hellip;\n  아\u0026hellip;저 맨 아래에 다리는 안달아도 되는거였어요\u0026hellip;.\n  케이스를 분리하면 이렇게 생겼어요\u0026hellip;(누가봐도 팬이 달릴 부분..)\n  달아줍니다..\n  차례 차례 전원 연결\u0026hellip;\n  뒷판을 조립하고!\n  브릿지? 를 연결!\n  어댑터 연결 후 켜시면 됩니다\u0026hellip; 좀 더 정확한 크기 확인을 위해 사진을 좀 찍어봤어요!\n  제 주먹이랑 한 컷\u0026hellip;\n  마지막으로 제 12인치 맥북이랑 한컷\u0026hellip;.\n  처음으로 라즈베리파이를 사용해보면서\u0026hellip;정말 재밌고 신기하네요 ㅎㅎㅎ (물론 가끔 설정하다가 뻑나면\u0026hellip;짜증\u0026hellip;.)\n나중엔 캠을 달아서 사물인식 같은 걸 해볼까 합니다!\n너무\u0026hellip;.불친절한 글 같네요\u0026hellip;.\n나중에 또 뭔가 재미난걸 하면 올리겠습니다!\n감사합니다!\n","permalink":"https://jjerry-test.github.io/blog/raspberry-mini-pc/","tags":["Hardware"],"title":"라즈베리파이로 미니 PC 만들기!"},{"categories":["Living"],"contents":"Building Desktop 학부생 연구원으로 연구실에서 공부와 연구를 하고 있지만..\n개인용 데스크탑이 없어서 노트북이 굉장히 고생을 많이 했습니다..\n1차 피해자 Dell Inspiron 15 7559..\n현재는 다른 주인에게로 넘어간 상태..\n2차 피해자 Lenovo yoga 2 pro..\n근래에 고생하다가 친구한테 넘어감..\n결국!\n어짜피 나중에 집에서 쓸 데스크탑을 맞춘다고 생각하고!\n부품을 각각 사서 조립을 하기로 했습니다!!\nCPU : 인텔 i5-7600\n메인보드 : 기가바이트 Z270N-WIFI 듀러블에디션\n메모리 : 커세어 VENGEANCE PC4-21300 8G x 2\nVGA : EVGA GeForce GTX 1050ti sc gaming\nSSD : 삼성 850 EVO M.2 (256GB)\nHDD : Toshiba 1TB MQ01ABD100\n케이스 : Fractal Design node 202\n파워 : 커세어 SF450 80PLUS GOLD\n오자마자 포스팅 생각없이 조립을 해버림\u0026hellip;\n  커세어의 위엄\u0026hellip;..번쩍번쩍 램\u0026hellip;.이쁘다\u0026hellip;\n  사이트에는 Fractal 로고가 위로 올라오게 해놨지만\u0026hellip;\n그렇게하면 파워의 위치가 위가 되버려서 난 반대로 로고가 아래로 내려가게 했습니다.\n후기\n  데스크탑을 사용만 해봤지 조립은 처음이였음..\n  괜히 작은 케이스로 해서 선정리가 힘들었음..\n  살 수 있는건 중고나라를 이용해서 미개봉품을 싸게 삼.\n  25만원 정도 절약함.\n  내 눈에만 이쁘면 됬음.\n  아.. ITX 케이스를 조립할 땐 대부분 SFX 규격을 쓰지만..\n같은 SFX라도 크기를 고려해야함 . .\n  ","permalink":"https://jjerry-test.github.io/blog/desktop/","tags":["Hardware"],"title":"데스크탑 조립기"},{"categories":["Ubuntu"],"contents":"환경 구축\u0026hellip;. Deep Learning을 하는데 GPU를 사용해야하니 CUDA와 cuDNN을 설치하기로 하죠.\n1. NVIDIA 그래픽 드라이버 설치 Ubuntu 14.04 에선 그래픽 드라이버를 설치하는데 굉장히\u0026hellip;\n많은 고난과 역경을 겪었습니다\u0026hellip;\n(그래서 결국 설치 못해봄..)\n하지만 16.04에선 어렵지 않음!!!!!!\n(갈아탄 결정적인 이유\u0026hellip;)\n그렇다면 설치를 시작해보겠습니다.\nsudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-364 (제 노트북은 GTX 960m 이므로..)\n그냥 터미널에 입력하시면 되요.\nNVIDIA 사이트에서 GPU 버전이랑 맞는걸로!!\n그게 나을 듯합니다.\n그 후 로그아웃을 하고 다시 로그인을 하려고 하면 무한 로그인이 될 거에요.\n(안그러면 좋고..)\nCtrl + Alt + F1을 눌러 tty1으로 들어가서 로그인 후 reboot을 실행해줍시다.\n로그인 후 NVIDIA X Server Settings 를 켜면\n  요래 설치 된걸 확인할 수 있습니다.\n2. CUDA 설치 NVIDIA 드라이버를 설치 했으니 이제 CUDA를 설치해보도록 하죠.\nhttps://developer.nvidia.com/cuda-downloads 에 들어가시면 요로코롬 뜨는데요.\n각자의 환경에 맞춰서 선택한 후 다운로드 하시면 됩니다.\nrun 파일을 다운을 받은 후에 터미널을 여시고 run 파일이 다운로드된 디렉토리로 이동합니다.\n그리고 파일의 권한을 모든 사용자가 사용할 수 있도록 변경합니다.\n그리고 실행!\ncd Downloads sudo chmod a+r cuda_7.5.18_linux.run sudo ./cuda_7.5.18_linux.run --override or sudo sh cuda_7.5.18_linux.run --override 16.04는 gcc, g++ 버전이 5.x 라 override 를 해줘야합니다..\n실행하면 어쩌고 저쩌고 약관 내용..? 이 나오는데요\u0026hellip;\n살포시 q 를 눌러줍니다.\n그러면\u0026hellip;사용자 정의 설치라고 할까요??\n이것 저것 물어봅니다.\nDo you accept the previously read EULA? (accept/decline/quit): accept You are attempting to install on an unsupported configuration. Do you wish to continue? ((y)es/(n)o) [ default is no ]: yes Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 352.39? ((y)es/(n)o/(q)uit): no Install the CUDA 7.5 Toolkit? ((y)es/(n)o/(q)uit): yes Enter Toolkit Location [ default is /usr/local/cuda-7.5 ]: 그냥 엔터 Do you want to install a symbolic link at /usr/local/cuda? ((y)es/(n)o/(q)uit): yes Install the CUDA 7.5 Samples? ((y)es/(n)o/(q)uit): no 이런식으로 세팅을 해줍니다.\n그리고 그저 기다리고 기다림\u0026hellip;.\n설치가 다 되었으면 뭐 따로 실행할 건 없습니다.\n바로 cuDNN 설치를 하죠.\n이건 설치\u0026hellip;.는 아니고 그냥 cuda 디렉토리에 파일을 파일 복사? 하는거에요.\n3. cuDNN 설치 먼저 cuDNN 파일을 받으셔야하는데요.\nhttps://developer.nvidia.com/rdp/cudnn-download\n여기에 들어가셔서 계정 만드시고\u0026hellip;.\n사용 용도\u0026hellip; 사용 할 라이브러리.. 등등 선택하시고 원하시는 버전 download 하시면 됩니다.\n저는 4.0으로 했어요.\nTensorflow라는 라이브러리를 사용할 것이기 때문에\u0026hellip; 그리고 나서 터미널을 켜고\ncd /usr/local sudo tar zxf ~/Downloads/cudnn-7.0-linux-x64-v4.0-prod.tgz 이렇게 하시거나 아니면\ncd cudnn 다운로드경로 sudo tar zxf cudnn-7.0-linux-x64-v4.0-prod.tgz sudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 이렇게 해주시구요.\n환경변수를 설정해줘야 합니다. 터미널을 껐다가 다시 켜주시고\nsudo gedit ~/.bashrc 그려면 어떤 메모장? 문서 파일이 열립니다. 매~~~~앤 아래 부분에\nexport PATH=/usr/local/cuda-7.5/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda-7.5/lib64:$LD_LIBRARY_PATH 를 추가해주세요.\n이렇게 GPU 설정이 끝났습니다\u0026hellip;\n이제 caffe와 tensorflow를 설치해야겠군요..\n험난허다\u0026hellip;\n개강이라 죽겠는데..\n","permalink":"https://jjerry-test.github.io/blog/cuda/","tags":["Setting"],"title":"Ubuntu CUDA \u0026 Cudnn 설치하기"},{"categories":["Ubuntu"],"contents":"우분투를 설치를 했으나!!!! 한글 키보드를 설정을 해야합니다. 그럼 시작해볼까요. (이번에도 사진 촤라라락..)\n1. 시스템 한글 설정 부팅을 했습니다. 크으\u0026hellip; 깔끔깔끔\u0026hellip;\n  오른쪽 위에 전원 버튼 누르고\nSystem Settings로 들어가서 Language Support를 눌러줍니다.\n  그럼 설치를 해야한다고 하네요. 묻지도 따지지도 말고 설치합시다. (뒤에 사진찍은건 애교로 넘어갑시다\u0026hellip;)\n  로그인 비밀번호 입력하구요. (여기도 있네\u0026hellip;)\n  그럼 설치가 시작됩니다.\n  설치가 끝나면 한국어라고 똿! 나옵니다.\n  아래에 있는 한국어를 드래그해서 맨 위로 올려줍니다.\n    그리고 \u0026lsquo;Apply System-wide\u0026rsquo;를 클릭!\n  비밀번호 입력하시구요.\n  Regional Formats는 설치할때 이미 Seoul로 해서 상관은 없지만 그냥 들어가서 \u0026lsquo;Apply system-wide\u0026rsquo;눌러줍니다.\n  로그아웃을 하고 다시 로그인 해주세요!! 그럼 다음과 같이 창이 뜨는데 저는 영문이 편해서 그냥 이름을 유지하기로 했습니다.\n  2. 한글 키보드 설정하기 키보드를 설정해보죠.\n시스템 설정으로 들어갑니다.\n그리고 텍스트 입력창을 눌러줍니다.\n  다음과 같이 뜨는데요. 저기에 +를 눌러줍니다. (정말 더럽게 못그리네..)\n  쭉쭉쭈욱~ 내려서 \u0026lsquo;한국어 (Hangul)(IBus)\u0026lsquo;를 선택하고 \u0026lsquo;추가\u0026rsquo;버튼을 누릅니다.\n  지금은 키 변환이 Super+스페이스로 되어있네요. 이대로 쓰실분들은 쓰셔도 됩니다. 하지만!! 저는 한/영 키를 사용하고 싶기 때문에\u0026hellip; 저 키를 바꿔보도록 하죠. \u0026lsquo;키보드 설정\u0026hellip;\u0026lsquo;을 누릅니다.\n  자판입력 탭에서 구성키를 \u0026lsquo;오른쪽 Alt\u0026rsquo;로 바꾼 후에 \u0026lsquo;다음 입력 소스로 전환\u0026rsquo; 키는 한/영으로 바꿉니다.\n  다시 텍스트 입력으로 돌아와서 한국어를 선택한 후 사진에 표시된 도구 버튼을 누릅니다.\n  다음과 같이 나오는데요.\n\u0026lsquo;한글 모드로 시작\u0026rsquo;을 체크한 후 확인을 눌러줍니다.\n  이제 한글에 대한 설정이 끝났습니다!\n다시 로그아웃을 한 후 로그인을 하시면 한글을 쓰실 수 있습니다!!\n오늘은 포스팅 Day인듯합니다. 왜냐하면 오늘 우분투를 재설치 했기 때문이죠\u0026hellip; 으하하하하하하하핳\u0026hellip;\u0026hellip; 우분투 다시 셋팅하면서 포스팅중\u0026hellip;. 다음은 소프트웨어 관련 세팅 포스팅하겠습니다.\n","permalink":"https://jjerry-test.github.io/blog/setting/","tags":["Setting"],"title":"Ubuntu 한글 설정"},{"categories":["Ubuntu"],"contents":"Deep Learning을 하기 위해 windows를 사용할 수도 있겠지만\u0026hellip;\n저는 Ubuntu를 사용하기로 했습니다. 왜냐하면 Ubuntu로 배웠으니..ㅎㅎ\n자, 그럼 시작하겠습니다 .\n제가 예전에 16.04를 다운받아서 설치한거라..\n다운로드는 16.04.1로 되어있는데 설치는 16.04로 되어있습니다.\n하지만 설치 방법은 똑같음!\n그리고 듀얼부팅은 Windows가 먼저 설치되어야 한다는거 잊지마세요.\n1. 설치 전에 확일 해야할 사항  BIOS에서 secure booting disable Windows 에서 빠른 시작 켜기 옵션 끄기          1.1 설치 할 디스크 볼륨 만들기   저는 HDD에 설치할 거니까 HDD 주 파티션에서 마우스 오른쪽 클릭!\n  볼륨을 축소합시다.\n  원하시는 만큼 입력하세요. 102,400 MB는 예시일뿐\u0026hellip;\n  할당되지 않은 부분이 100GB 생겼네요!\n  2. Ubuntu 설치 과정 2.1 Ubuntu 16.04.1 LTS image Download http://www.ubuntu.com/download/desktop에 들어갑니다.\n  Download를 클릭하면 다음 사진과 같이 나오는데 흠\u0026hellip;잘 모르겠습니다.\n후원..같지만 \u0026lsquo;Not now, take me to the download\u0026rsquo;를 클릭합니다.\n  다운로드 창이 자동으로 나옵니다.\n저장할 경로 지정해주세요!\n  2.2 Ubuntu 16.04.1 LTS 부팅 디스크 만들기 https://rufus.akeo.ie에서 util을 받습니다.\n  Util을 실행!\n처음으로 부팅 디스크로 쓸 USB를 선택합니다.\n  OS image를 불러와야겠죠?\n저~~기 아이콘 눌러주세요!\n  ISO 파일이 있는 곳으로 가서 선택하고 열기!\n  그리고 시작을 누릅니다!\n  구구절절\u0026hellip; 구구절절\u0026hellip; \u0026lsquo;OK\u0026rsquo; 눌러주세요.\n  이런 경고는\u0026hellip;저에게 통하지 않습니다..\n\u0026lsquo;확인\u0026rsquo;을 눌러주세요!\n  진행이 됩니다.\n  되고 있는\u0026hellip;건가..?\n  6분 36초 정도 걸렸네요..\n  그럼 이제 설치를 해보겠습니다.\n2.3 Ubuntu 설치하기 BIOS 세팅에서 부팅 순서를 USB로 바꿔주시거나\nBoot Option에서 USB를 선택해서 부팅합니다.\n검은 화면이 나오면서 \u0026lsquo;Try Ubuntu without installing\u0026rsquo;, \u0026lsquo;Install 어쩌고\u0026hellip;\u0026rsquo; 등이 있는데요.\n저는 스크린샷을 해야하므로 \u0026lsquo;Try Ubuntu without installing\u0026rsquo; 을 선택했습니다.\n\u0026lsquo;또로롱\u0026rsquo;하면서 화면이 뜹니다!\n  바탕화면에 \u0026lsquo;Install Ubuntu 16.04 LTS\u0026rsquo; 실행합니다. 디렉토리 이름이 영어인게 좋으니까 일단 English로 진행.\n  설치하면서 업데이트, 다른 소프트웨어 설치 여부는..마음대로\u0026hellip;\n  설치 방식에 대한 옵션이 나옵니다.\n\u0026lsquo;Something else\u0026rsquo;를 선택해주세요.\n그래야 더 안심하고 설치하니까요\u0026hellip;\n  윈도우에서 파티션을 나눴었죠.\n저는 SSD에 윈도우, HDD에 우분투를 설치할 것이니..\nHDD 파티션을 나눠놨었습니다.\n얼마나 나눌건지는..여러분 마음\u0026hellip; 전 64GB 정도 나눴습니다. (100GB 뺐다가 다시 줄임\u0026hellip;) \u0026lsquo;free space\u0026rsquo;를 선택하고 왼쪽 아래에 \u0026lsquo;+\u0026lsquo;버튼을 눌러주세요.\n  먼저 Swap 파티션부터 만들죠.\n자신의 RAM 용량과 똑같이 맞추라는 분도 계시고\n요즘은 RAM 용량이 다 커서 필요없다는 분도 계셔서\n저는 그 중간인 8GB를 Swap으로 만들었습니다.\n설정법은 아래 사진과 같이!\n그리고 \u0026lsquo;OK\u0026rsquo;를 누릅시다.\n  아래 사진과 같이 swap 영역이 unknown으로 만들어집니다.\n이제 Ubuntu 파티션을 만들어보죠.\n똑같이 \u0026lsquo;free space\u0026rsquo; 를 선택하고 \u0026lsquo;+\u0026rsquo; 버튼을 눌러줍니다.\n  \u0026lsquo;Size\u0026rsquo;에는 남은 용량을 다 씁니다.\n나머지는 아래 사진과 같이 설정 !\n그리고 \u0026lsquo;OK\u0026rsquo;를 누릅시다.\n  이제 Boot loader 경로를 지정해야하는데요.\n아랫 부분에 \u0026lsquo;Device for boot loader installation\u0026rsquo;에서\n디스크를 선택하면 됩니다.\nSSD에 지정을 하시기도 하지만 저는 HDD 지정을 했습니다.\n그리고 \u0026lsquo;Install Now\u0026rsquo;을 클릭합니다.\n  디스크가 변경됩니다. 뭐 이런 내용이 뜨는데\n그냥. 조용히. 살포시.\n\u0026lsquo;Continue\u0026rsquo; 를 눌러줍시다.\n(불안하시면 다시 확인 하시구요..)\n  위치 설정하시구요!\n  키보드 설정은 그냥 영어로 놔두세요. 나중에 설치 후에 설정 할겁니다!\n  계정 설정을 합니다!\n저는 Jerry니까\u0026hellip; 우분투 계정도 Jerry.\n  설치 시작!!\n  그저\u0026hellip; 기다림\u0026hellip;\n    설치가 완료되었습니다!\n과감히 바로 \u0026lsquo;Restart Now\u0026rsquo;를 클립했습니다.\n  첫 포스팅이라 굉장히 난잡하고 스크롤이 촤라라라락하네요\u0026hellip;\n좀 더 정리해서 쓰는 습관을 들여야지\u0026hellip;\nUbuntu를 접하게 되면서 굉장히 다사다난한 경험을 하는 저와 제 노트북\u0026hellip;.\n어쩌면 조만간 Windows 와 Ubuntu를 재설치 할 수도 있을 듯합니다..\n다음 포스팅은 설치 후 세팅에 관련하여 하겠습니다!\n","permalink":"https://jjerry-test.github.io/blog/dualboot/","tags":["Setting"],"title":"Windows 10 \u0026 Ubuntu 16.04 듀얼부팅"}]