[{"categories":["Blog"],"contents":"ì´ë²ˆì— Github Blogë¥¼ ì´ì „í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤!\nê¸°ì¡´ì—ëŠ” Jekyll ê¸°ë°˜ì˜ ë¸”ë¡œê·¸ë¥¼ ì‚¬ìš©í–ˆì—ˆëŠ”ë°ìš”.\nì´ê²Œ ì•½ê°„.. í¸í•œ ë“¯ ë¶ˆí¸í•œ ë“¯(íŠ¹íˆë‚˜ gem bundle\u0026hellip;.ê´€ë¦¬).. ë­”ê°€ ë¶€ì¡±í•œ ëŠë‚Œì´ì˜€ìŠµë‹ˆë‹¤.\nJekyllì´ ì•„ë‹Œ ë‹¤ë¥¸ ê¸°ë°˜ì˜ Generator ë¥¼ ì°¾ì•„ë³´ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.\ní›„ë³´êµ°ì€ ì„¸ ê°œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. Hexo, Hugo, Gatsby\u0026hellip;\nHexoëŠ” Jekyll ë³´ë‹¤ëŠ” ë‚«ì§€ë§Œ ë³„ë¡œë¼ëŠ” í‰ì´ ìˆì—ˆê³  GastbyëŠ” í”„ë¡ íŠ¸ ì•Œëª»ì¸ ì €ì—ê² ê·¸ëƒ¥ ì–´ë µ\u0026hellip;ë”êµ°ìš”.\nê·¸ë˜ì„œ Hugoë¥¼ ì´ìš©í•´ë³´ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ Hugoë¥¼ ì´ìš©í•œ ë¸”ë¡œê·¸ ì„¸íŒ… ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.\nì¤€ë¹„ë¬¼   2ê°œì˜ github repository\n ë¸”ë¡œê·¸ ì†ŒìŠ¤ìš©: ì´ë¦„ ë§‰ ì§€ì–´ë„ ìƒê´€ì—†ìŒ. ex) blog_source í˜¸ìŠ¤íŒ…ìš©: {Username}.github.io ë¡œ ë§Œë“œì…”ì•¼ í•©ë‹ˆë‹¤. ex) jjerry-k.github.io    ì›í•˜ëŠ” í…Œë§ˆ\n https://themes.gohugo.io ì—¬ê¸°ì„œ ì›í•˜ëŠ”ê±¸ ê³¨ë¼ë†“ìœ¼ì„¸ìš”.    ì„¸íŒ… ê³¼ì • 1. Repository ìƒì„± ì¤€ë¹„ë¬¸ì—ì„œ ì–˜ê¸°í–ˆë“¯ ë‘ê°œì˜ repositoryë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n  ë¸”ë¡œê·¸ ì†ŒìŠ¤ìš©     í˜¸ìŠ¤íŒ…ìš©\n ì‚¬ì§„ì„ ì˜¬ë ¤ì•¼í•˜ëŠ”ë° ì €ëŠ” ì´ë¯¸ \u0026hellip; Jekyllë¡œ ì“°ê³  ìˆë˜ì§€ë¼ \u0026hellip; íŒ¨ìŠ¤ ë¸”ë¡œê·¸ ì†ŒìŠ¤ìš© repository ì²˜ëŸ¼ ë¹„ì–´ìˆë„ë¡ ë§Œë“¤ì–´ì£¼ì„¸ìš”.    2. Hugo ì„¤ì¹˜ ë° í”„ë¡œì íŠ¸ ìƒì„±. brew install hugo hugo new site {í”„ë¡œì íŠ¸ ì´ë¦„} hugoë¥¼ ì„¤ì¹˜í•˜ê³  ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\nì €ëŠ” í¸ì˜ë¥¼ ìœ„í•´ í”„ë¡œì íŠ¸ ì´ë¦„ì„ blog_source ë¼ê³  í–ˆìŠµë‹ˆë‹¤.\n  3. í…Œë§ˆ ì„¤ì¹˜ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì— themes ë””ë ‰í† ë¦¬ê°€ ë³´ì…ë‹ˆë‹¤. themes ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì—¬ ìœ„ì—ì„œ ê³¨ëë˜ í…Œë§ˆë¥¼ ë‹¤ìš´ë¡œë“œ í•©ë‹ˆë‹¤. ì €ëŠ” liva-hugo ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\ncd blog_source cd themes git clone https://github.com/themefisher/liva-hugo.git     liva-hugo ëŠ” ì´í•´ê°€ ì‰½ë„ë¡ exampleSiteë¥¼ ì´ìš©í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. blog_source/themes/liva-hugo/exampleSite ì— ìˆëŠ” content, static, config.toml ì„ blog_source ì— ìˆëŠ” ìœ„ì¹˜ì— ë³µì‚¬í•´ì¤ë‹ˆë‹¤.\ncp -r themes/liva-hugo/exampleSite/content/* ./content cp -r themes/liva-hugo/exampleSite/static/* ./static cp themes/liva-hugo/exampleSite/config.toml ./   4. ì„œë²„ ì‹¤í–‰ ë° ê¸€ì“°ê¸° í˜„ì¬ ë¸”ë¡œê·¸ ìƒíƒœê°€ ì–´ë–¤ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤. ì¼ë‹¨ hugo serverë¥¼ ì‹¤í–‰ì‹œì¼œ ë´…ë‹ˆë‹¤.\n# í˜„ì¬ ìœ„ì¹˜: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ hugo server -p {ì›í•˜ëŠ” í¬íŠ¸ ë²ˆí˜¸} -D # Default PORT 1313    ê·¸ë¦¬ê³  localhost:1313 ë¡œ ì ‘ì†ì„ í•˜ë©´ ì˜ˆì‹œë¥¼ ì‚¬ì´íŠ¸ ì–‘ì‹ì„ ê°€ì ¸ì™”ê¸° ë•Œë¬¸ì— ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ì˜µë‹ˆë‹¤.   í¬ìŠ¤íŒ…ì„ í•˜ê¸° ì „ì— ë‹¤ë¥¸ í¬ìŠ¤íŒ…ì„ í™•ì¸í•´ë´…ë‹ˆë‹¤.   liva-hugoëŠ” ì´ëŸ¬í•œ í…œí”Œë¦¿ì„ ë°”íƒ•ìœ¼ë¡œ í¬ìŠ¤íŒ…ì„ ì‘ì„±í•´ì•¼í•©ë‹ˆë‹¤.\nì„¸ë¶€ ë‚´ìš©ì€ ì§ì ‘ ì‚¬ìš©í•˜ì‹œë©´ì„œ ìµíˆì‹œê¸¸\u0026hellip;.\nì ê·¸ëŸ¼ í¬ìŠ¤íŒ…ì„ í•œë²ˆ ì¨ë³´ë„ë¡ í•©ì‹œë‹¤.\nhugo new blog/hello.md ë¼ê³  ì…ë ¥ì„ í•´ë„ ë˜ê³  blog/hello.md ë¥¼ ì§ì ‘ ìƒì„±í•˜ì…”ë„ ë©ë‹ˆë‹¤.\nê·¸ í›„ í…œí”Œë¦¿ì— ë§ê²Œ ì‘ì„±ì„ í•´ì¤ë‹ˆë‹¤.\n  ì´ë ‡ê²Œ ì‘ì„± í›„ ì €ì¥ì„ í•˜ë©´ ë¸”ë¡œê·¸ì— ë‹¤ìŒê³¼ ê°™ì´ ê¸€ì´ ìƒì„±ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n  hugo ê³µí†µì ìœ¼ë¡œ í…œí”Œë¦¿ì— draft:  ë¶€ë¶„ì„ trueë¡œ í•œë‹¤ë©´ ë¸”ë¡œê·¸ì— ë°˜ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\në°˜ëŒ€ë¡œ false ë¥¼ ì ëŠ”ë‹¤ë©´ ë¸”ë¡œê·¸ì— ë°˜ì˜ì´ ë©ë‹ˆë‹¤.\n5. Githubì— ë°°í¬ ì»¤ë§¨ë“œë¡œ ë°”ë¡œ ì ê² ìŠµë‹ˆë‹¤.\n# í˜„ì¬ ìœ„ì¹˜: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ git init git remote add origin https://github.com/jjerry-k/blog_source #{ë¸”ë¡œê·¸ ì†ŒìŠ¤ìš© repository} git submodule add -b master https://github.com/jjerry-k/jjerry-k.github.io public #{í˜¸ìŠ¤íŒ…ìš© repository} ê·¸ í›„ ë¹Œë“œ \u0026amp; ë°°í¬ë¥¼ í•©ë‹ˆë‹¤.\n# í˜„ì¬ ìœ„ì¹˜: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ hugo -t liva-hugo # hugo -t {í…Œë§ˆ ì´ë¦„} cd public git add . git commit -m \u0026#34;Update\u0026#34; git push cd .. git add . git commit -m \u0026#34;Update\u0026#34; git push 1~5ë²ˆ ê¹Œì§€ì˜ ê³¼ì •ì„ ê±°ì¹œ í›„ {Username}.github.io ë¡œ ì ‘ì†ì„ í•´ë³´ë©´ ì •ìƒì ìœ¼ë¡œ ë¸”ë¡œê·¸ê°€ ìƒê¸´ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në§Œì•½ ìƒˆ ê¸€ì„ ì“°ê³  ì‹¶ë‹¤ë©´?\n4ë²ˆê³¼ 5ë²ˆì„ ë°˜ë³µí•˜ì‹œë©´ ë©ë‹ˆë‹¤.\nP.S  ë¸”ë¡œê·¸ ì´ì „ ëŠë¯€ í˜ë“¤ë‹¤\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/hugo-blog/","tags":["Hugo"],"title":"Hugo Blog"},{"categories":["Living"],"contents":"í˜„ì¬ D-Link ê³µìœ ê¸°ë¥¼ ì‚¬ìš©ì¤‘ì¸ë° ì˜¬í•´ ì´ˆ? D-Link ì¸¡ì—ì„œ DDNS ì„œë¹„ìŠ¤ë¥¼ ì¢…ë£Œí•œë‹¤ëŠ” ê³µì§€ê°€ ë–´ìŠµë‹ˆë‹¤. (ì°¸ê³  í¬ìŠ¤íŒ…)\nê·¸ë˜ì„œ..ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë¥¼ ì°¾ê³  ìˆë˜ì¤‘ CODNS ë¼ëŠ” êµ­ë‚´ ì„œë¹„ìŠ¤ë¥¼ ì°¾ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\nê·¼ë° ì´ ì„œë¹„ìŠ¤ëŠ” ì¼ì • ê¸°ê°„ ì‚¬ìš©í•˜ë©´ ìˆ˜ë™ìœ¼ë¡œ ê°±ì‹ ì„ í•´ì¤˜ì•¼í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìˆë”ë¼êµ¬ìš”.\nê·¸ë˜ì„œ\u0026hellip;ë¼ì¦ˆë² ë¦¬íŒŒì´ì— crontabì„ ì´ìš©í•˜ì—¬ ìë™ ê°±ì‹  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ëŒë ¤ ì‚¬ìš©ì¤‘ì…ë‹ˆë‹¤.\n# Edit this file to introduce tasks to be run by cron. # # Each task to run has to be defined through a single line # indicating with different fields when the task will be run # and what command to run for the task # # To define the time you can provide concrete values for # minute (m), hour (h), day of month (dom), month (mon), # and day of week (dow) or use \u0026#39;*\u0026#39; in these fields (for \u0026#39;any\u0026#39;). # # Notice that tasks will be started based on the cron\u0026#39;s system # daemon\u0026#39;s notion of time and timezones. # # Output of the crontab jobs (including errors) is sent through # email to the user the crontab file belongs to (unless redirected). # # For example, you can run a backup of all your user accounts # at 5 a.m every week with: # 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/ # # For more information see the manual pages of crontab(5) and cron(8) # # m h dom mon dow command 0 12 * * * /home/pi/codns/CODNS_CLIENT.LINUX -ipupdate ","permalink":"https://jjerry-test.github.io/blog/codns/","tags":["Hardware"],"title":"ê°œì¸ì ì¸ CODNS CRONTAB"},{"categories":["DeepLearning"],"contents":"ì €ë²ˆ Mediapipeì˜ Hands í¬ìŠ¤íŒ…ì— ì´ì–´ì„œ Mediapipeì˜ Pose ë¥¼ í…ŒìŠ¤íŠ¸ í•´ë³´ê² ìŠµë‹ˆë‹¤.\nì‹¤í–‰ ì½”ë“œ import time import cv2 as cv import mediapipe as mp mp_drawing = mp.solutions.drawing_utils mp_pose = mp.solutions.pose prevTime = 0 idx = 0 pose = mp_pose.Pose( min_detection_confidence=0.5, min_tracking_confidence=0.5) cap = cv.VideoCapture(\u0026#39;./ufc.gif\u0026#39;) while cap.isOpened(): success, image = cap.read() curTime = time.time() if not success: break image = cv.cvtColor(cv.flip(image, 1), cv.COLOR_BGR2RGB) image.flags.writeable = False results = pose.process(image) image.flags.writeable = True image = cv.cvtColor(image, cv.COLOR_RGB2BGR) mp_drawing.draw_landmarks( image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS) sec = curTime - prevTime prevTime = curTime fps = 1/(sec) str = f\u0026#34;FPS : {fps:0.1f}\u0026#34; cv.putText(image, str, (0, 100), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0)) cv.imshow(\u0026#39;MediaPipe Pose\u0026#39;, image) cv.imwrite(f\u0026#34;./sample_{idx:05d}.jpg\u0026#34;, image) # for making gif idx += 1 if cv.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break pose.close() cap.release() ì¢€ ë” í•´ë´ì•¼ê² ì§€ë§Œ ì¼ë‹¨ ì§€ê¸ˆ ì½”ë“œë¡œëŠ” ì œëŒ€ë¡œ ì¡ì§„ ëª»í•˜ëŠ” ê²ƒ ê°™ë„¤ìš”..\ní \u0026hellip;.ì½”ë“œë¥¼ ì¢€ ìˆ˜ì •í•´ë´ì•¼ê² ìŠµë‹ˆë‹¤..\n  ","permalink":"https://jjerry-test.github.io/blog/mediapipe_3/","tags":["Tools"],"title":"MediaPipe - (3)"},{"categories":["DeepLearning"],"contents":"ì €ë²ˆ Mediapipeì™€ Face mesh í¬ìŠ¤íŒ…ì— ì´ì–´ì„œ Mediapipeì˜ Hands ë¥¼ í…ŒìŠ¤íŠ¸ í•´ë³´ê² ìŠµë‹ˆë‹¤.\nì‹¤í–‰ ì½”ë“œ import time import cv2 as cv import mediapipe as mp mp_drawing = mp.solutions.drawing_utils mp_hands = mp.solutions.hands hands = mp_hands.Hands( min_detection_confidence=0.7, min_tracking_confidence=0.5) cap = cv.VideoCapture(0) prevTime = 0 # idx = 0 while cap.isOpened(): success, image = cap.read() curTime = time.time() if not success: break image = cv.cvtColor(cv.flip(image, 1), cv.COLOR_BGR2RGB) image.flags.writeable = False results = hands.process(image) image.flags.writeable = True image = cv.cvtColor(image, cv.COLOR_RGB2BGR) if results.multi_hand_landmarks: for hand_landmarks in results.multi_hand_landmarks: mp_drawing.draw_landmarks( image, hand_landmarks, mp_hands.HAND_CONNECTIONS) sec = curTime - prevTime prevTime = curTime fps = 1/(sec) str = f\u0026#34;FPS : {fps:0.1f}\u0026#34; cv.putText(image, str, (0, 100), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0)) cv.imshow(\u0026#39;MediaPipe Hands\u0026#39;, image) # cv.imwrite(f\u0026#34;./sample_{idx:05d}.jpg\u0026#34;, image) # for making gif # idx += 1 if cv.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break hands.close() cap.release() ìŒ\u0026hellip;.ì‚´ì§ì‚´ì§ ëŠê¸°ê¸´ í•˜ë„¤ìš”.\n  ë‹¤ìŒì—” íŒŒì´ì¬ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ë§ˆì§€ë§‰ì¸ Pose ì˜ˆì œë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤.\n","permalink":"https://jjerry-test.github.io/blog/mediapipe_2/","tags":["Tools"],"title":"MediaPipe - (2)"},{"categories":["DeepLearning"],"contents":"ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„  Googleì˜ MediaPipeì— ëŒ€í•´ì„œ ë‹¤ë¤„ë³´ë ¤í•©ë‹ˆë‹¤.\nMediaPipe is a framework for building multimodal (eg. video, audio, any time series data), cross platform (i.e Android, iOS, web, edge devices) applied ML pipelines. With MediaPipe, a perception pipeline can be built as a graph of modular components, including, for instance, inference models (e.g., TensorFlow, TFLite) and media processing functions.\n MediaPipeë€ multi modal, cross platform ì„ êµ¬ì¶•í•˜ê¸°ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. MediaPipeë¥¼ ì‚¬ìš©í•˜ë©´ TensorFlow, TFLite ê°™ì€ inference modelê³¼ ë¯¸ë””ì–´ ì²˜ë¦¬ ê¸°ëŠ¥ë“¤ì„ ëª¨ë“ˆí˜•ì‹ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n ë¼ê³  ì í˜€ìˆëŠ” ë“¯í•©ë‹ˆë‹¤.\nê·¸ëƒ¥ ê°„ë‹¨íˆ ë§í•˜ë©´..\n ë§Œë“¤ì–´ ë†“ì€ê±° ìˆìœ¼ë‹ˆê¹Œ ëª¨ë°”ì¼, Edge device, Web í•„ìš”í•œ ê³³ì— ê°€ì ¸ë‹¤ ì“°ì„¸ìš”.\n ê·¸ë ‡ë‹¤ë©´ ì¨ë´ì•¼ì£ .\në¨¼ì € ì§€ì›í•˜ëŠ” í•­ëª©ì…ë‹ˆë‹¤.\nSolution list ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„  Face Meshë¥¼ í…ŒìŠ¤íŠ¸ í•´ë³´ê² ìŠµë‹ˆë‹¤.\nì„¤ì¹˜ë²• # mediapipeê°€ opencv 4.0.0 ì´í•˜ì™€ í˜¸í™˜ì´ë˜ìš”.. pip install mediapipe opencv-python==3.4.11.45 \u0026hellip;? ê²ë‚˜\u0026hellip;ê°„ë‹¨í•©ë‹ˆë‹¤.\nì‹¤í–‰ ì½”ë“œ import cv2 as cv import mediapipe as mp mp_drawing = mp.solutions.drawing_utils mp_face_mesh = mp.solutions.face_mesh face_mesh = mp_face_mesh.FaceMesh( min_detection_confidence=0.5, min_tracking_confidence=0.5) drawing_spec = mp_drawing.DrawingSpec(color=(128,128,128), thickness=1, circle_radius=1) cap = cv.VideoCapture(0) while cap.isOpened(): success, image = cap.read() if not success: break image = cv.cvtColor(cv.flip(image, 1), cv.COLOR_BGR2RGB) image.flags.writeable = False results = face_mesh.process(image) image.flags.writeable = True image = cv.cvtColor(image, cv.COLOR_RGB2BGR) if results.multi_face_landmarks: for face_landmarks in results.multi_face_landmarks: mp_drawing.draw_landmarks( image=image, landmark_list=face_landmarks, connections=mp_face_mesh.FACE_CONNECTIONS, landmark_drawing_spec=drawing_spec, connection_drawing_spec=drawing_spec) cv.imshow(\u0026#39;MediaPipe FaceMesh\u0026#39;, image) if cv.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break face_mesh.close() cap.release() ì´ê±¸ ëŒë¦°ë‹¤ë©´ ìš°ì§¸ ë‚˜ì˜¬ê¹Œìš”???\në‹¤ìŒ ì• ë‹ˆë©”ì´ì…˜ì€ ì½”ë“œë¥¼ ê¹¨ì‘ ìˆ˜ì •í•˜ì—¬ GIFë¡œ ë§Œë“  ê²ƒì…ë‹ˆë‹¤.\n  ë³„ ê¸°ëŒ€ ì•ˆí•˜ê³  ëŒë ¸ëŠ”ë° ì œ ë§¥ë¯¸ë‹ˆ (Intel(R) Core(TM) i5-8500B CPU @ 3.00GHz) ìœ¼ë¡œ í‰ê·  30fps ì´ìƒ ë‚˜ì˜¤ë„¤ìš”!\nGIFëŠ” ì´ê²ƒì €ê²ƒ ì²˜ë¦¬ë¥¼ í•œê±°ë¼ 30fps ì²˜ëŸ¼ ì•ˆë³´ì…ë‹ˆë‹¤..\nì´ìš©í•´ì„œ ë­”ê°€ ì¬ë°ŒëŠ”ê±¸ ë§Œë“¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤!\nì¶”í›„ì— ì‹œê°„ì´ ëœë‹¤ë©´ Handsë‘ Poseì— ëŒ€í•œ ê²°ê³¼ë„ ì¶”ê°€ë¡œ ì˜¬ë¦¬ê² ìŠµë‹ˆë‹¤!\nì°¸ê³  ìë£Œ https://mediapipe.dev/\nhttps://google.github.io/mediapipe/\nhttps://opensource.google/projects/mediapipe\nP.S  êµ¬ê¸€\u0026hellip;ê°“ê¸€\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/mediapipe_1/","tags":["Tools"],"title":"MediaPipe - (1)"},{"categories":["Ubuntu"],"contents":"Docker ì— ëŒ€í•œ ì„¤ëª…ì€ ì•ˆí•©ë‹ˆë‹¤.\nì‚¬ìš©ë²•ì„ ë³´ëŸ¬ ì˜¤ì‹œëŠ” ë¶„ì´ë¼ë©´ ì–´ëŠ ì •ë„ Docker ê°€ ë­”ì§€ëŠ” ì•„ì‹¤í…Œë‹ˆ\u0026hellip;\nìì„¸í•œ ì„¤ëª…ì´ ë³´ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ê³µì‹ í™ˆí˜ì´ì§€ í˜¹ì€ ì±…ì„ ì°¸ê³ í•˜ì„¸ìš”.\nì„¤ì¹˜ ë°©ë²•ì€ ê°„ë‹¨í•˜ê²Œ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\nsudo apt-get update sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; # DOCKER ENGINE ì„¤ì¹˜ sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # Docker ë™ì‘ í™•ì¸ sudo docker run hello-world # sudo ë¥¼ ë¶™ì´ì§€ ì•Šê¸° ìœ„í•´ ì‚¬ìš©ì ê³„ì •ì„ docker ê·¸ë£¹ì— ì¶”ê°€ sudo usermode -aG docker {ê³„ì •ëª…} sudo reboot ì»¤ë§¨ë“œ ì†Œê°œ docker ë¼ê³  ì…ë ¥í•˜ë©´ ìˆ˜ ë§ì€ ì»¤ë§¨ë“œê°€ ë‚˜ì˜µë‹ˆë‹¤.\nê·¸ ì¤‘ì— ì´ë²ˆ í¬ìŠ¤íŒ…ì— ë“±ì¥í•˜ëŠ” ì»¤ë§¨ë“œë¥¼ ì •ë¦¬ í•´ë³´ë ¤ í•©ë‹ˆë‹¤.\ndocker ps: ì»¨í…Œì´ë„ˆ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\ndocker images: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\ndocker rm: ì»¨í…Œì´ë„ˆ ì œê±°\ndocker rmi: ì´ë¯¸ì§€ ì œê±°\ndocker build: Dockerfile ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ êµ¬ì¶•\ndocker pull: Docker hub ì— ìˆëŠ” ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\ndocker run: ì´ë¯¸ì§€ë¥¼ ì‹¤í–‰í•˜ì—¬ ì»¨í…Œì´ë„ˆ ìƒì„±\nìì‹ ë§Œì˜ Image, Container ë§Œë“¤ì–´ë³´ê¸° ì˜ˆì‹œëŠ” ubuntuì— python 3.xë¥¼ ì„¤ì¹˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\ndocker run ì˜µì…˜ì— ëŒ€í•´ì„œëŠ” ë§í¬(ì—¬ê¸°) ì—ì„œ í™•ì¸í•˜ì„¸ìš”.\n# ì§€ì›í•˜ëŠ” ubuntu tag # 14.04, trusty-20191217, trusty # 16.04, xenial-20200916, xenial # 18.04, bionic-20200921, bionic # 20.04, focal-20200925, focal, latest, rolling # 20.10, groovy-20200921, groovy, devel # Docker image download docker pull ubuntu:{ì›í•˜ëŠ” ë²„ì „ tag} # ex) # docker pull ubuntu:18.04 # docker pull ubuntu:groovy # Container ìƒì„± docker run -ti ubuntu:{ì›í•˜ëŠ” ë²„ì „ tag} bash     # Docker container ì—ì„œ ì‹¤í–‰. apt update apt install -y build-essential cmake git curl wget vim unzip apt install -y ca-certificates libjpeg-dev libpng-dev software-properties-common add-apt-repository ppa:deadsnakes/ppa apt install python{ì›í•˜ëŠ” ë²„ì „} # ì„¤ì¹˜ í™•ì¸ ls /usr/bin/ | grep python exit apt install ë¶€ë¶„ì—ì„œ python 3 ì¤‘ stable ë²„ì „ì´ ì„¤ì¹˜ë  ê²ë‹ˆë‹¤.\nê·¸ë˜ì„œ ì§€ê¸ˆì€ 3.8ì´ ì„¤ì¹˜ ë©ë‹ˆë‹¤. ( 3.8 ì„ ì„¤ì¹˜í•˜ì‹¤ ë¶„ë“¤ì€ aptë¡œ pythonì„ ì„¤ì¹˜í•  í•„ìš” ì—†ìŒ )\nì¶”ê°€ì ìœ¼ë¡œ ì €ëŠ” 3.7ì„ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤.\nê·¸ ê²°ê³¼ ë§ˆì§€ë§‰ ì»¤ë§¨ë“œ (ls /usr/bin/ | grep python)ë¥¼ ì…ë ¥í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥ì´ ë‚˜ì˜µë‹ˆë‹¤.\n  ì´ë ‡ê²Œ í•˜ë©´ Containerì— ì›í•˜ëŠ” í™˜ê²½ì„ ì„¸íŒ…í–ˆìŠµë‹ˆë‹¤.\ndocker ps -a   ì´ë¥¼ Imageë¡œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.\ndocker commit {ì»¨í…Œì´ë„ˆ ì´ë¦„} {ì´ë¯¸ì§€ ì´ë¦„:íƒœê·¸} docker images docker rmi {ì»¨í…Œì´ë„ˆ ì´ë¦„}   ì•ìœ¼ë¡œëŠ” docker run -ti --rm {ì´ë¯¸ì§€ ì´ë¦„:íƒœê·¸} bash ì™€ ê°™ì´ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤!\nDockerfile ì´ìš©í•´ë³´ê¸° ì´ì „ì˜ ì˜ˆì‹œëŠ” Image download ë¶€í„° commit ê¹Œì§€ í•˜ë‚˜í•˜ë‚˜ ì§ì ‘ ì»¤ë§¨ë“œë¥¼ ì…ë ¥í–ˆìŠµë‹ˆë‹¤.\nê·¸ë ‡ì§€ ì•Šê³  íŒŒì¼ì„ ë§Œë“¤ê³  ê·¸ê±¸ ì´ìš©í•´ì„œ Imageë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\nDockerfileê³¼ docker buildë¥¼ ì´ìš©í•©ë‹ˆë‹¤.\nDockerfileì„ ë§Œë“¤ ë•Œë„ ì»¤ë§¨ë“œë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ”ë° ì˜ˆì‹œì—ì„œ ì‚¬ìš©í•  ì»¤ë§¨ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\nFROM: ì–´ë–¤ imageë¥¼ ë² ì´ìŠ¤ë¡œ í•  ê²ƒì¸ì§€ ì§€ì •.\nLABEL: í•´ë‹¹ Dockerfileë¡œ ë§Œë“  ì´ë¯¸ì§€ì˜ ì •ë³´, ì‘ì„±ìë“±ì˜ ì •ë³´ ì‘ì„±. (ê¼­ í•  í•„ìš”ëŠ” ì—†ìŒ)\nARG: Dockerfile ë‚´ì—ì„œ ì‚¬ìš©ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ (ENVì™€ í˜¼ë™ë˜ê¸° ì‰¬ì›€)\nRUN: imageë¥¼ êµ¬ì„±ì— í•„ìš”í•œ ê° ë‹¨ê³„ë¥¼ ì‹¤í–‰\nFROM ubuntu:{ì›í•˜ëŠ” ë²„ì „ tag} LABEL maintainer \u0026#34;Jerry Kim \u0026lt;jaeyeol2931@gmail.com\u0026gt;\u0026#34; ARG PYTHON_VERSION={ì›í•˜ëŠ” ë²„ì „} # Docker container ì—ì„œ ì‹¤í–‰. RUN apt update -q RUN DEBIAN_FRONTEND=\u0026#39;noninteractive\u0026#39; apt install -y build-essential cmake git curl wget vim unzip RUN DEBIAN_FRONTEND=\u0026#39;noninteractive\u0026#39; apt install -y ca-certificates libjpeg-dev libpng-dev software-properties-common RUN add-apt-repository ppa:deadsnakes/ppa RUN apt install -y python$PYTHON_VERSION # ì„¤ì¹˜ í™•ì¸ RUN ls /usr/bin/ | grep python ëŒ€ì¶© ë””ë ‰í† ë¦¬ë¥¼ í•˜ë‚˜ ë§Œë“¤ê³  ìœ„ ë‚´ìš©ì´ ë‹´ê¸´ Dockerfileì„ ë§Œë“­ë‹ˆë‹¤.\n  ê·¸ë¦¬ê³  Dockerfileì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì—¬ ë‹¤ìŒ ì»¤ë§¨ë“œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.\ndocker build -t {ì´ë¯¸ì§€ ì´ë¦„:íƒœê·¸} .   ê·¸ëŸ¼ ë‹¤ìŒê³¼ ë§ˆì§€ë§‰ì— ë‹¤ìŒê³¼ ê°™ì€ ì¶œë ¥ì´ ë‚˜ì˜¤ê³  docker imagesë¥¼ í•´ë³´ë©´ ì˜ ìƒì„±ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n    P.S  apt â†’ apt-get ìœ¼ë¡œ ë°”ê¿”ë„ ìƒê´€ì—†ì–´ìš”. í”¼ê³¤í•˜ë„¤ìš”. ì˜ìš•ì´ ì—†ë„¤ìš”. Docker Usage (2) ëŠ” ë­ë¡œ í•˜ì§€..  ","permalink":"https://jjerry-test.github.io/blog/docker_usage_01/","tags":["Docker"],"title":"Docker Usage (1)"},{"categories":["DeepLearning"],"contents":"Deep learning model ì„ ì‹¤í—˜í•˜ë‹¤ë³´ë©´ reproducibility ë¼ëŠ” ë§ì„ ìì£¼ ë“£ê²Œ ë©ë‹ˆë‹¤.\nëŒ€ì¶©\u0026hellip; ëŒë¦´ ë•Œ ë§ˆë‹¤ ê²°ê³¼ê°€ ë‹¬ë¼ìš”...ë¼ëŠ” ì˜ë¯¸ì£ .\nê·¸ëŸ¬ë©´ì„œ seed ë¼ëŠ” ë‹¨ì–´ë¥¼ ë§ì´ ë“£ê²Œ ë˜ëŠ”ë°ìš”.\nseedëŠ” ìœ„í‚¤í”¼ë””ì•„, í˜¹ì€ ë‹¤ë¥¸ ë¸”ë¡œê·¸ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\nì´ë²ˆì—” seed ì— ë”°ë¼ ì–´ë–¤ ê²°ê³¼ë¥¼ ë³´ì´ëŠ”ì§€ ì‹¤í—˜ì„ í•´ë³´ë ¤ í•©ë‹ˆë‹¤.\nì‹¤í—˜ ë°©ë²•  ì‹¤í—˜ ë³€ìˆ˜  ë°ì´í„°: Mnist ëª¨ë¸: Simple cnn GPU: V100 32GB í•™ìŠµ íšŸìˆ˜: 10 ë°°ì¹˜ ì‚¬ì´ì¦ˆ: 1024 ì‚¬ìš© Seed ì¢…ë¥˜: random seed, numpy seed, tensorflow seed, python hashseed   ì•„ë¬´ seed ê³ ì • ì—†ì´ 5ë²ˆ ì”© ì§„í–‰. ê° seed ë³„ë¡œ 5ë²ˆì”© ì§„í–‰.  ì‹¤í—˜ ê²°ê³¼ Seed ê³ ì • ì—†ìŒ   PYTHONHASHSEED   Random   Numpy   TensorFlow   ì›ë˜ ë§ˆì§€ë§‰ìœ¼ë¡œ 4ê°œì˜ seed ë¥¼ ëª¨ë‘ ê³ ì •í•˜ê³  ì‹¤í—˜ë„ í•´ë³´ë ¤ê³  í–ˆëŠ”ë° TensorFlow ë§Œ ê³ ì •í•´ë„ ê±°ì˜ ë™ì¼í•œ ê²°ê³¼ê°€\u0026hellip;ë‚˜ì˜¤ë”êµ°ìš”. (ì˜ˆì „ì—” ì•„ë‹ˆì—ˆëŠ”ë°\u0026hellip;)\nTensorFlow ë¥¼ ì“°ì‹œëŠ” ë¶„ë“¤ì€ tf ë§Œ ê³ ì •í•´ë„ ì–´ëŠì •ë„ ê· ì¼í•œ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\nP.S  ì •ë§ ê°„ë‹¨í•œ ì‹¤í—˜ì´ë‹ˆ ë„ˆë¬´ ë¯¿ì§€ëŠ” ë§ˆì„¸ìš”.  ","permalink":"https://jjerry-test.github.io/blog/seed/","tags":["TensorFlow"],"title":"Seed ê°€ ë­ê¸¸ë˜.."},{"categories":["Ubuntu"],"contents":"# nvidia docker ì„œë¹„ìŠ¤ ì‹œì‘ sudo service nvidia-docker start # íŠ¹ì • í¬íŠ¸ í„°ë„ë§, ê²½ë¡œ ë§ˆìš´íŠ¸, Docker image:tag ë¥¼ ì´ìš©í•˜ì—¬ container ë¡œ ì‹¤í–‰ docker run -ti -rm -p {í˜¸ìŠ¤íŠ¸í¬íŠ¸}:{ë„ì»¤í¬íŠ¸} -v {í˜¸ìŠ¤íŠ¸ê²½ë¡œ}:{ë„ì»¤ê²½ë¡œ} {IMAGE_NAME:tag} bash # ì¢…ë£Œëœ Container ì‚­ì œ docker rm {CONTAINER_NAEM} # Docker Image ì‚­ì œ docker rmi {IMAGE_NAME} # ì¢…ë£Œëœ Containerë¥¼ ì‚­ì œí•˜ì§€ ì•Šê³  ìƒˆë¡œìš´ IMAGEë¡œ ìƒì„± docker commit {CONTAINER_NAEM} {IMAGE_NAME:TAG} # Docker container ë‚´ì—ì„œ jupyter notebook í˜¹ì€ jupyter labì„ ì‹¤í–‰í• ë•Œ ì‚¬ìš©ë˜ëŠ” ì»¤ë§¨ë“œ CUDA_VISIBLE_DEVICES=0 jupyter notebook --ip=0.0.0.0 --port=í¬íŠ¸ë²ˆí˜¸ --allow-root ","permalink":"https://jjerry-test.github.io/blog/docker/","tags":["Docker"],"title":"ìì£¼ ì‚¬ìš©í•˜ëŠ” Docker Command ì •ë¦¬"},{"categories":["DeepLearning"],"contents":"ì˜¤ëŠ˜ì€ Weights \u0026amp; Biases ì˜ ê¸°ëŠ¥ì¤‘ í•˜ë‚˜ì¸ sweepì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë ¤ê³  í•©ë‹ˆë‹¤.\nsweepì€ ì •ë§ ì‰½ê²Œ ë§í•´ì„œ Hyper parameter search and model optimization ì„ ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.\nìì„¸í•œ ì¥ë‹¨ì ì— ëŒ€í•´ì„œ ê¶ê¸ˆí•˜ì‹  ë¶„ì€ ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ì„œ í™•ì¸í•´ì£¼ì„¸ìš”!\nê·¸ëŸ¼ ì‚¬ìš©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n1. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±.  model.pyì™€ train.py ì´ë ‡ê²Œ ë‘ ê°œì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.  # model.py import tensorflow as tf from tensorflow.keras import models, layers, losses, optimizers from tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201 from tensorflow.keras.applications import VGG16, VGG19, Xception, InceptionResNetV2, InceptionV3 from tensorflow.keras.applications import MobileNet, MobileNetV2, NASNetLarge, NASNetMobile from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152, ResNet50V2, ResNet101V2, ResNet152V2 model_dict = { \u0026#34;vgg16\u0026#34;: VGG16, \u0026#34;vgg19\u0026#34;: VGG19, \u0026#34;resnet50\u0026#34;: ResNet50, \u0026#34;resnet101\u0026#34;: ResNet101, \u0026#34;resnet152\u0026#34;: ResNet152, \u0026#34;resnet50v2\u0026#34;: ResNet50V2, \u0026#34;resnet101v2\u0026#34;: ResNet101V2, \u0026#34;resnet152v2\u0026#34;: ResNet152V2, \u0026#34;densenet121\u0026#34;: DenseNet121, \u0026#34;densenet169\u0026#34;: DenseNet169, \u0026#34;densenet201\u0026#34;: DenseNet201, \u0026#34;mobilenet\u0026#34;: MobileNet, \u0026#34;mobilenetv2\u0026#34;: MobileNetV2, \u0026#34;xception\u0026#34;: Xception, \u0026#34;inceptionresnetv2\u0026#34;: InceptionResNetV2, \u0026#34;inceptionv3\u0026#34;: InceptionV3, \u0026#34;nasnetlarge\u0026#34;: NASNetLarge, \u0026#34;nasnetmobile\u0026#34;: NASNetMobile } def build_model(config, num_classes=10, name=\u0026#34;model\u0026#34;): assert config.model_name.lower() in model_dict.keys(), f\u0026#34;Please, check pretrained model list {list(model_dict.keys())}\u0026#34; last_activation = \u0026#34;softmax\u0026#34; if num_classes \u0026gt; 1 else \u0026#34;sigmoid\u0026#34; base_model = model_dict[config.model_name.lower()](include_top=False, weights=\u0026#34;imagenet\u0026#34;, pooling=\u0026#34;avg\u0026#34;) if config.freeze: base_model.trainable = False output = layers.Dropout(config.dropout, name=f\u0026#34;{name}_dropout\u0026#34;)(base_model.output) output = layers.Dense(num_classes, last_activation, name=f\u0026#34;{name}_output\u0026#34;)(output) model = models.Model(base_model.input, output, name=name) return model #train.py import os import cv2 as cv import numpy as np import tensorflow as tf from tensorflow.keras import optimizers, utils from model import * import wandb from wandb.keras import WandbCallback tf.random.set_seed(42) hyperparameter_defaults = dict( model_name=\u0026#34;mobilenet\u0026#34;, dropout = 0.5, freeze = 1, batch_size = 128, learning_rate = 0.001, epochs = 5, GPUs=\u0026#34;0\u0026#34; ) wandb.init(project=\u0026#34;usage_02\u0026#34;, config=hyperparameter_defaults) config = wandb.config os.environ[\u0026#34;CUDA_DEVICE_ORDER\u0026#34;]=\u0026#34;PCI_BUS_ID\u0026#34; os.environ[\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;]=config.GPUs # For Efficiency gpus = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(\u0026#39;GPU\u0026#39;) print(len(gpus), \u0026#34;Physical GPUs,\u0026#34;, len(logical_gpus), \u0026#34;Logical GPUs\u0026#34;) except RuntimeError as e: print(e) # Data Prepare URL = \u0026#39;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\u0026#39; path_to_zip = tf.keras.utils.get_file(\u0026#39;flower_photos.tgz\u0026#39;, origin=URL, extract=True) PATH = os.path.join(os.path.dirname(path_to_zip), \u0026#39;flower_photos\u0026#39;) category_list = [i for i in os.listdir(PATH) if os.path.isdir(os.path.join(PATH, i)) ] print(category_list) num_classes = len(category_list) img_size = 128 def read_img(path, img_size): img = cv.imread(path) img = cv.cvtColor(img, cv.COLOR_BGR2RGB) img = cv.resize(img, (img_size, img_size)) return img imgs_tr = [] labs_tr = [] imgs_val = [] labs_val = [] for i, category in enumerate(category_list): path = os.path.join(PATH, category) imgs_list = os.listdir(path) print(\u0026#34;Total \u0026#39;%s\u0026#39; images : %d\u0026#34;%(category, len(imgs_list))) ratio = int(np.round(0.05 * len(imgs_list))) print(\u0026#34;%sImages for Training : %d\u0026#34;%(category, len(imgs_list[ratio:]))) print(\u0026#34;%sImages for Validation : %d\u0026#34;%(category, len(imgs_list[:ratio]))) print(\u0026#34;=============================\u0026#34;) imgs = [read_img(os.path.join(path, img),img_size) for img in imgs_list] labs = [i]*len(imgs_list) imgs_tr += imgs[ratio:] labs_tr += labs[ratio:] imgs_val += imgs[:ratio] labs_val += labs[:ratio] imgs_tr = np.array(imgs_tr)/255. labs_tr = utils.to_categorical(np.array(labs_tr), num_classes) imgs_val = np.array(imgs_val)/255. labs_val = utils.to_categorical(np.array(labs_val), num_classes) print(imgs_tr.shape, labs_tr.shape) print(imgs_val.shape, labs_val.shape) # Build Network strategy = tf.distribute.MirroredStrategy() with strategy.scope(): model = build_model(config, num_classes) loss = \u0026#39;binary_crossentropy\u0026#39; if num_classes==1 else \u0026#39;categorical_crossentropy\u0026#39; model.compile(optimizer=optimizers.Adam(config.learning_rate), loss=loss, metrics=[\u0026#39;acc\u0026#39;]) # Training Network model.fit(x=imgs_tr, y=labs_tr, batch_size=config.batch_size, epochs=config.epochs, callbacks = [WandbCallback()], validation_data=(imgs_val, labs_val)) 2. ê¸°ë³¸ ê°’ìœ¼ë¡œ í•™ìŠµ ì§„í–‰. python train.py ê·¸ í›„ì— Weights \u0026amp; Biases ë¡œ ê°€ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ í”„ë¡œì íŠ¸ê°€ ë§Œë“¤ì–´ì§„ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n  3. sweep configuration í”„ë¡œì íŠ¸ ì°½ì—ì„œ ì™¼ìª½ì— ë¹—ìë£¨ ëª¨ì–‘ ì•„ì´ì½˜ì„ ëˆ„ë¥¸ í›„ì— ì˜¤ë¥¸ìª½ ìƒë‹¨ì— Create sweep ë¥¼ ëˆŒëŸ¬ì¤ë‹ˆë‹¤. ****\n  ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì°½ì´ ë‚˜ì˜¤ëŠ”ë°ìš”. ì´ì œ hyper parameter searchë¥¼ ì–´ë–»ê²Œ í• ê±´ì§€ ì„¸íŒ…í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.\n  ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„¸íŒ…ì„ í–ˆìŠµë‹ˆë‹¤.\nì´ë ‡ê²Œ í•˜ë©´ grid search\u0026hellip;ë‹ˆê¹Œ\u0026hellip;.\n2 * 5 * 3 * 2 * 3 = 180 ê°œì˜ ê²°ê³¼ê°€ ë‚˜ì˜¤ê² êµ°ìš”\u0026hellip;\nprogram: train.py method: grid metric: name: loss goal: minimize parameters: GPUs: value: \u0026#34;0\u0026#34; epochs: value: 10 freeze: values: [0, 1] dropout: values: [0.1, 0.2, 0.4, 0.5, 0.7] batch_size: values: [64, 128, 256] model_name: values: [mobilenet, mobilenetv2] learning_rate: values: [0.001, 0.005, 0.0005] ê·¸ë¦¬ê³  Initialize Sweep ë¥¼ ëˆŒëŸ¬ì¤ë‹ˆë‹¤! ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì€ í™”ë©´ì´ ë‚˜ì˜¬ê±°ì—ìš”!\n  4. Sweep ì‹¤í–‰ ê°€ìš´ë°ì— ì í˜€ìˆëŠ” ì»¤ë§¨ë“œë¥¼ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰ì‹œì¼œì¤ë‹ˆë‹¤!\nwandb agent {sweep-id} ê·¸ëŸ¬ë©´ í„°ë¯¸ë„ì— wandb: Starting wandb agent ğŸ•µï¸ ì´ëŸ° ë¬¸êµ¬ì™€ í•¨ê»˜ ì„¸íŒ…ëŒ€ë¡œ Networkë¥¼ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤!\nì´ì œ ì ì‹œ í‹°íƒ€ì„ì„\u0026hellip;ê°€ì§‘ë‹ˆë‹¤.. (ì ê¹ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŒ..)\n  5. ê²°ê³¼ í™•ì¸ Weights \u0026amp; Biases í™”ë©´ì—ì„œ View Sweep ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!\nê·¸ë¦¬ê³  ì¢Œì¸¡ì— Plot ì•„ì´ì½˜ì„ ëˆ„ë¥´ë©´ ë‹¤ìŒê³¼ ê°™ì€ í™”ë©´ì´ ë‚˜ì˜µë‹ˆë‹¤!\nì›ë˜ëŠ” ìœ„ì— Chart Panel ìˆëŠ”ë° ì ‘ì–´ë†¨ìŠµë‹ˆë‹¤.\nì´ Panelì€ ê¸°ë³¸ì ìœ¼ë¡œ ì„¸ ê°œì˜ ê·¸ë˜í”„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n ê° í•™ìŠµ ë³„ ê²°ê³¼ ë¶„í¬ë„ hyper parameterì™€ ëª©í‘œ ê°’( ì €ëŠ” loss ) ê°„ì˜ ì¤‘ìš”ë„, ìƒê´€ê´€ê³„ ê° í•™ìŠµ ë³„ Parallel graph    ì´ë²ˆì—” Hyper parameter tuningì„ í¸í•˜ê²Œ í•  ìˆ˜ ìˆëŠ” Weighs \u0026amp; Biasesì˜ Sweepì— ëŒ€í•´ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤.\nì¼ì¼íˆ ê°’ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ì§€ì •í•´ ë†“ìœ¼ë©´ ì•Œì•„ì„œ ì§„í–‰ì„ í• í…Œë‹ˆ..ì‚¬ìš©ìëŠ” ì¢€ ì‰´\u0026hellip;\u0026hellip;ìˆ˜ ìˆê² ì£ ..? (ì•„ë‹ ë“¯..)\nê·¸ëŸ¼ í¬ìŠ¤íŒ…ì„ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤! ê°ì‚¬í•©ë‹ˆë‹¤!\nP.S   wandb sweep ì‹¤í–‰ì„ background ì—ì„œ í•˜ëŠ” ë°©ë²•\u0026hellip;?\nnohup wandb agent {sweep_id} \u0026gt; nohup.out   ì´ ë‹¤ìŒì€ ë­ í•˜ì§€\u0026hellip;\n  ","permalink":"https://jjerry-test.github.io/blog/sweep/","tags":["Tools"],"title":"Weights \u0026 Biases Usage - [2]"},{"categories":["DeepLearning"],"contents":"ì´ë²ˆ í¬ìŠ¤íŒ…ì€ Weights \u0026amp; Biases ë¥¼ í™œìš©í•˜ì—¬ Network ì„±ëŠ¥ ë¹„êµ ì˜ˆì‹œë¥¼ í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\nì–´ë ¤ìš´ ê¸€ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ê¸ˆë°© ê¸ˆë°© ë”°ë¼í•˜ì‹¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤!\nTask  Flower classification  daisy, roses, dandelion, sunflowers, tulips   List of pretrained-model  tf.keras.applications   Detail of training  Image size: 150x150x3 Epochs: 5 Batch size: 256 Freezing: True    ìì„¸í•œ ì½”ë“œëŠ” for_wandb ë¼ëŠ” repositoryì— ì˜¬ë ¤ë†¨ìœ¼ë‹ˆ í™•ì¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\ntrain.shë¥¼ ì‹¤í–‰í•˜ê²Œ ë˜ë©´ ìœ„ì— ì í˜€ìˆëŠ” pretrined model ë§Œ ë³€ê²½í•´ì„œ classification ì„ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤!\nê·¸ í›„ weights \u0026amp; biases í™”ë©´ì„ ê°€ì„œ í™•ì¸í•´ ë³´ë©´\u0026hellip;\nê° ëª¨ë¸ë³„ loss, acc, val_loss, val_acc graphë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n  ì¢Œì¸¡ ìƒë‹¨ì— Run ì˜†ì„ ë³´ì‹œë©´ í…Œì´ë¸” ëª¨ì–‘ ì•„ì´ì½˜ì´ ìˆëŠ”ë° ì´ë¥¼ ëˆ„ë¥´ë©´ ë‹¤ìŒê³¼ ê°™ì´ í…Œì´ë¸”ë¡œ ì •ë¦¬ë˜ì–´ ìˆëŠ” ê²ƒì„ í™•ì¸ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n  ì´ë²ˆ í¬ìŠ¤íŒ…ì€ Weights \u0026amp; Biasesë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ ë³„ ì„±ëŠ¥ ë¹„êµë¥¼ í•´ë³´ì•˜ìŠµë‹ˆë‹¤.\nì¡°ë§Œê°„\u0026hellip;Sweep ì´ë¼ëŠ” Hyperparameter search and model optimization ë°©ë²•ì— ëŒ€í•´ í¬ìŠ¤íŒ…ì„ í•´ë³´ë ¤ í•©ë‹ˆë‹¤!\nP.S  ë§ˆìŠ¤í¬ ë‹µë‹µ\u0026hellip;.. ë‚ ì”¨ í›„ë¥ì§€ê·¼, ì§œì¦\u0026hellip;.  ","permalink":"https://jjerry-test.github.io/blog/wandb_usage_1/","tags":["Tools"],"title":"Weights \u0026 Biases Usage - [1]"},{"categories":["DeepLearning"],"contents":" ë³¸ í¬ìŠ¤íŒ…ì€ ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ Data Science \u0026amp; Business Analytics ì—°êµ¬ì‹¤ì˜ ê°•í•„ì„± êµìˆ˜ë‹˜ì˜ ìë£Œë¥¼ ì •ë¦¬í•œ í¬ìŠ¤íŒ…ì…ë‹ˆë‹¤.\n  Contents of Posting  Contents of Posting Paper Reading Roadmap  ML Basics Data Mining  General Patter Mining Clustering   Artificial Intelligence  General Reinforcement Learning Transfer Learning   Supervised Learning  Kernel Machines Ensemble   Semi-supervvised Learning Unsupervised Learning Neural Network  General Structure Learning Strategies   NLP  General Topic Modeling Repersentation Learning Classification Summarization Machine Translation Question Answering   Vision  Classification Object Detection Localization \u0026amp; Segmentation      Paper Reading Roadmap ML Basics  The matrix calculus you need for deep learning Statistical Modeling: The Two Cultures Machine learning: Trends, perspectives, and prospects An introduction to ROC analysis Learning from imbalanced data Variational inference: A review for statisticians The expectation-maximization algorithm Dimension Reduction: A Guided Tour  Data Mining General  Interestingness Measures for Data Mining: A Survey The PageRank citation ranking: Bringing order to the web Process Mining Manifesto An Introduction to Variable and Feature Selection  Patter Mining  Fast Algorithm for Mining Association Rules A survey of sequential pattern mining A Survey of Parallel Sequential Pattern Mining  Clustering  A density-based algorithm for discovering clusters in large spatial databases with noise Data Clustering: A Review Techniques of Cluster Algorithms in Data Mining Survey of Clustering Data Mining Techniques On Clustering Validation Techniques clValid: An R Package for Cluster Validation  Artificial Intelligence General  Learning Deep Architectures for AI Representation learning: A review and new perspectives Generative Adversarial Networks From evolutionary computation to the evolution of things Probabilistic machine learning and artificial intelligence AutoML: A Survey of the State-of-the-Art  Reinforcement Learning  Human-level control through deep reinforcement Mastering the game of Go with deep neural networks and tree search An Introduction to Deep Reinforcement Learning World Models  Transfer Learning  Zero-shot learning through cross-modal transfer Lifelong Learning with Dynamically Expandable Networks  Supervised Learning Kernel Machines  An Introduction to Kernel-based Learning Algorithms A Tutorial on Support Vector Machine for Pattern Recognition A Tutorial on Support Vector Regression A Tutorial on nu-Support Vector Machines  Ensemble  Bagging Predictors Random Forests A short introduction to boosting Greedy Function Approximation: A Gradient Boosting Machine Gradient Boosting Machine, A Tutorial XGBoost: A Scalable Tree Boosting System LightGBM: A Highly Efficient Gradient Boosting Decision Tree CatBoost : unbiased boosting with categorical features  Semi-supervvised Learning  Combining Labeled and Unlabeled Data with Co-Training Semi-supervised Learning with Deep Generative Models Semi-Supervised Classification with Graph Convolutional Networks MixMatch: A Holistic Approach to Semi-Supervised Learning ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence  Unsupervised Learning  Anomaly Detection: A Survey Deep Learning for Anomaly Detection: A Survey A Review of Novelty Detection LOF: Identifying Density-Based Local Outliers Support Vector Data Description Isolation Forest Isolation-based Anomaly Detection DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning  Neural Network General  Deep learning  Structure  Long Short-Term Memory LSTM: A Search Space Odyssey Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling Sequence to sequence learning with neural networks Memory Networks End-To-End Memory Networks WaveNet: A Generative Model for Raw Audio An Introduction to Variational Autoencoders A Comprehensive Survey on Graph Neural Networks  Learning Strategies  Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Dropout: A Simple Way to Prevent Neural Networks from Overtting ADAM: A Method for Stochastic Optimization An overview of gradient descent optimization algorithms Layer normalization Group normalization  NLP General  Natural Language Processing (Almost) from Scratch Advances in natural language processing Recent trends in deep learning based natural language processing  Topic Modeling  An introduction to latent semantic analysis Probabilistic latent semantic analysis Probabilistic topic models Latent Dirichlet Allocation  Repersentation Learning  A Neural Probabilistic Language Model Distributed representations of words and phrases and their compositionality Efficient Estimation of Word Representations in Vector Space Glove: Global vectors for word representation Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation Enriching word vectors with subword information Bert: Pre-training of deep bidirectional transformers for language understanding Deep contextualized word representations Improving language understanding by generative pre-training Language models are unsupervised multitask learners Language Models are Few-Shot Learners  Classification  Convolutional neural networks for sentence classification Deep learning for sentiment analysis: A survey  Summarization  TextRank: Bringing Order into Texts A Neural Attention Model for Abstractive Sentence Summarization  Machine Translation  On the Properties of Neural Machine Translation: Encoder-Decoder Approaches Effective Approaches to Attention-based Neural Machine Translation Neural Machine Translation by Jointly Learning to Aligh and Translate Google\u0026rsquo;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation Attention is all you need  Question Answering  VQA: Visual Question Answering Ask Me Anything: Dynamic Memory Networks for Natural Language Processing Squad: 100,000+ questions for machine comprehension of text Know what you don\u0026rsquo;t know: Unanswerable questions for SQuAD  Vision Classification  Imagenet classification with deep convolutional neural networks Visualizing and understanding convolutional networks Very Deep Convolutional Networks for Large-Scale Image Recognition Going deeper with convolutions Deep residual learning for image recognition Densely Connected Convolutional Networks  Object Detection  Overfeat: Integrated recognition, localization and detection using convolutional networks Rich feature hierarchies for accurate object detection and semantic segmentation Fast R-CNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks You Only Look Once: Unified, Real-Time Object Detection YOLO9000: Better, Faster, Stronger YOLOv3: An Incremental Improvement YOLOv4: Optimal Speed and Accuracy of Object Detection  Localization \u0026amp; Segmentation  U-Net: Convolutional Networks for Biomedical Image Segmentation Learning deep features for discriminative localization Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization  ","permalink":"https://jjerry-test.github.io/blog/paper_roadmap/","tags":["Paper"],"title":"Paper Reading Roadmap"},{"categories":["DeepLearning"],"contents":"ì–´ì œëŠ” RAPIDSì— ëŒ€í•œ ì†Œê°œì™€ RAPIDS APIs ì¤‘ cuDF ì— ëŒ€í•œ ì˜ˆì œì— ëŒ€í•œ í¬ìŠ¤íŒ…ì„ í–ˆìŠµë‹ˆë‹¤.\nì˜¤ëŠ˜ì€ RAPIDS APIS ì¤‘ cuML ì— ëŒ€í•œ ì˜ˆì œ í¬ìŠ¤íŒ…ì„ ê°„.ë‹¨.í•˜.ê²Œ í•´ë³´ê² ìŠµë‹ˆë‹¤.\në¹„êµë¥¼ ìœ„í•´ KNN Classifierë¥¼ ì¤€ë¹„í•˜ì˜€ê³  ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´ Scikit-learn ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\n# %% # RAPIDS cuML kNN model import time import cudf, cuml import pandas as pd from cuml.neighbors import KNeighborsClassifier as cuKNeighbors from sklearn.neighbors import KNeighborsClassifier as skKNeighbors print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;============= Using cuML ===============\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========================================\u0026#34;) train = cudf.read_csv(\u0026#39;./mnist/train.csv\u0026#39;) test = cudf.read_csv(\u0026#39;./mnist/test.csv\u0026#39;) start = time.time() model = cuKNeighbors(n_neighbors=7) model.fit(train.iloc[:,1:785], train.iloc[:,0]) # y_hat = model.predict(test) # Exception occured (version 0.14.0)  print(f\u0026#34;Elapsed Time: {time.time()-start}\\n\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========== Using Scikit-learn ==========\u0026#34;) print(\u0026#34;========================================\u0026#34;) print(\u0026#34;========================================\u0026#34;) train = pd.read_csv(\u0026#39;./mnist/train.csv\u0026#39;) test = pd.read_csv(\u0026#39;./mnist/test.csv\u0026#39;) start = time.time() model = skKNeighbors(n_neighbors=7) model.fit(train.iloc[:,1:785], train.iloc[:,0]) # y_hat = model.predict(test) print(f\u0026#34;Elapsed Time: {time.time()-start}\u0026#34;) ======================================== ======================================== ============= Using cuML =============== ======================================== ======================================== Elapsed Time: 0.3921339511871338 ======================================== ======================================== ========== Using Scikit-learn ========== ======================================== ======================================== Elapsed Time: 23.88076663017273 Test ì‹œ ì„±ëŠ¥ë„ ë¹„êµí•˜ë ¤í–ˆìœ¼ë‚˜ cuML ë²„ì „ ì—ëŸ¬ë¡œ ì¸í•´ í…ŒìŠ¤íŠ¸ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤.\nê·¸ë˜ë„ fit ë¶€ë¶„ì—ì„œ ë§¤ìš° í° ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤!\nì—­ì‹œë‚˜\u0026hellip;.GPUë„¤ìš”!\nP.S  ë„ˆë¬´ ê°„ë‹¨í•˜ê²Œ í…ŒìŠ¤íŠ¸í•˜ëŠ”ê±´ê°€\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/rapids-cuml/","tags":["Tools"],"title":"RAPIDS APIs ( cuML )"},{"categories":["DeepLearning"],"contents":" RAPIDS is a suite of software libraries for executing end-to-end data science \u0026amp; analytics pipelines entirely on GPUs.\n ìœ„ ê¸€ì€ RAPIDS ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ ë‚´ìš©ì…ë‹ˆë‹¤.\në§ ê·¸ëŒ€ë¡œ ëª¨ë“  ê³¼ì •ì„ GPU ì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\nPREREQUISITES GPU\n NVIDIA Pascalâ„¢ or better withÂ compute capabilityÂ 6.0+  OS\n Ubuntu 16.04/18.04 or CentOS 7 withÂ gcc/++Â 7.5+ SeeÂ RSN 1Â for details on our recent update toÂ gcc/++Â 7.5Â RHEL 7 support is provided through CentOS 7 builds/installs  Docker\n Docker CE v19.03+ andÂ nvidia-container-toolkit Legacy SupportÂ - Docker CE v17-18 andÂ nvidia-docker2  CUDA \u0026amp; NVIDIA Drivers\n One of the following supported versions: 10.0Â \u0026amp; v410.48+Â 10.1.2Â \u0026amp; v418.87+Â 10.2Â \u0026amp; v440.33+  RAPIDS APIs  RAPIDS ì—ëŠ” í˜„ì¬ 5ê°œì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì œê³µ. cuDF ( ìš”ê²ƒì´ Main )  Python GPU DataFrame library   cuML  a suite of libraries that implement machine learning algorithms and mathematical primitives functions   cuGraph  a GPU accelerated graph analytics library   nvStrings  a pandas-like API that will be familiar to data engineers \u0026amp; data scientists   Cyber Log Accelerators(CLX)  a collection of RAPIDS examples for security analysts, data scientists, and engineers to quickly get started applying RAPIS and GPU acceleration to real-world cybersecurity use cases    Installation Guide https://rapids.ai/start.html#get-rapids\nAnaconda or Miniconda  ì„¤ì¹˜í•˜ëŠ”ë° ì‹œê°„ì´ ê½¤ë‚˜ ì˜¤ë˜ ê±¸ë ¤ìš”\u0026hellip;  conda install -c rapidsai -c nvidia -c conda-forge \\  -c defaults rapids=0.14 python=3.7 cudatoolkit=10.1 # cudatoolkitì€ ì‚¬ìš©í•˜ëŠ” DL í”„ë ˆì„ì›Œí¬ í˜¸í™˜ì„± ê³ ë ¤í•˜ì„¸ìš”! Example ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„  cuDF ì— ëŒ€í•œ ì˜ˆì œë¥¼ ë³´ì—¬ë“œë¦¬ë ¤ í•©ë‹ˆë‹¤.\nì¶”ê°€ì ìœ¼ë¡œ Dask, Dask-cuDF ë¼ëŠ” 2ê°œì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\në¨¼ì € ì‚¬ìš©í•  Package ë“¤ì„ import í•´ì¤ë‹ˆë‹¤.\nimport os import numpy as np import pandas as pd import cudf import dask_cudf ë‹¤ìŒ ì˜ˆì‹œëŠ” pandasì™€ cudf ë¥¼ ë¹„êµí•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\n# Series s = pd.Series([1,2,3,None,4]) print(\u0026#34;Pandas\u0026#34;) print(s) gs = cudf.Series([1,2,3,None,4]) print(\u0026#34;\\ncuDF\u0026#34;) print(gs) dask_gs = dask_cudf.from_cudf(gs, npartitions=2) print(\u0026#34;\\nDask-cuDF\u0026#34;) print(dask_gs.compute()) Pandas 0 1.0 1 2.0 2 3.0 3 NaN 4 4.0 dtype: float64 cuDF 0 1 1 2 2 3 3 null 4 4 dtype: int64 Dask-cuDF 0 1 1 2 2 3 3 null 4 4 dtype: int64 # DataFrame  pdf = pd.DataFrame({\u0026#39;a\u0026#39;: list(range(20)), \u0026#39;b\u0026#39;: list(reversed(range(20))), \u0026#39;c\u0026#39;: list(range(20))}) print(\u0026#34;\\nPandas\u0026#34;) print(pdf.head(5)) gdf = cudf.DataFrame.from_pandas(pdf) print(\u0026#34;\\ncuDF\u0026#34;) print(gdf.head(5)) dask_gdf = dask_cudf.from_cudf(gdf, npartitions=2) print(\u0026#34;\\nDask-cuDF\u0026#34;) print(dask_gdf.compute().head(5)) Pandas a b c 0 0 19 0 1 1 18 1 2 2 17 2 3 3 16 3 4 4 15 4 cuDF a b c 0 0 19 0 1 1 18 1 2 2 17 2 3 3 16 3 4 4 15 4 Dask-cuDF a b c 0 0 19 0 1 1 18 1 2 2 17 2 3 3 16 3 4 4 15 4 # Sorting print(\u0026#34;\\nPandas\u0026#34;) print(pdf.sort_values(by=\u0026#39;b\u0026#39;).head(5)) print(\u0026#34;\\ncuDF\u0026#34;) print(gdf.sort_values(by=\u0026#39;b\u0026#39;).head(5)) print(\u0026#34;\\nDask-cuDF\u0026#34;) print(dask_gdf.sort_values(by=\u0026#39;b\u0026#39;).compute().head(5)) Pandas a b c 19 19 0 19 18 18 1 18 17 17 2 17 16 16 3 16 15 15 4 15 cuDF a b c 19 19 0 19 18 18 1 18 17 17 2 17 16 16 3 16 15 15 4 15 Dask-cuDF a b c 19 19 0 19 18 18 1 18 17 17 2 17 16 16 3 16 15 15 4 15 ì´ë²ˆì—” í° DataFrameì„ ë§Œë“¤ì–´ì„œ sorting ì†ë„ë¥¼ ë¹„êµí•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\n# %% # Speed Test num_element = 1000000 pdf = pd.DataFrame({\u0026#39;a\u0026#39;: list(range(num_element)), \u0026#39;b\u0026#39;: list(reversed(range(num_element))), \u0026#39;c\u0026#39;: np.random.randint(0, 1200000, num_element)}) print(pdf.head(5)) gdf = cudf.DataFrame.from_pandas(pdf) print(gdf.head(5)) dask_gdf = dask_cudf.from_cudf(gdf, npartitions=2) print(dask_gdf.compute().head(5)) ì‹¤í–‰ ì†ë„ëŠ” ipythonì˜ timeit ì´ë¼ëŠ” magic function ì„ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\n%%timeit pdf.sort_values(by=\u0026#34;c\u0026#34;) %%timeit gdf.sort_values(by=\u0026#34;c\u0026#34;) %%timeit dask_gdf.sort_values(by=\u0026#34;c\u0026#34;).compute() num_elementë¥¼ ë°”ê¿”ê°€ë©´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë´¤ìŠµë‹ˆë‹¤.\n# num_element = 1000 448 Âµs Â± 38.8 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each) 2.84 ms Â± 1.56 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each) 83.4 ms Â± 47.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) # num_element = 10000 1.39 ms Â± 42.8 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each) 3.14 ms Â± 1.99 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each) 89.9 ms Â± 24.3 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) # num_element = 100000 13.8 ms Â± 467 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each) 5.61 ms Â± 2.55 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each) 116 ms Â± 29.3 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) #num_element = 1000000 225 ms Â± 31.4 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) 8.17 ms Â± 534 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each) 118 ms Â± 27 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) ì ì€ ì–‘ì—ì„œëŠ” CPU ê°€ ë” ë¹ ë¥´ì§€ë§Œ ì ì  ëŒ€ê·œëª¨ë¡œ ëŠ˜ì–´ë‚˜ë‹ˆ\u0026hellip;.GPU ê°€ ë¹ ë¥´êµ°ìš”.\nì•„! Dask_cuDFëŠ” ë³´í†µ multi gpu ì¼ ë•Œ ì‚¬ìš©í•˜ëŠ”ë° ì œê°€ ì‹¤í—˜í•œ í™˜ê²½ì€ NIPA ìˆ˜ì‹œ ì‚¬ìš©ì\u0026hellip;í™˜ê²½ì´ë¼ V100 ë‹¨ì¼ í™˜ê²½ì´ì—ˆìŠµë‹ˆë‹¤.\nê·¸ë˜ì„œ\u0026hellip;dask_gdf ì— ëŒ€í•œ ì†ë„ëŠ” ë¯¿ì§€ ë§ˆì„¸ìš”..\nP.S  ì„œí„°ë ˆìŠ¤ë¥¼ ë§ì´ ë°›ìŠµë‹ˆë‹¤. ì›ì¸ì€\u0026hellip;.\u0026lsquo;ê·¸\u0026rsquo; ì§‘íšŒ\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/rapids/","tags":["Tools"],"title":"RAPIDS!!!"},{"categories":["Ubuntu"],"contents":"ì§€ê·¹íˆ ê°œì¸ì´ ì‚¬ìš©í•˜ê¸° ìœ„í•œ vimrc   ë§ˆìŒê» í¸í•˜ì‹ ëŒ€ë¡œ Copy \u0026amp; Paste í•˜ì„¸ìš”!  call plug#begin(\u0026#39;~/.vim/plugged\u0026#39;)Plug \u0026#39;davidhalter/jedi-vim\u0026#39;Plug \u0026#39;preservim/nerdtree\u0026#39;Plug \u0026#39;jeetsukumaran/vim-pythonsense\u0026#39;Plug \u0026#39;jiangmiao/auto-pairs\u0026#39;Plug \u0026#39;Vimjas/vim-python-pep8-indent\u0026#39;Plug \u0026#39;dense-analysis/ale\u0026#39;Plug \u0026#39;neoclide/coc.nvim\u0026#39;, {\u0026#39;branch\u0026#39;: \u0026#39;release\u0026#39;}\u0026#34;Plug \u0026#39;liuchengxu/vista.vim\u0026#39;Plug \u0026#39;sheerun/vim-polyglot\u0026#39;\u0026#34;Plug \u0026#39;python-mode/python-mode\u0026#39;, { \u0026#39;for\u0026#39;: \u0026#39;python\u0026#39;, \u0026#39;branch\u0026#39;: \u0026#39;develop\u0026#39; }Plug \u0026#39;junegunn/seoul256.vim\u0026#39;call plug#end()\u0026#34; ì„¸ë¶€ ì •ë³´ ì¶œë ¥set nuset titleset showmatchset ruler\u0026#34; êµ¬ë¬¸ ê°•ì¡° ì‚¬ìš©if has(\u0026#34;syntax\u0026#34;) syntax onendif\u0026#34; ë“¤ì—¬ì“°ê¸° ì„¤ì •set autoindentset smartindentset tabstop=4set shiftwidth=4set softtabstop=4set smarttabset expandtab\u0026#34; í•œê¸€ ì…ë ¥ ì„¤ì •set encoding=utf-8set termencoding=utf-8\u0026#34; ì»¤ì„œê°€ ìˆëŠ” ì¤„ì„ ê°•ì¡°í•¨set cursorline\u0026#34; ìƒíƒœë°” í‘œì‹œë¥¼ í•­ìƒí•œë‹¤set laststatus=2set statusline=\\ %\u0026lt;%l:%v\\ [%P]%=%a\\ %h%m%r\\ %F\\\u0026#34; ê²€ìƒ‰ ì„¤ì •set ignorecase\u0026#34; ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì •ëœ ê³³ì— ì»¤ì„œë¥¼ ìœ„ì¹˜í•¨au BufReadPost *\\ if line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026gt; 0 \u0026amp;\u0026amp; line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026lt;= line(\u0026#34;$\u0026#34;) |\\ exe \u0026#34;norm g`\\\u0026#34;\u0026#34; |\\ endif\u0026#34;seoul256let g:seoul256_background = 233colo seoul256","permalink":"https://jjerry-test.github.io/blog/vimrc/","tags":["Setting"],"title":"ê°œì¸ì ì¸ Vim ì„¤ì •"},{"categories":["DeepLearning"],"contents":"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Author: Jun-Yan Zhuâˆ—, Taesung Parkâˆ—, Phillip Isola, Alexei A. Efros\nDate: Mar 30, 2017\nURL: https://arxiv.org/abs/1703.10593\nIntroduction  Image-to-image translation ì€ paired dataì˜ ìƒí™©ì—ì„œ ë§ì´ ì—°êµ¬. í•˜ì§€ë§Œ ì‹¤ì œ í™˜ê²½ì—ì„  ì´ëŸ° paired dataë¥¼ êµ¬í•˜ê¸° í˜ë“¦. ë³¸ ë…¼ë¬¸ì€ unpaired data ìƒí™©ì—ì„œ Networkê°€ image-to-image ë¥¼ ì˜ í•™ìŠµí•˜ëŠ” ê²ƒì— ì´ˆì ì„ ë§ì¶¤.    image     image   Formulation  \\(X, Y\\): X, Y ë„ë©”ì¸ì˜ ë°ì´í„° \\(G, F\\): X to Y generator, Y to X generator \\(D_X, D_Y\\): X, Y ë„ë©”ì¸ì— ëŒ€í•œ Discriminator  Adversarial Loss $$\\mathcal{L}_{GAN}(G, D_Y, X, Y) = \\mathbb{E}_{y\\sim p_{data}(y)}[\\log D_Y(y)] + \\mathbb{E}_{x\\sim p_{data}(x)}[\\log (1 - D_Y(G(x)))]$$\n$$\\mathcal{L}_{GAN}(F, D_X, Y, X) = \\mathbb{E}_{x\\sim p_{data}(x)}[\\log D_X(x)] + \\mathbb{E}_{y\\sim p_{data}(y)}[\\log (1 - D_x(F(y)))]$$\nCycle Consistency Loss  ê°ê°ì˜ X, Y ë°ì´í„°ë¥¼ Y, X ë°ì´í„°ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ X, Y ë°ì´í„° ë³µì›.  $$\\mathcal{L}_{cyc}(G, F) = \\mathbb{E}_{x\\sim p_{data}(x)}[||F(G(x)) - x||_1 + \\mathbb{E}_{y\\sim p_{data}(y)}[||G(F(y)) - y||_1$$\nFull Objective $$\\mathcal{L}(G, F, D_X, D_Y) = \\mathcal{L}_{GAN}(G, D_Y, X, Y) + \\mathcal{L}_{GAN}(F, D_X, Y, X) + \\lambda\\mathcal{L}_{cyc}(G, F)$$\n$$G^*,F^* = argmin_{G, F}argmax_{D_X, D_Y}\\mathcal{L}(G, F, D_X, D_Y)$$\n  image   Implementation Network Architecture  Generatorì—ì„œ Instance Normalization ì‚¬ìš©. PixelGANì´ ì•„ë‹Œ 70x70 PatchGAN ì‚¬ìš©.  Training detail  Loss ì—ì„œ \\(\\lambda\\) ëŠ” 10. Optimizer: Adam Learning rate: 0.002  Result   image     image     image   P.S  Cycle consistencyê°€ ë§¤ìš° ì‹ ë°• Image translation ì—ì„  Instance Normalization !  ","permalink":"https://jjerry-test.github.io/blog/cycle_gan/","tags":["Paper"],"title":"Review: Cycle GAN"},{"categories":["DeepLearning"],"contents":"Image-to-Image Translation with Conditional Adversarial Networks Author: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros Date: Nov 26, 2018 URL: https://arxiv.org/abs/1611.07004\nGANì„ ì´ìš©í•œ Image translation ì˜ ì‹œì´ˆì— ê°€ê¹Œìš´ Pix2Pixë¥¼ ì•Œì•„ë³´ë ¤ê³  í•©ë‹ˆë‹¤.\nìì„¸í•˜ê²Œ ë¦¬ë·°í•˜ì§„ ì•Šì„ ê²ë‹ˆë‹¤.\nIntroduction  ì´ ë…¼ë¬¸ì˜ main contributionì€ ë‹¤ì–‘í•œ ë¬¸ì œì—ì„œ Conditional GANì´ í•©ë¦¬ì ì¸ ê²°ê³¼ë¥¼ ìƒì„±í•œë‹¤ëŠ” ê²ƒì„ ì…ì¦í•˜ëŠ” ê²ƒ. ì´ë¥¼ ìœ„í•´ Image-to-Image translation ìœ¼ë¡œ ì—°êµ¬ ì§„í–‰.    Method  ì •ë§ ê°„ë‹¨í•œ êµ¬ì¡° ë‹¤ìŒ ì‚¬ì§„ì€ Edge(Sketch) ì˜ìƒì„ Photo ì˜ìƒìœ¼ë¡œ ë§Œë“œëŠ” ì˜ˆì‹œ.    Objective $$\\mathcal{L}{cGAN}(G, D) = \\mathbb{E}{x, y}[\\log D(x, y)] + \\mathbb{E}_{x, z}[\\log (1-D(x, G(x, z))]$$\n$$\\mathcal{L}{L1}(G) = \\mathbb{E}{x, y, z}[||y-G(x,z)||_1]$$\n$$G^* = argmin_Gmax_D\\mathcal{L}_{cGAN}(G, D) + \\lambda\\mathcal{L}_{L1}(G)$$\nNetwork architectures  DCGAN ì—ì„œ ì œì•ˆí•œ ë°©ë²•ìœ¼ë¡œ ê° blockì„ êµ¬ì„±.  Convolution â†’ BatchNorm â†’ ReLU    Generator with skips  U-Net ê³¼ ë¹„ìŠ·í•˜ë‚˜ Concatenateë¥¼ Addë¡œ ë³€ê²½.    Markovian discriminator (PatchGAN)  L1, L2 ë§Œ ì‚¬ìš©í•˜ë©´ blurry resultê°€ ë§Œë“¤ì–´ì§„ë‹¤ëŠ” ê²ƒì€ ë§ì´ ì•Œë ¤ì§„ ì‚¬í•­. Low frequency ë¶€ë¶„ì— ëŒ€í•œ ë¶€ë¶„ì€ L1 lossë¡œ High frequencyëŠ” GAN loss ê°€ ë‹´ë‹¹. ì¢€ ë” ë‹¤ì–‘í•œ High frequency ì— ì í•©í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ local image patch ì— ëŒ€í•´ discriminatorë¥¼ ì ìš©. (PatchGAN) NxN patch ì— ëŒ€í•´ ê°ê° real/fakeë¥¼ íŒë³„.    Optimization and Inference  D â†’ G ì™€ ê°™ì€ ìˆœì„œë¡œ í•™ìŠµ ì§„í–‰ Optimizer: Adam  Learning rate: 0.0002 Momentum $\\beta_1$: 0.5 Momentum $\\beta_2$: 0.999   Batch size: 1~10  Experimants  Semantic labels â†”photo Architectural labelsâ†’photo Mapâ†”aerial photo BWâ†’color photos Edgesâ†’photo Sketchâ†’photo Dayâ†’night Thermalâ†’color photos Photo with missing pixelsâ†’*inpainted photo*  Evaluation metrics  Amazon Mechanical Turk (AMT) FCN-score  Analysis of the objective function     Analysis of the generator architecture  U-Net ê¸°ë°˜ì˜ êµ¬ì¡°ë¡œ í–ˆì„ ë•Œê°€ í›¨ì”¬ ì¢‹ìŒ.      From PixelGANs to PatchGANs to ImageGANs  Discriminatorì˜ ì¶œë ¥ì„ 1x1, 16x16, 70x70, 286x286ê³¼ ê°™ì´ ì°¨ë¡€ë¡œ í‚¤ìš°ë©´ì„œ ì‹¤í—˜.      Perceptual validation  ì‚¬ëŒì´ ë³´ê¸°ì—ë„ L1 + cGANì´ ì¢‹ìŒ.     Colorizationì—ì„œëŠ” ì¢€ ë–¨ì–´ì§.    Semantic segmentation  í•´ë‹¹ task ì—ì„œëŠ” ì˜¤íˆë ¤ L1ê³¼ ê°™ì€ reconstruction lossë§Œì„ ì´ìš©í•œ êµ¬ì¡°ê°€ ì í•©í•´ ë³´ì„.      Community-driven Research  Twitter ì— ê³µê°œí•œ í›„ ë‹¤ë¥¸ ì—°êµ¬ìë“¤ì˜ ì‹¤í—˜.      P.S  ì¥ë§ˆì² \u0026hellip; ë¬¼ ì¡°ì‹¬í•˜ì„¸ìš”..  ","permalink":"https://jjerry-test.github.io/blog/pix2pix/","tags":["Paper"],"title":"Review: Pix2Pix"},{"categories":["DeepLearning"],"contents":"ì˜¤ëŠ˜ì€ neptune.ai ë¼ëŠ” íˆ´ì„ ì†Œê°œì‹œì¼œ ë“œë¦¬ë ¤ í•©ë‹ˆë‹¤.\nJupyterëŠ” ì•„ëŠ”ë°\u0026hellip;. Neptuneì€ ë˜ ë­ì—¬\u0026hellip;. ì•”íŠ¼ ì´ ë¶„ì•¼ëŠ” ì°¸.. íƒœì–‘ê³„ë¥¼ ì¢‹ì•„í•˜ëŠ” ë“¯í•©ë‹ˆë‹¤.\nì´ëŠ” NN ì‹¤í—˜ì„ í¸í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” íˆ´ì…ë‹ˆë‹¤.\nê° ì‹¤í—˜ ì„¸íŒ… ë³„ë¡œ Hyper parameter, Metricsë¥¼ ê¸°ë¡í•˜ê³  ì‹œê°í™”í•˜ì—¬ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nê·¸ë¦¬ê³  ê°œì¸ì—ê² 100GBì˜ ì €ì¥ì†Œë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— íƒ€ì¸ê³¼ ê³µìœ ë„ ê°€ëŠ¥í•´ìš”!\nì–´ì°Œë³´ë©´ Weights \u0026amp; Biases ë‘ ë¹„ìŠ·í•˜ì£ .\në‹¤ìŒì€ Neptune docs ì—ì„œ ê°€ì ¸ì˜¨ Neptuneì˜ íŠ¹ì§•ì…ë‹ˆë‹¤.\n data exploration and analysis â†’ decision science â†’ machine learning and deep learning ì™€ ê°™ì€ ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ”ë° ì í•©. Python, Jupyter Notebook, R ì—ì„œ ë™ì‘. Keras,Â PyTorch Lightning,Â XGBoost,Â Matplotlib ì™€ ê°™ì€ ML, DLì— ì‚¬ìš©ë˜ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì—°ë™(í†µí•©). MLflow,Â TensorBoardÂ , Sacred ì™€ ê°™ì€ Tracking tool ê³¼ ì—°ë™(í†µí•©) ê°€ëŠ¥. AWS, GCP, Kubernetes, Azure ì™€ ê°™ì€ Cloud ì™€ë„ ì›í™œíˆ ì‘ë™.  ê·¸ëŸ¼ ì‹¤ì œë¡œ í•œë²ˆ ì¨ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n Sign up ì€ ë„˜ì–´ê°‘ë‹ˆë‹¤.\n 1. ê¸°ë³¸ í™˜ê²½ êµ¬ì¶• ê¸°ì¡´ í™˜ê²½ì—ì„œ ì“°ì‹¤ ë¶„ë“¤ì€ ì“°ì…”ë„ ë©ë‹ˆë‹¤.\nì €ëŠ” ë”°ë¡œ í™˜ê²½ì„ ë§Œë“¤ì–´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ í–ˆìŠµë‹ˆë‹¤.\nconda create -n neptune python=3.7 conda activate neptune conda install psutil matplotlib tensorflow-gpu # í˜¹ì€ tensorflow 2. Neptune ì„¤ì • Neptuneì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.\ncondaë¥¼ ì“°ì‹œëŠ” ë¶„ë“¤ê»˜ ë³´í†µ conda install ë¡œ ì„¤ì¹˜í•˜ë¼ê³  ë§ì”€ì„ ë“œë¦¬ì§€ë§Œ.. ì´ë²ˆì—” íŠ¹ë³„íˆ pip installë¡œ ì„¤ì¹˜ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\n  ì„¤ì¹˜ í›„ ìì‹ ì´ ì‚¬ìš©í•˜ëŠ” OSì— ë§ì¶°ì„œ API tokenì„ PCì— ê¸°ì…(?) í•´ì£¼ì„¸ìš”.\n  ì—¬ê¸°ê¹Œì§€ í•˜ë©´ ì„¸íŒ…ì€ ëë‚©ë‹ˆë‹¤. ê·¸ ì´í›„ì— Run script, Result in UI ë¶€ë¶„ì€ í•˜ê³  ì‹¶ìœ¼ì‹  ë¶„ë§Œ í•´ë³´ì„¸ìš”!\n3. Mnist ë¥¼ ì´ìš©í•œ ì‹¤í—˜ ë‹¤ìŒê³¼ ê°™ì€ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\nimport hashlib import os import tempfile import matplotlib.pyplot as plt import neptune import numpy as np import tensorflow as tf from tensorflow.keras import models, layers, optimizers, callbacks, datasets # For Efficiency gpus = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(\u0026#39;GPU\u0026#39;) print(len(gpus), \u0026#34;Physical GPUs,\u0026#34;, len(logical_gpus), \u0026#34;Logical GPUs\u0026#34;) except RuntimeError as e: print(e) # select project neptune.init(\u0026#39;jjerry-k/mnist\u0026#39;) # Define parameters PARAMS = {\u0026#39;batch_size\u0026#39;: 64, \u0026#39;n_epochs\u0026#39;: 100, \u0026#39;shuffle\u0026#39;: True, \u0026#39;learning_rate\u0026#39;: 0.001, \u0026#39;early_stopping\u0026#39;: 10, \u0026#39;optimizer\u0026#39;: \u0026#39;Adam\u0026#39;, } # Create experiment neptune.create_experiment(name=\u0026#39;classification_example\u0026#39;, tags=[\u0026#39;classification\u0026#39;, \u0026#39;MNIST\u0026#39;], params=PARAMS) # Dataset mnist = datasets.mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images = train_images / 255.0 test_images = test_images / 255.0 class_names = [str(i) for i in range(10)] neptune.set_property(\u0026#39;class_names\u0026#39;, class_names) for j, class_name in enumerate(class_names): plt.figure(figsize=(10, 10)) label_ = np.where(train_labels == j) for i in range(9): plt.subplot(3, 3, i + 1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[label_[0][i]], cmap=plt.cm.binary) plt.xlabel(class_names[j]) neptune.log_image(\u0026#39;example_images\u0026#39;, plt.gcf()) def lr_scheduler(epoch): if epoch \u0026lt; 10: new_lr = PARAMS[\u0026#39;learning_rate\u0026#39;] else: new_lr = PARAMS[\u0026#39;learning_rate\u0026#39;] * np.exp(0.1 * ((epoch//50)*50 - epoch)) neptune.log_metric(\u0026#39;learning_rate\u0026#39;, new_lr) return new_lr # Model model = models.Sequential([ layers.Flatten(input_shape=(28, 28)), layers.Dense(128, activation=\u0026#39;relu\u0026#39;), layers.Dense(len(class_names), activation=\u0026#39;softmax\u0026#39;) ]) if PARAMS[\u0026#39;optimizer\u0026#39;] == \u0026#39;Adam\u0026#39;: optimizer = optimizers.Adam( learning_rate=PARAMS[\u0026#39;learning_rate\u0026#39;], ) elif PARAMS[\u0026#39;optimizer\u0026#39;] == \u0026#39;Nadam\u0026#39;: optimizer = optimizers.Nadam( learning_rate=PARAMS[\u0026#39;learning_rate\u0026#39;], ) elif PARAMS[\u0026#39;optimizer\u0026#39;] == \u0026#39;SGD\u0026#39;: optimizer = optimizers.SGD( learning_rate=PARAMS[\u0026#39;learning_rate\u0026#39;], ) model.compile(optimizer=optimizer, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) # Log model summary model.summary(print_fn=lambda x: neptune.log_text(\u0026#39;model_summary\u0026#39;, x)) def log_train_data(logs): neptune.log_metric(\u0026#39;epoch_acc\u0026#39;, logs[\u0026#39;accuracy\u0026#39;]) neptune.log_metric(\u0026#39;epoch_loss\u0026#39;, logs[\u0026#39;loss\u0026#39;]) def log_val_data(logs): # Evaluate model eval_metrics = model.evaluate(test_images, test_labels, verbose=0) for j, metric in enumerate(eval_metrics): neptune.log_metric(\u0026#39;eval_\u0026#39; + model.metrics_names[j], metric) # Training Network model.fit(train_images, train_labels, batch_size=PARAMS[\u0026#39;batch_size\u0026#39;], epochs=PARAMS[\u0026#39;n_epochs\u0026#39;], shuffle=PARAMS[\u0026#39;shuffle\u0026#39;], validation_data=[test_images, test_labels] callbacks=[callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_train_data(logs)), callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_valdata(logs)), callbacks.EarlyStopping(patience=PARAMS[\u0026#39;early_stopping\u0026#39;], monitor=\u0026#39;accuracy\u0026#39;, restore_best_weights=True), callbacks.LearningRateScheduler(lr_scheduler)] ) # Log model weights with tempfile.TemporaryDirectory(dir=\u0026#39;.\u0026#39;) as d: prefix = os.path.join(d, \u0026#39;model_weights\u0026#39;) model.save_weights(os.path.join(prefix, \u0026#39;model\u0026#39;)) for item in os.listdir(prefix): neptune.log_artifact(os.path.join(prefix, item), os.path.join(\u0026#39;model_weights\u0026#39;, item)) 4. ê²°ê³¼ í™•ì¸ neptuneì—ëŠ” ì´ 7ê°œì˜ íƒ­ì´ ìˆìŠµë‹ˆë‹¤.\nê°ê° ì˜ˆì‹œë¥¼ ë³´ì‹œë©´ ì—­í• ì„ ì´í•´í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nCharts ì—¬ê¸°ì—ëŠ” neptune.log_metric()ì„ ì‚¬ìš©í•œ ë³€ìˆ˜ë“¤ì´ ê·¸ë˜í”„ë¡œ ê¸°ë¡ë©ë‹ˆë‹¤.\n  Log Chartsì— ê¸°ë¡ëœ ë³€ìˆ˜ì˜ ê°’ì´ ê¸°ë¡ë˜ê³  ì¶”ê°€ì ìœ¼ë¡œ neptune.log_image(), neptune.log_text() ë¥¼ ì‚¬ìš©í•œ ë³€ìˆ˜ ë˜í•œ ê¸°ë¡ë©ë‹ˆë‹¤.\n  Monitoring ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ê³¼ standard error, standard outputì„ ë³¼ ìˆ˜ ìˆì–´ìš”.\n  Artifacts neptuneì— ì›í•˜ëŠ” íŒŒì¼ì„ ì „ì†¡í•˜ì—¬ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n  Source code ìƒëµ\n  Parameters ì½”ë“œ ìƒë‹¨ì— ì ì€ PARAMS ê°€ ê¸°ë¡ì´ ë©ë‹ˆë‹¤.\n  Detail ë­\u0026hellip;ì‹¤í—˜ì˜ ì •ë³´, ì‹œê°„ ë“± ê¸°ë¡ì´ ë˜ê³  ì¶”ê°€ì ìœ¼ë¡œ neptune.set_propertyë¥¼ ì´ìš©í•˜ì—¬ Properties ì— ë³€ìˆ˜ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆì–´ìš”!\n  wandbì™€ ë¹„ìŠ·í•œ ì—­í• ì„ í•˜ëŠ” neptune.aië¼ëŠ” toolì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤.\nìŒ\u0026hellip;.ì†”ì§íˆ wandbê°€ ë” í¸í•œ ë“¯ í•˜ë„¤ìš”. (ì½”ë“œê°€ ì§§ì•„ì„œ)\në­ ì¢€ ë” ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©í•˜ë©´ ì´ê²Œ ë” ë‚˜ì„\u0026hellip;ìˆ˜ë„?\nì„ íƒì€ ê°œì¸ì˜ ëª«ì…ë‹ˆë‹¤.\nP.S.  ì ì \u0026hellip;í¬ìŠ¤íŒ… ì£¼ì œ ê³ ê°ˆ\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/neptune/","tags":["Tools"],"title":"Neptune...? ì€ ë˜ ë­ì—¬.."},{"categories":["DeepLearning"],"contents":"ì˜¤ëŠ˜ì€ netron ì´ë¼ëŠ” Toolë¥¼ ë“œë¦¬ë ¤ í•©ë‹ˆë‹¤.\n ë³¸ í¬ìŠ¤íŒ…ì€ https://github.com/lutzroeder/netron ë‚´ìš©ì„ ì´ìš©í•˜ì—¬ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\n ê°„ë‹¨íˆ ë§ì”€ë“œë¦¬ë©´ Neural network viewer ì…ë‹ˆë‹¤.\nêµ‰ì¥íˆ ë§ì€ Frameworkë“¤ì„ ì§€ì›í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n ONNXÂ (.onnx,Â .pb,Â .pbtxt) KerasÂ (.h5,Â .keras) Core MLÂ (.mlmodel) CaffeÂ (.caffemodel,Â .prototxt) Caffe2Â (predict_net.pb) DarknetÂ (.cfg) MXNetÂ (.model,Â -symbol.json) BarracudaÂ (.nn) ncnnÂ (.param) TengineÂ (.tmfile) TNNÂ (.tnnproto) UFFÂ (.uff) TensorFlow LiteÂ (.tflite)  ì´ ì™¸ì—ë„ ë¶ˆì•ˆì •í•˜ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì€ Framework ë„ ì§€ì›í•©ë‹ˆë‹¤.\n TorchScriptÂ (.pt,Â .pth) PyTorchÂ (.pt,Â .pth) TorchÂ (.t7) Arm NNÂ (.armnn) BigDLÂ (.bigdl,Â .model) ChainerÂ (.npz,Â .h5) CNTKÂ (.model,Â .cntk) Deeplearning4jÂ (.zip) MediaPipeÂ (.pbtxt) ML.NETÂ (.zip) MNNÂ (.mnn) PaddlePaddleÂ (.zip,Â __model__) OpenVINOÂ (.xml) scikit-learnÂ (.pkl) TensorFlow.jsÂ (model.json,Â .pb) TensorFlowÂ (.pb,Â .meta,Â .pbtxt,Â .ckpt,Â .index)  (ì†”ì§íˆ Torch, TensorFlow ì¢…ë¥˜ëŠ” ì™œ ë‚˜ëˆ ì„œ ì„¤ëª…í•˜ëŠ”ì§€ ì˜ ëª¨ë¥´ê² ìŒ\u0026hellip;..í•˜ë‚˜ë¡œ í•©ì³ ë†“ìœ¼ë©´ ì•ˆë˜ë‚˜..)\nì„¤ì¹˜ ë° ì‚¬ìš©ë²• netronì€ PCì— ì„¤ì¹˜í•´ì„œ ì‚¬ìš©í•˜ê±°ë‚˜ ì„¤ì¹˜ì—†ì´ Browser ë²„ì „ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n macOS:Â DownloadÂ theÂ .dmgÂ file or runÂ brew cask install netron Linux:Â DownloadÂ theÂ .AppImageÂ file or runÂ snap install netron Windows:Â DownloadÂ theÂ .exeÂ installer or runÂ winget install netron Python Server: RunÂ pip install netronÂ andÂ netron [FILE]Â orÂ import netron; netron.start('[FILE]') Browser:Â StartÂ the browser version.  í•œë²ˆ ë¸Œë¼ìš°ì € ë²„ì „ì„ ì‹¤í–‰ì‹œì¼œ ë³´ì•˜ìŠµë‹ˆë‹¤.\n  Open Model ì„ í´ë¦­ í•˜ì‹  í›„ ë³´ê³  ì‹¶ì€ Network ì €ì¥ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤.\n  Network êµ¬ì¡°ê°€ ì°¨ë¡€ë¡œ ë³´ì´ë„¤ìš”.\n  ë‹¤ìŒì€ kerasë¡œ ì‘ì„±ëœ MobileNetV2 ë¥¼ netronìœ¼ë¡œ ë„ìš´ í›„ ì• ë¶€ë¶„ì„ ì˜ë¼ë‚¸ ì‚¬ì§„ì…ë‹ˆë‹¤.\n  ì™¼ìª½ì—ëŠ” ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ë³¼ ìˆ˜ ìˆê³  ë§Œì•½ ë ˆì´ì–´ë¥¼ ì„ íƒí•˜ë©´ ì˜¤ë¥¸ìª½ì— Node(Layer)ì— ëŒ€í•œ ì„¸ë¶€ ì„¤ì • ê°’ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nëª‡ëª‡ í”„ë ˆì„ì›Œí¬ ë³„ë¡œ sampleì„ ì´ë ‡ê²Œ ì œê³µí•´ì¤ë‹ˆë‹¤!\nopenì„ ëˆ„ë¥´ë©´ browserë¥¼ ì´ìš©í•˜ì—¬ ë³¼ ìˆ˜ ìˆë„¤ìš”!\n ONNX:Â squeezenetÂ [open] CoreML:Â exermoteÂ [open] Darknet:Â yoloÂ [open] Keras:Â mobilenetÂ [open] MXNet:Â inception_v3Â [open] TensorFlow:Â chessbotÂ [open] TensorFlow Lite:Â hair_segmentationÂ [open] TorchScript:Â traced_online_pred_layerÂ [open] Caffe:Â mobilenet_v2Â [open]  P.S.  í”¼ê³¤\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/netron/","tags":["Tools"],"title":"Netron! Network êµ¬ì¡°ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”!"},{"categories":["DeepLearning"],"contents":"ì§€ê¸ˆê¹Œì§€ Docker(ì´í•˜ ë„ì»¤) ì„¸íŒ… ê¹Œì§€ í¬ìŠ¤íŒ…ì„ í–ˆì—ˆìŠµë‹ˆë‹¤.\nDocker ê¹Œì§„ ì¢‹ì€ë°\u0026hellip;ì—°ê²°ì„ í•­ìƒ Terminal í˜¹ì€ CMDë¥¼ ì¼œì„œ sshë¡œ í•´ì•¼í•˜ë‚˜..? ìƒê°ì´ ë“­ë‹ˆë‹¤.\nê·¸ë˜ì„œ ì´ë˜ì €ë˜ ì°¾ì•„ë´¤ìŠµë‹ˆë‹¤.\nDocker ê³µì‹ ë¬¸ì„œë¥¼ ë³´ë‹ˆ Dockerize an SSH service ì´ëŸ° ê¸€ì´ ìˆë”êµ°ìš”.\nì´ ê¸€ì„ ì°¸ê³ í•˜ì—¬ image(ì´í•˜ ì´ë¯¸ì§€)ë¥¼ ë§Œë“¤ì–´ ë³´ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ ë¹ ë¥´ê²Œ ë¹ ë¥´ê²Œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n1. Dockerfile ìƒì„± ë° build FROMnvidia/cuda:10.1-cudnn7-runtime-ubuntu16.04LABEL maintainer \u0026#34;Jerry Kim \u0026lt;jaeyeol2931@gmail.com\u0026gt;\u0026#34;ARG PYTHON_VERSION=3.7RUN apt-get updateRUN apt-get install -y \\  build-essential \\  cmake \\  git \\  curl \\  wget \\  vim \\  unzip \\  ca-certificates \\  libjpeg-dev \\  libpng-dev \\  openssh-serverRUN apt-get update \u0026amp;\u0026amp; apt-get -y upgrade#RUN mkdir /var/run/sshdRUN echo \u0026#39;root:{ê°œì¸ ë¹„ë°€ë²ˆí˜¸}\u0026#39; | chpasswdRUN sed -i \u0026#39;s/PermitRootLogin prohibit-password/PermitRootLogin yes/\u0026#39; /etc/ssh/sshd_config# SSH login fix. Otherwise user is kicked off after loginRUN sed \u0026#39;s@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g\u0026#39; -i /etc/pam.d/sshdENV NOTVISIBLE \u0026#34;in users profile\u0026#34;RUN echo \u0026#34;export VISIBLE=now\u0026#34; \u0026gt;\u0026gt; /etc/profileEXPOSE22CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;]RUN rm -rf /var/lib/apt/lists/*RUN curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o ~/miniconda.shRUN chmod +x ~/miniconda.sh \u0026amp;\u0026amp; \\  ~/miniconda.sh -b -p /opt/conda \u0026amp;\u0026amp; \\  rm ~/miniconda.shRUN /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include ninja cython typing opencv matplotlib tqdm \u0026amp;\u0026amp; \\  /opt/conda/bin/conda install -y jupyter jupyterlab seaborn pillow pandas pylint scikit-learn scikit-image tensorflow-gpu \u0026amp;\u0026amp; \\  /opt/conda/bin/conda update -y --all \u0026amp;\u0026amp; \\  /opt/conda/bin/conda clean -yaì¤‘ê°„ì— RUN echo 'root:{ê°œì¸ ë¹„ë°€ë²ˆí˜¸}' | chpasswd ì—ì„œ ssh ì ‘ì†ì‹œ ì‚¬ìš©í•  ë¹„ë°€ë²ˆí˜¸ë¥¼ ì ì–´ì£¼ì„¸ìš”!\n# Example RUN echo \u0026#39;root:test\u0026#39; | chpasswd ìœ„ ë‚´ìš©ë“¤ì„ vimì´ë‚˜ nanoê°™ì€ ì—ë””í„°ë¥¼ ì´ìš©í•´ì„œ Dockerfileì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”!\n  ê·¸ë¦¬ê³  ë‹¤ìŒê³¼ ê°™ì´ ì»¤ë§¨ë“œë¥¼ ì…ë ¥í•˜ì—¬ ë„ì»¤ ì´ë¯¸ì§€ë¥¼ build í•©ë‹ˆë‹¤.\ndocker build -t {ì´ë¯¸ì§€ ì´ë¦„} .   ë­ ì´ëŸ° ì €ëŸ° Log ë“¤ì´ ì´¤ë¼â€”â€”â€”ë¼ë½ ë„˜ì–´ê°ˆê²ë‹ˆë‹¤. ê³„ì† ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”\u0026hellip;.\ndocker images ë¥¼ ì…ë ¥í•˜ë©´ ì œëŒ€ë¡œ ìƒì„±ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n  2. Container ì‹¤í–‰ ì´ë¯¸ì§€ë¥¼ ì‹¤í–‰í•˜ì—¬ container(ì´í•˜ ì»¨í…Œì´ë„ˆ) ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\nì„¸ë¶€ ì˜µì…˜ì€ êµ¬.ê¸€.ë§ ì•„ì‹œì£ ?\ndocker run -d -P -v /home/ubuntu/jerry/:/jerry --name {ì»¨í…Œì´ë„ˆ ì´ë¦„} {ì´ë¯¸ì§€ ì´ë¦„} docker port {ì»¨í…Œì´ë„ˆ ì´ë¦„} 22   ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“¤ê³  í•´ë‹¹ ì»¨í…Œì´ë„ˆì˜ 22ë²ˆ í¬íŠ¸ (ssh í¬íŠ¸)ê°€ nipaì˜ ëª‡ë²ˆ í¬íŠ¸ì™€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ì¶œë ¥í•´ì¤ë‹ˆë‹¤.\n3. VSCodeë¥¼ ì´ìš©í•˜ì—¬ ì ‘ì†  vscode ì— Remote - SSH í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ê°€ ë˜ì–´ ìˆë‹¤ëŠ” ì „ì œ í•˜ì— ì§„í–‰í•©ë‹ˆë‹¤.\n ìƒˆë¡œìš´ í˜¸ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•´ì¤ë‹ˆë‹¤.\n  ssh root@{NIPA IP} -p {í¬íŠ¸ ë²ˆí˜¸} ë¼ê³  ì…ë ¥í•˜ê³  ì—”í„°!\n  ê·¸ëŸ¼ ìš°í•˜ë‹¨ì— Hostë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤! ì™€ ê°™ì´ ì•Œë¦¼ì´ ëœ¹ë‹ˆë‹¤.\nì¶”ê°€ë¥¼ í–ˆìœ¼ë‹ˆ ì´ì œ Hostì— ì—°ê²°ì„ í•´ë´…ë‹ˆë‹¤.\në¹„ë°€ë²ˆí˜¸ëŠ” ë§¨ ì²˜ìŒì— Dockerfileì—ì„œ ì¼ë˜ ë¹„ë°€ë²ˆí˜¸ ì…ë‹ˆë‹¤!\n  ì•„ë§ˆ ì²˜ìŒì—ëŠ” ì„¸íŒ…ì´ ì˜¤ë˜ ê±¸ë¦´ê²ë‹ˆë‹¤. ìš°í•˜ë‹¨ì— ì•Œë¦¼ì´ ì—†ì–´ì§ˆ ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” !\nê¸°ì´ˆ ì„¸íŒ…ì´ ë‹¤ ì™„ë£Œ ë˜ë©´ ì»¨í…Œì´ë„ˆì˜ vscode-serverì— í”ŒëŸ¬ê·¸ì¸ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\nì €ëŠ” ì•„ë˜ 4ê°œ í”ŒëŸ¬ê·¸ì¸ì„ ì„¤ì¹˜í–ˆì–´ìš”.\n  ê·¸ í›„ì— í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ëŒë ¤ë´…ë‹ˆë‹¤.\n  ì •ìƒì ìœ¼ë¡œ ì˜ ì‘ë™í•˜ë„¤ìš”!\ní›„\u0026hellip;ì´ì œ Vimìœ¼ë¡œ ì½”ë”©ì„ ì•ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ë²ˆ í¬ìŠ¤íŒ…ì€ ì •ë§ í™˜ê²½ êµ¬ì¶•ì´ ì²˜ìŒì´ì‹  ë¶„ë“¤ê»˜ëŠ” ë¶ˆì¹œì ˆí•œ í¬ìŠ¤íŒ…ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\nì£„ì†¡í•©ë‹ˆë‹¤. ì¼ë‹¨ ì œ ì¼ê¸°ì²˜ëŸ¼ ì“°ëŠ” í¬ìŠ¤íŒ…ì´ë¼ì„œìš”. ê°ì•ˆí•´ì„œ ì½ì–´ì£¼ì„¸ìš”.\nP.S.  ê·¼ë° ë§‰ìƒ ë‚´ê°€ NIPAë¥¼ \u0026hellip;..ì£¼ë¡œ ì“°ì§€ ì•ŠìŒ\u0026hellip; ë‚˜ì—ê²Œ NIPAëŠ” í™˜ê²½ êµ¬ì¶• í¬ìŠ¤íŒ…ì„ ì ëŠ” ìš©ë„\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/nipa_vscode/","tags":["Tools"],"title":"NIPA x VScode !"},{"categories":["DeepLearning"],"contents":" ë³¸ í¬ìŠ¤íŒ…ì€ Colab on steroids: free GPU instances with SSH access and Visual Studio Code Server ë‚´ìš©ì„ (ë§ì´) ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\n ë§ì€ ë¶„ë“¤ì´ Google Colaboratory (ì½”ë©) ë¥¼ ì‚¬ìš©í•˜ì‹¤ê²ë‹ˆë‹¤. (ë¬´ë£Œë‹ˆê¹Œ\u0026hellip;)\në¬´ë£Œë¡œ ê³ ì„±ëŠ¥(?)ì˜ í•˜ë“œì›¨ì–´ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤ëŠ” ê±´ ì°¸ ì¢‹ì€ ê²ƒì…ë‹ˆë‹¤!\ní•˜ì§€ë§Œ\u0026hellip;ë¬¸ì œëŠ” Jupyter Notebook í˜•ì‹ìœ¼ë¡œ ì¨ì•¼í•œë‹¤ëŠ”ê±°\u0026hellip;ì•„ ë¬¼ë¡  í¸í•˜ì‹  ë¶„ë“¤ë„ ìˆê² ì£ !?\nì „ ê°œì¸ì ìœ¼ë¡œ ë³„ë¡œ ì•ˆì¢‹ì•„í•©ë‹ˆë‹¤. (ë¬¼ë¡  ì• ì´ˆì— ì•ˆì“°ê¸°ë„ í•¨.)\nê·¼ë° SSH, VSCode Serverë¥¼ ì´ìš©í•´ì„œ Colabì— ì ‘ì†ì„\u0026hellip;í•˜ëŠ” í¬ìŠ¤íŒ…ì´ ìˆë”êµ°ìš”.\nê°‘ìê¸° ì‹¤í—˜ì¥ ì •ì‹ ì´ íŠ€ì–´ë‚˜ì™€ì„œ ì§„í–‰ì„ í•´ë´¤ìŠµë‹ˆë‹¤.\në»˜ì†Œë¦¬ ê·¸ë§Œí•˜ê³  ê°„ë‹¨ ê°„ë‹¨ ì„¤ëª… ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.\n1. ngrokìœ¼ë¡œ token ë§Œë“¤ê¸° ngrokì€ ëŒ€ì¶© ë¡œì»¬ ì›¹ ì„œë²„ë¥¼ SSH ì ‘ì†ì´ë‚˜ ëª¨ë°”ì¼ í…ŒìŠ¤íŠ¸ í•  ìˆ˜ ìˆë„ë¡ ê³µê³µ URLë¡œ ì ‘ê·¼ ê°€ëŠ¥í† ë¡ í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°€ê²©ì€ ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”. 1ê°œëŠ” ë¬´ë£Œê±°ë“ ìš”.\n  ì–´ì¨Œë“  ê°€ì…ì„ í•˜ê³  tokenì„ ìƒì„± í•´ì¤ë‹ˆë‹¤.\n    2. Colabì— ngrok ì„¤ì¹˜ ë° ì‹¤í–‰ ë…¸íŠ¸ë¶ ì„¤ì •ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”. (ex) CPU, GPU, TPU\n  ê·¸ë¦¬ê³  ì…€ì— ë‹¤ìŒê³¼ ê°™ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\nì•„ë˜ authtoken ì—ëŠ” ngrokì˜ Authtokenì„ ì ì–´ì£¼ì„¸ìš”. (ë‹¹ì—°í•˜ì§€ë§Œ ìŠ¤íŠ¸ë§ìœ¼ë¡œ)\n# Install useful stuff ! apt install --yes ssh screen nano htop ranger git \u0026gt; /dev/null # SSH setting ! echo \u0026#34;root:carbonara\u0026#34; | chpasswd ! echo \u0026#34;PasswordAuthentication yes\u0026#34; \u0026gt; /etc/ssh/sshd_config ! echo \u0026#34;PermitUserEnvironment yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config ! echo \u0026#34;PermitRootLogin yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/sshd_config ! service ssh restart \u0026gt; /dev/null # Download ngrok ! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ! unzip -qq -n ngrok-stable-linux-amd64.zip # Run ngrok authtoken = **\u0026#34;PUT_YOUR_TOKEN_HERE\u0026#34;** get_ipython().system_raw(\u0026#39;./ngrok authtoken $authtoken \u0026amp;\u0026amp; ./ngrok tcp 22 \u0026amp;\u0026#39;) ! sleep 3 # Get the address for SSH import requests from re import sub r = requests.get(\u0026#39;http://localhost:4040/api/tunnels\u0026#39;) str_ssh = r.json()[\u0026#39;tunnels\u0026#39;][0][\u0026#39;public_url\u0026#39;] str_ssh = sub(\u0026#34;tcp://\u0026#34;, \u0026#34;\u0026#34;, str_ssh) str_ssh = sub(\u0026#34;:\u0026#34;, \u0026#34; -p \u0026#34;, str_ssh) str_ssh = \u0026#34;ssh root@\u0026#34; + str_ssh print(str_ssh)   ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ ì»¤ë§¨ë“œë¥¼ ì´ìš©í•˜ì—¬ í•œë²ˆ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë´…ë‹ˆë‹¤. ë¹„ë°€ë²ˆí˜¸ëŠ” carbonara ì…ë‹ˆë‹¤.\n  3. VSCodeì—ì„œ ì‹¤í–‰í•˜ê¸° ë¨¼ì € Colab í™”ë©´ì—ì„œ ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•´ì„œ Google Driveë¥¼ ë§ˆìš´íŠ¸ í•´ì¤ë‹ˆë‹¤.\n# Mount Google Drive and make some folders for vscode from google.colab import drive drive.mount(\u0026#39;/googledrive\u0026#39;) ! mkdir -p /googledrive/My\\ Drive/colabdrive ! mkdir -p /googledrive/My\\ Drive/colabdrive/root/.local/share/code-server ! ln -s /googledrive/My\\ Drive/colabdrive / ! ln -s /googledrive/My\\ Drive/colabdrive/root/.local/share/code-server /root/.local/share/   VSCodeë¥¼ ì¼œê³  sshë¡œ ì ‘ì†ì„ í•´ë´…ë‹ˆë‹¤.\n  ì ‘ì†í•´ì„œ Colab Notebooks ë””ë ‰í† ë¦¬ì— ì ‘ê·¼í•œ í™”ë©´ì…ë‹ˆë‹¤. Google Driveë‘ ë™ì¼í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì£ !\n  4. ì½”ë“œ ì‘ì„± ë° ì‹¤í–‰ ì½”ë“œ ì‹¤í–‰ ì—¬ë¶€ë¥¼ í…ŒìŠ¤íŠ¸ í•´ë³´ê² ìŠµë‹ˆë‹¤.\nì¼ë‹¨ Colab ì—ëŠ” VSCode í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— pythonì´ë‚˜ ê°œì¸ì ìœ¼ë¡œ í•„ìš”í•œ í”ŒëŸ¬ê·¸ì¸ë“¤ì„ ì„¤ì¹˜í•´ì¤ë‹ˆë‹¤.\nì €ëŠ” í…ŒìŠ¤íŠ¸ì´ë‹ˆ ì•„ë˜ ì„¸ ê°€ì§€ë§Œ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤.\n  ê·¸ëŸ¼ í•œë²ˆ ì‹¤í–‰í•´ë³´ì£ .\n  ì˜¤\u0026hellip;..ì‹ ê¸°\u0026hellip;\nì´ë ‡ê²Œ ì—°ê²°í•´ì„œ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\nhtopë¥¼ ì´ìš©í•´ì„œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ë„ ê°€ëŠ¥í•˜ì£ .\n  ì—¬ê¸°ê¹Œì§€ Colab x VSCode í¬ìŠ¤íŒ…ì…ë‹ˆë‹¤. ë­\u0026hellip;.ì–´ë–¤ ë©´ì—ì„  í¸í•  ê²ƒ ê°™ì§€ë§Œ ë§‰\u0026hellip;í¸í•  ê²ƒ ê°™ì§„ ì•Šë„¤ìš”.\nì•„ì§ ì´ ìƒí™©ì„ ê²ªì§€ ì•Šì•„ì„œ ëª¨ë¥´ê² ì§€ë§Œ ì›ë˜ ì½”ë©ì˜ í° ë¬¸ì œê°€ ìˆì£ . Session timeout\u0026hellip;.\nì¼ì • ì‹œê°„ë™ì•ˆ ë™ì‘ì´ ì—†ìœ¼ë©´ ì—°ê²°ì´ ëŠê²¨ì„œ ë‹¤ì‹œ ì‹¤í–‰ì„ ì‹œì¼œì•¼í•˜ëŠ”\u0026hellip;.ìƒí™©ì´ ì˜¤ì£ .\n  ë‚˜í•œí…Œ ì™œ ê·¸ë˜\u0026hellip;\nê·¸ ìƒí™©ì´ ì™”ì„ ë•Œ ëŠë‚Œìœ¼ë¡œëŠ” VSCodeì˜ SSH ì ‘ì†ì´ ëŠê¸¸ ê²ƒ ê°™ë„¤ìš”. í \u0026hellip;. ì‹¤í—˜ í›„ì— ì¶”ê°€ë¡œ ì ì–´ ë³´ê² ìŠµë‹ˆë‹¤.\nP.S ëŠê²¼ìŠµë‹ˆë‹¤.\n  VSCode ë¥¼ ë´ë„..\n  ê·¸ë ‡ìŠµë‹ˆë‹¤. ì œ ìƒê°ëŒ€ë¡œ Colabì˜ Session timeì´ ëë‚˜ë©´\u0026hellip;ëŠê¸°ë„¤ìš”..ã…ã… í¸í•˜ê²Œ ì“°ê¸°ëŠ” í˜ë“¤ ë“¯\u0026hellip;\n","permalink":"https://jjerry-test.github.io/blog/colab_vscode/","tags":["Tools"],"title":"Colab x VSCode !"},{"categories":["DeepLearning"],"contents":" ì›ë˜ NIPA GPU ì„œë²„ë¥¼ ëŒ€ì—¬ë°›ì€ í›„ì— í¬íŠ¸ í¬ì›Œë”©ì„ ë¨¼ì € í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ ë¶€ë¶„ì— ëŒ€í•´ì„  ë³´ì•ˆì ì¸ ë¶€ë¶„ì´ ìˆê¸° ë•Œë¬¸ì— ìƒëµí•˜ê² ìŠµë‹ˆë‹¤.\n ì´ë²ˆì—” NIPA ë‚´ ê°œì¸ í™˜ê²½ ì„¸íŒ…ì— ëŒ€í•´ í¬ìŠ¤íŒ…ì„ í•´ë³´ë ¤ í•©ë‹ˆë‹¤.\nê°œì¸ë§ˆë‹¤ ì›í•˜ëŠ” í™˜ê²½ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì •ë§ í•„ìš”í•˜ì£ .\në¬¼ë¡  ê¸°ë³¸ì ìœ¼ë¡œ ì„¤ì¹˜ëœ Anaconda í™˜ê²½ìœ¼ë¡œë„ ì¶©ë¶„í•  ìˆ˜ ìˆì§€ë§Œ ì‚´ì§ì¿µ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\në¬¸ì œì— ëŒ€í•´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\nNIPA GPU ì„œë²„ì— ì ‘ì†ì„ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í™”ë©´ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n  ë¨¼ì € ë§ì”€ë“œë ¸ë˜ conda í™˜ê²½ìœ¼ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ í™˜ê²½ì´ ì œê³µë˜ë„¤ìš”.\nì €ì˜ ê²½ìš° ì´ë²ˆì— tf-nightlyê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤.\nê·¸ë˜ì„œ TensorFlow2, python3.6 í™˜ê²½ì„ activate í•œ í›„ ì„¤ì¹˜ë¥¼ ì‹œë„í–ˆì£ .\n     what\u0026hellip;?         ìŒ\u0026hellip;.conda ë²„ì „ì˜ ë¬¸ì œì¸ê°€ ì‹¶ì–´ì„œ base condaë¥¼ update í•˜ë ¤ í–ˆìŠµë‹ˆë‹¤.\n     what\u0026hellip;?         ë­ì•¼..ì´ê±´ ë˜ ì™œ ì•ˆë˜ëŠ”ê±°ì•¼\u0026hellip; ì§œì¦ì´ ë‚¬ìŠµë‹ˆë‹¤.\nëŒ€ì¶© ë„ˆë¬´ ì˜›ë‚  ë²„ì „ì˜ condaë‹ˆê¹Œ ìµœì†Œ 4.8ë¡œ ì¬ì„¤ì¹˜ í•´ì£¼ì„¸ìš”. ë¼ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.\ní•˜....ì´ê±´ ì¢€ ë„ˆë¬´í•œë°...ê·¸ëƒ¥ Dockerë‚˜ ì„¤ì¹˜í•˜ì.. ë¼ëŠ” ìƒê°ì„ í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ Docker ì„¤ì¹˜ì— ëŒ€í•´ í¬ìŠ¤íŒ… í•´ë³´ê² ìŠµë‹ˆë‹¤.\nDocker ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ í•˜ì§€ ì•Šì„ ê²ë‹ˆë‹¤.\ní™ˆí˜ì´ì§€ í˜¹ì€ Docker ì— ëŒ€í•œ í¬ìŠ¤íŒ…ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\nê°„ë‹¨íˆ ë§ì”€ë“œë¦¬ë©´ OS ë‹¨ê³„ê¹Œì§€ ê°€ìƒí™˜ê²½ì„ ë§Œë“œëŠ” ê²ë‹ˆë‹¤.\nê·¸ëŸ¼ ì„¤ì¹˜ ë°©ë²•ì— ëŒ€í•´ ì ê² ìŠµë‹ˆë‹¤.\n# 2020.10.10 ì¶”ê°€ ì‚¬í•­ conda uninstall curl # SET UP THE REPOSITORY sudo apt-get update sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; # INSTALL DOCKER ENGINE sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io # VERIFY THAT DOCKER ENGINE IS INSTALLED CORRECTLY sudo docker run hello-world ë§Œì•½ì— ì œëŒ€ë¡œ ì„¤ì¹˜ ë˜ì—ˆë‹¤ë©´ ë§ˆì§€ë§‰ì— ë‹¤ìŒê³¼ ê°™ì€ ì¶œë ¥ì´ ë‚¨ìŠµë‹ˆë‹¤\n  ì—¬ê¸°ê¹Œì§€ í•˜ì‹œë©´ ê¸°ë³¸ Docker ì„¤ì¹˜ëŠ” ëë‚¬ìŠµë‹ˆë‹¤.\ní•˜ì§€ë§Œ ì´ê²ƒë§Œ ì„¤ì¹˜í•˜ë©´ GPU ëŠ” ì‚¬ìš©í•˜ì§€ ëª»í•©ë‹ˆë‹¤.\nGPUë¥¼ ì“°ê¸° ìœ„í•´ì„  nvidia-dockerë¥¼ ì„¤ì¹˜ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤.\nnvidia-dockerëŠ” ê°„ë‹¨íˆ ë§í•˜ë©´ docker ì—ì„œ ë°ìŠ¤í¬íƒ‘ì˜ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ nvidiaì—ì„œ ë§Œë“ (?)ê²ƒì…ë‹ˆë‹¤.\nì„¤ì¹˜ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n# Ubuntu 16.04/18.04/20.04, Debian Jessie/Stretch/Buster # Add the package repositories distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker # Test nvidia-docker sudo docker run --gpus all nvidia/cuda:10.0-base nvidia-smi ì´ ë˜í•œ ì„¤ì¹˜ê°€ ì œëŒ€ë¡œ ë˜ì—ˆë‹¤ë©´ ë§ˆì§€ë§‰ì— ë‹¤ìŒê³¼ ê°™ì´ nvidia-smi ì¶œë ¥ì´ ë‚˜ì˜¬ ê²ë‹ˆë‹¤.\n     í•˜\u0026hellip;.í¸ì•ˆ\u0026hellip;.         ì´ë²ˆì—” NIPAì— Docker ì„¤ì¹˜í•˜ëŠ” ê³¼ì •ì„ í¬ìŠ¤íŒ… í•´ë´¤ìŠµë‹ˆë‹¤.\nê³µì§œë¡œ ë¹Œë ¤ì£¼ëŠ” ê±´ ì¢‹ìœ¼ë‚˜ í™˜ê²½ êµ¬ì¶•ì€ ì—­ì‹œë‚˜\u0026hellip;.í•´ì•¼ í•˜ë„¤ìš”.\nì œê°€ ì“°ëŠ” Docker imageëŠ” ê°œì¸ì ì¸ ë„ì»¤ íŒŒì¼ ì— ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì„¸ìš”!\nP.S  2020.10.10 ë‚´ìš© ì¶”ê°€  sudo usermod -aG docker {ìœ ì €ëª…} ì„ ìˆ˜í–‰í•˜ì‹œë©´ sudo ëª…ë ¹ì–´ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤!    ","permalink":"https://jjerry-test.github.io/blog/nipa_docker/","tags":["Tools"],"title":"NIPA x Docker !"},{"categories":["DeepLearning"],"contents":"ì €ë²ˆ í¬ìŠ¤íŒ…ì— ì´ì–´ ì´ë²ˆì—” NIPA ì»´í“¨íŒ… ìì› ì´ìš© ì‹ ì²­ì— ê´€í•œ í¬ìŠ¤íŒ…ì„ í•´ë³´ë ¤ í•©ë‹ˆë‹¤!\në‚´ìš© ì¶œì²˜: http://www.aihub.or.kr/node/254\nì´ìš© ì ˆì°¨   ì¼ë°˜ ì‚¬ìš©ì\n ì‹ ì²­ ëŒ€ìƒ: AI ì œí’ˆÂ·ì„œë¹„ìŠ¤ë¥¼ ì—°êµ¬Â·ê°œë°œí•˜ê³ ì í•˜ëŠ” êµ­ë‚´ ì¤‘ì†ŒÂ·ë²¤ì²˜ ê¸°ì—…, ìŠ¤íƒ€íŠ¸ì—…, ê³µê³µê¸°ê´€, ëŒ€í•™êµ(ì›), ì¼ë°˜ í˜‘Â·ë‹¨ì²´ ì‹ ì²­ ê¸°ê°„: 2020. 1. 6.(ì›”) âˆ¼ 2. 7.(ê¸ˆ)â€» ì¶”ê°€ ëª¨ì§‘ : 2020.5.25.(ì›”) ~ 11.30.(ì›”) ì‹ ì²­ ì ˆì°¨: ì´ìš© ì‹ ì²­ì„œ ì‘ì„± â†’ ì˜¨ë¼ì¸ ì‹ ì²­ ì‚¬ì´íŠ¸ì— ì œì¶œ â†’ ì„œë©´ ì‹¬ì‚¬ â†’ ì„ ì •ê²°ê³¼ ë°œí‘œ\nâ€» ì„ ì •ê¸°ì¤€ : ì§€ì› ìê²©ìš”ê±´, AI ì—°êµ¬â€§ê°œë°œëŒ€ìƒ ì—¬ë¶€, ì‚¬ìš©ê³„íšì„œ(ëª©ì , ì—°êµ¬â€§ê°œë°œë¶„ì•¼, ì‚¬ìš©ë‚´ìš©, í•„ìš”ì„± ë“±)ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì‹¬ì‚¬â€§í‰ê°€\nâ€» ëª¨ì§‘ ê·œëª¨ ëŒ€ë¹„ ì´ˆê³¼ ì‹ ì²­ ì‹œ ì¤‘ì†ŒÂ·ë²¤ì²˜ê¸°ì—…, ê³µê³µê¸°ê´€, ëŒ€í•™êµ(ì›)ì— ìì›ì´ ìš°ì„  í• ë‹¹ë  ìˆ˜ ìˆìŒ    ìˆ˜ì‹œ ì‚¬ìš©ì\n ì‹ ì²­ ëŒ€ìƒ: ì¼ë°˜ ì‚¬ìš©ìì™€ ê°œë°œì, í•™ìƒ ë“± ê°œì¸ ì‹ ì²­ ê¸°ê°„: 2020.4.10.(ê¸ˆ) ~ 2020.12.21.(ì›”) ì‹ ì²­ ì ˆì°¨: ì´ìš©ì‹ ì²­ ì‘ì„±â€§ì œì¶œ â†’ AI ì—°êµ¬â€§ê°œë°œëŒ€ìƒ ì—¬ë¶€ í™•ì¸ â†’ ì—¬ìœ  ìì› í™•ì¸ â†’ ì„œë¹„ìŠ¤ ì´ìš©    ê°ê°ì˜ ì‹ ì²­ì„œ ì–‘ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\nì¼ë°˜ ì‚¬ìš©ììš© ì‹ ì²­ì„œ  ë‹¤ìš´ë¡œë“œ ë§í¬    ìˆ˜ì‹œ ì‚¬ìš©ììš© ì‹ ì²­ì„œ  ë‹¤ìš´ë¡œë“œ ë§í¬    ë‹¹ì—°í•œê±°ì§€ë§Œ ì¼ë°˜ ì‚¬ìš©ìê°€ ìˆ˜ì‹œ ì‚¬ìš©ìì— ë¹„í•´ ì‘ì„±í•´ì•¼í•  ê²ƒì´ ë§ìŠµë‹ˆë‹¤.\nì‹ ì²­ì„œ ì‘ì„± ë°©ë²•ì€ ì˜ˆì‹œê°€ ë„ˆë¬´ ì˜ ì í˜€ìˆê¸° ë•Œë¬¸ì— ë”°ë¡œ ì„¤ëª…ì„ ë“œë¦¬ì§€ ì•Šê² ìŠµë‹ˆë‹¤. (ê¶ê¸ˆí•˜ì‹ ê²Œ ìˆë‹¤ë©´ ë”°ë¡œ ëŒ“ê¸€ ë‚¨ê²¨ì£¼ì„¸ìš”!)\nì‹ ì²­ì„œë¥¼ ì‘ì„±í•˜ì‹  í›„ì—” AIHub ì‚¬ì´íŠ¸ ë¡œê·¸ì¸ì„ í•˜ì‹  í›„ ì´ìš©ì‹ ì²­ ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\nê·¸ëŸ¼ ë‹¤ìŒê³¼ ê°™ì€ í™”ë©´ì´ ë‚˜ì˜¤ëŠ”ë° (ìˆ˜ì‹œ ì‚¬ìš©ì ê¸°ì¤€) 1~8ë²ˆ ì¤‘ í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ ì„ íƒí•˜ê³  ê°œì¸ ì •ë³´ ì±„ì›Œì£¼ì‹œë©´ ë©ë‹ˆë‹¤!\n  ë‹¤ ì±„ìš´ í›„ ì´ìš©ì‹ ì²­ì„ ëˆ„ë¥´ì‹œë©´ í•œ\u0026hellip;. í•˜ë£¨, ì´í‹€? ì •ë„ í›„ì— ë©”ì¼ê³¼ í•¸ë“œí°ìœ¼ë¡œ ë¬¸ì ì•ˆë‚´ê°€ ì˜µë‹ˆë‹¤!\në©”ì¼ì—ëŠ” NIPA ì„œë²„ ì‚¬ìš©ë°©ë²•, ë¬¸ìì—ëŠ” ê´€ë¦¬ í˜ì´ì§€ ì´ìš© ì•ˆë‚´ê°€ ì˜µë‹ˆë‹¤.\nì‹ ì²­ì´ ì–´ë µì§€ ì•Šìœ¼ë‹ˆ ë§ì´ ë§ì´ ì‹ ì²­í•˜ì‹œê³  ì¦ê±°ìš´ ë”¥ëŸ¬ë‹ ê³µë¶€, ì—°êµ¬í•˜ì„¸ìš”! (í™ë³´ ì•„ë‹˜)\n  P.S.  ì¤‘ìš”! ! ! ìˆ˜ì‹œ ì‚¬ìš©ìëŠ” ê³„ì† ì‚¬ìš©í•˜ë ¤ë©´ 10ì¼ë§ˆë‹¤ ì—°ì¥ ì‹ ì²­ í•´ì•¼í•¨. ê¼­! ë°˜ë“œì‹œ! ìœ„ ë‚´ìš©ì„ ì§€í‚¤ì§€ ì•Šìœ¼ë©´ ì„œë²„ ì´ˆê¸°í™”ë˜ì„œ ë‚´ìš©ë“¤ ë‹¤ ì‚¬ë¼ì§!!! ë„ˆë¬´ ëŒ€ì¶© í¬ìŠ¤íŒ…ì„\u0026hellip;í•˜ëŠ”ê±¸ê¹Œ\u0026hellip; 2020.10.10 ë‚´ìš© ì¶”ê°€  ì„œì‹ ë§í¬ ë³€ê²½    ","permalink":"https://jjerry-test.github.io/blog/nipa_apply/","tags":["Tools"],"title":"NIPA ì»´í“¨íŒ… ìì› ì‹ ì²­ ë°©ë²• !"},{"categories":["DeepLearning"],"contents":"ìš”ì¦˜ ì»¤ë®¤ë‹ˆí‹°(V-AIS) ì—ì„  NIPAì— ëŒ€í•œ ì–˜ê¸°ê°€ ë§ì´ ë‚˜ì˜µë‹ˆë‹¤.\nNIPAëŠ” ì •ë³´í†µì‹ ì‚°ì—…ì§„í¥ì› ì˜ ì•½ìì¸ë° ê·¼ë° ê·¸ë˜ì„œ ì´ê²Œ ì™œ..? ë¼ëŠ” ì˜ë¬¸ì„ ê°€ì§€ì‹¤ ê²ë‹ˆë‹¤.\nì§€ê¸ˆ AI Hubì—ì„œ ì§€ì›í•˜ëŠ” AI ì»´í“¨íŒ… ìì› ì§€ì› ì‚¬ì—…ì„ í•˜ê³  ìˆëŠ”ë°ìš”.\nê·¸ ì‚¬ì—…ì—ì„œ ì»´í“¨íŒ… ìì›ì„ ê´€ë¦¬í•˜ëŠ”ê²Œ NIPA ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ NIPA, NIPA í•˜ì£ ..\nê°„ë‹¨í•˜ê²Œ ê·¸ë¦¼ìœ¼ë¡œ ë³´ì—¬ë“œë¦¬ë©´ ì´ë ‡ìŠµë‹ˆë‹¤.\n  ë” ê°„ë‹¨í•˜ê²Œ ì„¤ëª…ë“œë¦¬ë©´ GPUì„œë²„ ì§€ì›í•´ë“œë¦¼ ã…‡ã…‡ ì´ê²ë‹ˆë‹¤.\nì–´ëŠ ì •ë„ ì‚¬ì–‘ì´ëƒ\u0026hellip;\n  ì¼ë°˜ ì‚¬ìš©ìì™€ ìˆ˜ì‹œ ì‚¬ìš©ìë¡œ ë‚˜ë‰˜ëŠ”ë°ìš”. ê·¸ ì°¨ì´ëŠ” ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸í•˜ì‹œê¸¸..\nì´ë ‡ê²Œ ë³´ë©´ GPUë¡œ ë­ ì“°ëŠ”ì§€ ì˜ ëª¨ë¥´ì‹œê² ì£ . ê·¸ë˜ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.\nì œê°€ í¬ìŠ¤íŒ…, í™˜ê²½êµ¬ì¶•ê³¼ ê°™ì€ ìë£Œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì‹ ì²­í•œ ì„œë²„ì…ë‹ˆë‹¤.\n  ìœ„ GPU ì‚¬ì§„ì€ ìˆ˜ì‹œ ì‚¬ìš©ì ê¸°ì¤€ì…ë‹ˆë‹¤. ì¼ë°˜ ì‚¬ìš©ìë©´ V100 ë‘ê°œê² ë„¤ìš”.\në­ ì´ëŸ° ì„œë²„ë¥¼ ê³µì§œë¡œ ì œê³µì„ í•´ì¤ë‹ˆë‹¤.\nì§‘ì— ì„œë²„ê°€ ë”°ë¡œ ì—†ëŠ” ë¶„ë“¤ì´ë¼ë©´ ì´ë³´ë‹¤ ì¢‹ì€ ê±´ ì—†ê² ì£ . (ê²Œë‹¤ê°€ VRAMì´ 32ê¸°ê°€ì„\u0026hellip;.)\n   í—¤í—¿ ê²ë‚˜ ì¢‹êµ°         ë‹¤ìŒì—” NIPA ì‹ ì²­ ë°©ë²•ì— ëŒ€í•œ í¬ìŠ¤íŒ…ì€ ê°„ë‹¨íˆ í•´ë³´ê² ìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ ëª¨ë‘ ì¦ê±°ìš´ ë”¥ëŸ¬ë‹ ê³µë¶€, ì—°êµ¬í•˜ì„¸ìš”.\nP.S.  ìˆ˜ì‹œ ì‚¬ìš©ìì˜ ê²½ìš° ê¸°ë³¸ 10ì¼ ì‚¬ìš©, 10ì¼ë§ˆë‹¤ ì‚¬ìš© ì—°ì¥ì‹ ì²­ì„ í•´ì•¼í•¨. ë‹¹ì—°í•˜ì§€ë§Œ ì‹ ì²­í•´ë†“ê³  ì‚¬ìš©ì•ˆí•˜ë©´ ê²€ì—´ í›„ ê°•ì œ ë°˜ë‚©.  ","permalink":"https://jjerry-test.github.io/blog/nipa_intro/","tags":["Tools"],"title":"NIPA...NIPAê°€ ë­ì£ .."},{"categories":["DeepLearning"],"contents":"ì´ë²ˆ í¬ìŠ¤íŒ…ì€ Multi GPU ì‹œìŠ¤í…œì—ì„œ Googleì˜ ë¨¸ì‹ ëŸ¬ë‹ ì˜¤í”ˆ ì†ŒìŠ¤ í”Œë«í¼ì¸ TensorFlowì‚¬ìš©ë²•ì— ê´€í•œ ê²ƒì…ë‹ˆë‹¤!\nê±°ë‘ì ˆë¯¸í•˜ê³  ë°”ë¡œ ì½”ë”©ìœ¼ë¡œ ë“¤ì–´ê°€ê² ìŠµë‹ˆë‹¤!\nSingle GPU ì˜ˆì‹œ  ë‹¤ìŒ ì½”ë“œëŠ” Single GPUë¥¼ ì´ìš©í•˜ì—¬ mnist dataë¥¼ ë¶„ë¥˜í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.  # Import Package import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models, losses, optimizers, datasets, utils # Data Prepare (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.expand_dims(train_x/255., -1), np.expand_dims(test_x/255., -1) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # Build Network cnn = models.Sequential() cnn.add(layers.Conv2D(16, 3, activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1,))) cnn.add(layers.MaxPool2D()) cnn.add(layers.Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;)) cnn.add(layers.MaxPool2D()) cnn.add(layers.Flatten()) cnn.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) cnn.compile(optimizer=optimizers.Adam(), loss=losses.sparse_categorical_crossentropy, metrics=[\u0026#39;accuracy\u0026#39;]) print(\u0026#34;Network Built!\u0026#34;) # Training Network epochs=10 batch_size = 4096 history = cnn.fit(train_x, train_y, epochs=10, batch_size=batch_size, validation_data=(test_x, test_y)) Multi GPU ì˜ˆì‹œ  ë‹¤ìŒ ì½”ë“œëŠ” Multi GPUë¥¼ ì´ìš©í•œ ì½”ë“œì…ë‹ˆë‹¤.  # Import Package import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models, losses, optimizers, datasets, utils # Data Prepare (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.expand_dims(train_x/255., -1), np.expand_dims(test_x/255., -1) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # Build Network strategy = tf.distribute.MirroredStrategy() with strategy.scope(): cnn = models.Sequential() cnn.add(layers.Conv2D(16, 3, activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1,))) cnn.add(layers.MaxPool2D()) cnn.add(layers.Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;)) cnn.add(layers.MaxPool2D()) cnn.add(layers.Flatten()) cnn.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) cnn.compile(optimizer=optimizers.Adam(), loss=losses.sparse_categorical_crossentropy, metrics=[\u0026#39;accuracy\u0026#39;]) print(\u0026#34;Network Built!\u0026#34;) # Training Network epochs=10 batch_size_each_gpu = 4096 batch_size = batch_size_each_gpu*len(gpus) history = cnn.fit(train_x, train_y, epochs=10, batch_size=batch_size, validation_data=(test_x, test_y)) ì–´ë µì§€ ì•ŠìŠµë‹ˆë‹¤. Build Network ì£¼ì„ ë¶€ë¶„ê³¼ Training Network ë¶€ë¶„ì— batch_sizeë§Œ ì¡°ê¸ˆ ìˆ˜ì •í•´ì£¼ì‹œë©´ ëë‚©ë‹ˆë‹¤!\n  í•˜ì§€ë§Œ ì´ë ‡ê²Œ í•˜ë©´ ë¬´ì‹í•˜ê²Œ GPUì˜ ëª¨ë“  ë©”ëª¨ë¦¬ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\nê·¸ë ‡ê¸° ë–„ë¬¸ì— ë‹¤ìŒê³¼ ê°™ì´ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì—¬ í•„ìš”í•œ ë§Œí¼ í• ë‹¹í•˜ë„ë¡ í•©ë‹ˆë‹¤.\ní•„ìš”í•œ ë§Œí¼ì˜ GPU ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©í•˜ê¸° # Import Package import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models, losses, optimizers, datasets, utils # Data Prepare (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.expand_dims(train_x/255., -1), np.expand_dims(test_x/255., -1) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # For Efficiency gpus = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) if gpus: try: for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(\u0026#39;GPU\u0026#39;) print(len(gpus), \u0026#34;Physical GPUs,\u0026#34;, len(logical_gpus), \u0026#34;Logical GPUs\u0026#34;) except RuntimeError as e: print(e) # Build Network strategy = tf.distribute.MirroredStrategy() with strategy.scope(): cnn = models.Sequential() cnn.add(layers.Conv2D(16, 3, activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1,))) cnn.add(layers.MaxPool2D()) cnn.add(layers.Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;)) cnn.add(layers.MaxPool2D()) cnn.add(layers.Flatten()) cnn.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) cnn.compile(optimizer=optimizers.Adam(), loss=losses.sparse_categorical_crossentropy, metrics=[\u0026#39;accuracy\u0026#39;]) print(\u0026#34;Network Built!\u0026#34;) # Training Network epochs=10 batch_size_each_gpu = 4096 batch_size = batch_size_each_gpu*len(gpus) history = cnn.fit(train_x, train_y, epochs=10, batch_size=batch_size, validation_data=(test_x, test_y)) ê¸°ì¡´ Multi GPU ì½”ë“œì™€ ë‹¬ë¼ì§„ ì ì€\n For Efficiencyë¼ëŠ” ë¶€ë¶„ì´ ì¶”ê°€. strategy = tf.distribute.MirroredStrategy()ì„ Build Networkì—ì„œ For Efficiencyì˜ ê°€ì¥ ì²«ë²ˆì§¸ ë¼ì¸ìœ¼ë¡œ ì´ë™.  ì´ë ‡ê²Œ ë³€ê²½ í›„ ì‹¤í–‰ í›„ nvidia-smiì™€ ê°™ì€ ëª¨ë‹ˆí„°ë§ íˆ´ì„ í™•ì¸í•´ë³´ì‹œë©´ ì´ì „ê³¼ëŠ” ë‹¤ë¥´ê²Œ GPU ë©”ëª¨ë¦¬ë¥¼ í•„ìš”í•œ ë§Œí¼ë§Œ ì‚¬ìš©í•˜ëŠ”ê±¸ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\nP.S  ë‹¤ìŒì€ ë­˜ë¡œ í¬ìŠ¤íŒ…í•˜ì§€\u0026hellip;  ë²ˆì™¸í¸ (Using gradient tape) # %% # Import Package import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models, losses, optimizers, datasets, utils gpus = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(\u0026#39;GPU\u0026#39;) print(len(gpus), \u0026#34;Physical GPUs,\u0026#34;, len(logical_gpus), \u0026#34;Logical GPUs\u0026#34;) except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) # %% # Data Prepare epochs=10 batch_size_each_gpu = 4096 batch_size = batch_size_each_gpu*len(gpus) (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.expand_dims(train_x/255., -1), np.expand_dims(test_x/255., -1) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # %% # Build Network class build_model(models.Model): def __init__(self): super(build_model, self).__init__() self.conv1 = layers.Conv2D(16, 3, activation=\u0026#39;relu\u0026#39;) self.pool1 = layers.MaxPool2D() self.conv2 = layers.Conv2D(32, 3, activation=\u0026#39;relu\u0026#39;) self.pool2 = layers.MaxPool2D() self.flatten = layers.Flatten() self.dense = layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) def call(self, x): x = self.conv1(x) x = self.pool1(x) x = self.conv2(x) x = self.pool2(x) x = self.flatten(x) return self.dense(x) print(\u0026#34;Network Built!\u0026#34;) # Set mirrored Strategy strategy = tf.distribute.MirroredStrategy() with strategy.scope(): # Prepare dataset  train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(len(train_x)).batch(batch_size) train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset) test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size) test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset) # Make Network cnn = build_model() # Set Loss \u0026amp; Metric function loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE) def compute_loss(labels, predictions): per_example_loss = loss_object(labels, predictions) return tf.nn.compute_average_loss(per_example_loss, global_batch_size=batch_size) test_loss = tf.keras.metrics.Mean(name=\u0026#39;test_loss\u0026#39;) train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u0026#39;train_accuracy\u0026#39;) test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u0026#39;test_accuracy\u0026#39;) # Set optimizer optimizer = tf.keras.optimizers.Adam() # Define taining, test function def train_step(inputs): images, labels = inputs with tf.GradientTape() as tape: predictions = cnn(images, training=True) loss = compute_loss(labels, predictions) gradients = tape.gradient(loss, cnn.trainable_variables) optimizer.apply_gradients(zip(gradients, cnn.trainable_variables)) train_accuracy.update_state(labels, predictions) return loss def test_step(inputs): images, labels = inputs predictions = cnn(images, training=False) t_loss = loss_object(labels, predictions) test_loss.update_state(t_loss) test_accuracy.update_state(labels, predictions) # Define training, test function suitable for Mirrored Strategy  @tf.function def distributed_train_step(dataset_inputs): per_replica_losses = strategy.experimental_run_v2(train_step, args=(dataset_inputs,)) return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) @tf.function def distributed_test_step(dataset_inputs): return strategy.experimental_run_v2(test_step, args=(dataset_inputs,)) # Train Network for epoch in range(epochs): # Training Loop total_loss = 0.0 num_batches = 0 for x in train_dist_dataset: total_loss += distributed_train_step(x) num_batches += 1 train_loss = total_loss / num_batches # Test Loop for x in test_dist_dataset: distributed_test_step(x) template = (\u0026#34;ì—í¬í¬ {}, ì†ì‹¤: {}, ì •í™•ë„: {}, í…ŒìŠ¤íŠ¸ ì†ì‹¤: {}, í…ŒìŠ¤íŠ¸ ì •í™•ë„: {}\u0026#34;) print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100)) test_loss.reset_states() train_accuracy.reset_states() test_accuracy.reset_states() ","permalink":"https://jjerry-test.github.io/blog/tf2_multi_gpu/","tags":["TensorFlow"],"title":"TensorFlow Multi GPU ì‚¬ìš©ë²•"},{"categories":["DeepLearning"],"contents":"End-to-End Object Detection with Transformers Author: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko Date: May 27, 2020 URL: https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers\nIntroduction  í˜„ì¬ Object detection modelë“¤ì€ Inputë¶€í„° Output (bounding bos, category label) ê¹Œì§€ Direct í•˜ì§€ ëª»í•¨. Post processing ì´ ì˜í–¥ì„ ë¼ì¹˜ê¸° ë•Œë¬¸ì—\u0026hellip; ë³¸ ë…¼ë¬¸ì—ì„  Direct prediction approach ì œì•ˆ. ì´ì „ì—ë„ ëª‡ëª‡ ì‹¤í—˜ì´ ìˆì—ˆìœ¼ë‚˜ ê·¸ ë‹¹ì‹œì—ëŠ” prior knowledgeë¥¼ ì¤€ë‹¤ê±°ë‚˜ ì„±ëŠ¥ì´ ë³„ë¡œ ì¢‹ì§€ ëª»í–ˆìŒ. Transformer ë¥¼ ì‚¬ìš©. ìƒˆë¡œìš´ Loss function ë„ì….    Related Works Set Prediction  í˜„ì¬ê¹Œì§€ Directë¡œ set(box, class)ì„ prediction í•˜ëŠ” ë°©ë²•ì´ ì—†ìŒ. Post processing ì´ ì—†ëŠ” model ì œì•ˆ. ì´ë¥¼ ìœ„í•´ Hungarian algorithm ê¸°ë°˜ì˜ loss ì„¤ê³„.  Transformers and Parallel Decoding  ë‹¤ë¥¸ RNN ê³„ì—´ë³´ë‹¤ Long sequence ì— ì í•©í•œ model auto-regressive model  Object detection Set-based loss  ê¸°ì¡´ì— bipatite matching loss ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ NMS ë¥¼ ì‚¬ìš©í•´ì•¼ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìŒ. ê·¸ í›„ Learnable NMS ë¥¼ ì‚¬ìš©í•œ ë°©ë²•ì´ ì œì‹œ ë˜ì—ˆìœ¼ë‚˜ hand-crafted context feature ë¥¼ ì‚¬ìš© í•˜ê¸°ì— íš¨ìœ¨ì ì´ì§€ ëª»í•¨.  Recurrent detectors  ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ RNN ê³„ì—´ì„ ë„ì…í•œ Object detection ê¸°ì¡´ ë°©ë²•ì—ì„  Small datasetì„ ì´ìš©í–ˆê³  RNN ê³„ì—´ì„ ì´ìš©í–ˆê¸°ì— parallel êµ¬ì¡°ë¥¼ ê°€ì ¸ê°€ì§€ ëª»í–ˆìŒ.  The DETR model  DETRì€ í¬ê²Œ ë‘ ê°œì˜ ì¥ì ì´ ìˆìŒ. â†’ a set prediction loss, a architecture    Object detection set prediction loss $$\\hat{\\sigma} = {argmin}_{\\sigma \\in \\mathfrak{S}_N} \\sum^N_i \\mathcal{L}_{match}(y_i, \\hat{y}_{\\sigma(i)})$$\n$$\\mathcal{L}_{match}(y_i, \\hat{y}_{\\sigma(i)}) = -1_{c_i \\neq \\phi}\\hat{p}_{\\sigma(i)}(c_i) +1_{c_i \\neq \\phi}\\mathcal{L}_{box}(b_i, \\hat{b}_{\\sigma(i)})$$\n$$\\mathcal{L}_{Hungarian}(y, \\hat{y}) = \\sum^N_i[-\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_i) +1_{c_i \\neq \\phi}\\mathcal{L}_{box}(b_i, \\hat{b}_{\\hat{\\sigma}(i)})]$$\nBounding box loss $$\\lambda_{iou}\\mathcal{L}_{iou}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{\\mathrm{L}1}|b_i - \\hat{b}_{\\sigma(i)}|_1$$\nDETR architecture Backbone  ì¼ë°˜ì ì¸ Backbone ì‚¬ìš©. ë§ˆì§€ë§‰ feataure mapì€ ì›ë³¸ ì‚¬ì´ì¦ˆ H, W ì— ë¹„í•´ 32ë¶„ì˜ 1 downsampling, CëŠ” 2048  Transformer encoder  Attention is all you needì˜ Transformerì˜ encoderì™€ ë™ì¼í•œ êµ¬ì¡°. Fixed positional encodings ìœ¼ë¡œ ì¸í•˜ì—¬ permutation-invariant í•œ êµ¬ì¡°!  Transformer decoder  Attention is all you needì˜ Transformerì˜ decoderì™€ ë™ì¼í•œ êµ¬ì¡°. ê¸°ì¡´ Transformerì™€ ì°¨ì´ëŠ”  Prediction feed-forward networks (FFNs)  ReLUë¥¼ ì‚¬ìš©í•˜ëŠ” d dimensionì˜ linear layer 3 ê°œ ì‚¬ìš©. í•œ branch ì—ì„œëŠ” Normalized center coordinate, height, width ë¥¼ ì˜ˆì¸¡. ë‹¤ë¥¸ í•˜ë‚˜ì˜ branchëŠ” class labelì„ softmaxë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡. DETRì€ í•­ìƒ Nê°œì˜ boxì— ëŒ€í•´ ì˜ˆì¸¡. í•˜ì§€ë§Œ ì‹¤ì œ object ìˆ˜ê°€ ì ì„ë•ŒëŠ” ë‚˜ë¨¸ì§€ boxë“¤ì„ no object ë¡œ ì²˜ë¦¬.  Auxiliary decoding losses  Transformer decoderì— Auxiliary loss ë¥¼ ì¶”ê°€.  Experiment Comparison with Faster R-CNN   Ablations     Panoptic segmentation   P.S  ë‚´ìš© ë³´ì¶© ì˜ˆì •. ì‹ ë°•í•œ ì»¨ì…‰.  ","permalink":"https://jjerry-test.github.io/blog/detr/","tags":["Paper"],"title":"Review: DETR"},{"categories":["Living"],"contents":"D-Linkì˜ DIR-815L ì„ ì˜ ì“°ê³  ìˆì—ˆìŠµë‹ˆë‹¤\u0026hellip;\në¼ì¦ˆë² ë¦¬íŒŒì´ì™€ ì—°ê²°í•˜ì—¬ DDNS ë„í•´ë†“ê³ ..\nê·¼ë° ì–´ëŠ ë‚  ë‹¤ìŒê³¼ ê°™ì€ ê¸€ì´ ë©”ì¼, ê³µì§€ë¡œ ì˜¬ë¼ì˜¤ë”êµ°ìš”.\nì•ˆë…•í•˜ì„¸ìš”. D-Link Korea ì…ë‹ˆë‹¤. ì €í¬ dlink ì œí’ˆêµ°ì—ì„œ ì œê³µë˜ì—ˆë˜ ë¬´ë£Œ dlinkddns ì„œë¹„ìŠ¤ ì¢…ë£Œì— ê´€í•œ ì•ˆë‚´ ë§ì”€ë“œë¦½ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤ë§Œ, dlinkddns ë¬´ë£Œ ì„œë¹„ìŠ¤ëŠ” ê¸°ì¡´ì— ì˜¤ë¼í´ì‚¬ì˜ dyn ddns ì™€ì˜ ê³„ì•½ë§Œë£Œì¼ì¸ 2020ë…„ 7ì›” 2ì¼ ì¢…ë£Œë  ì˜ˆì •ì…ë‹ˆë‹¤. ì´í›„ì—ëŠ” ì •ìƒì ì¸ ë¬´ë£Œ ddnsì„œë¹„ìŠ¤ ì´ìš©ì´ ë¶ˆê°€í•˜ë‹ˆ ê³„ì†í•´ì„œ dlinkddns ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ê¸°ë¥¼ ì›í•˜ì‹¤ ê²½ìš°ì—ëŠ” ìœ ë£Œìƒí’ˆìœ¼ë¡œ ì „í™˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. â–  ìœ ë£Œì „í™˜ http://dlinkddns.com ì‚¬ì´íŠ¸ ì ‘ì†ì‹œ ìœ ë£Œì „í™˜ì€ ì•ˆë‚´ íŒì—…ê³¼ í•¨ê»˜ ê²°ì¬ì‹œ 50% í• ì¸ ë©”ë‰´ í´ë¦­í•˜ì—¬ ì§„í–‰í•©ë‹ˆë‹¤. ì´ì™¸ì— ê´€ë ¨í•´ì„œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹  ë¶„ì€ D-Link Korea ê³ ê°ì„¼í„°(1899-3540)ë¡œ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ê¸°ì¡´ ë¬´ë£Œë¡œ ì§€ì›ë˜ë˜ dlinkddns ì„œë¹„ìŠ¤ ì¢…ë£Œì—, ì•ìœ¼ë¡œ ë”ìš± ë‚˜ì€ ì„œë¹„ìŠ¤ë¡œ ë³´ë‹µí•˜ê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤. What the\u0026hellip;?\ní›„\u0026hellip; ê³µìœ ê¸°ë¥¼ ìƒˆë¡œ ì‚¬ë˜ DDNS ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ë˜ í•´ì•¼ê² ë„¤ìš”.\në””ìì¸ì´ë‘ DDNS ë•Œë¬¸ì— ìƒ€ëŠ”ë°.. ì  ì¥.. ì•ìœ¼ë¡  ê·¸ëƒ¥ Iptime ì“¸ë˜ìš”.\n","permalink":"https://jjerry-test.github.io/blog/dlink_ddns/","tags":["Hardware"],"title":"D-Link ddns ì„œë¹„ìŠ¤ ì¢…ë£Œ"},{"categories":["DeepLearning"],"contents":"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design Author: Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun\nDate: Jul 30, 2018\nURL: https://arxiv.org/abs/1807.11164\nIntroduction  CNN ê³„ì—´ì´ AlexNet ë¶€í„° ì •í™•ë„, ì†ë„ê°€ ë§ì´ ë°œì „í•˜ê³  ìˆìŒ. ì‹¤ì œë¡œëŠ” ì œí•œëœ ì»´í“¨íŒ… íŒŒì›Œì—ì„œ(Mobileê³¼ ê°™ì€) ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚´ëŠ”ë° ëª©í‘œë¡œ í•˜ê³  ìˆìŒ. ì´ë¡œ ì¸í•´ Xception, MobileNet, ShuffleNet ë“±ì´ ê°œë°œ ë˜ì—ˆìŒ. ì§€ê¸ˆê¹Œì§€ëŠ” ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ì„ íŒë‹¨í•˜ì˜€ìœ¼ë‚˜ ì í•©í•œ ì§€í‘œê°€ ì•„ë‹˜ì„ ì£¼ì¥. FLOPsì™€ speed ê°„ì˜ ì„±ëŠ¥ ë¹„êµê°€ ì˜³ì§€ ì•Šì€ ì£¼ìš” ì´ìœ ê°€ ë‘ ê°€ì§€.  memory access cost(MAC): ë©”ëª¨ë¦¬ ì ‘ê·¼ëŸ‰(ì‚¬ìš©ëŸ‰) depending on the platform      Practical Guidelines for Ecient Network Design  ë³¸ ì—°êµ¬ëŠ” GPU í•˜ë“œì›¨ì–´(1080ti), ARM í•˜ë“œì›¨ì–´(Snapdragon 810) ì´ ë‘ ê°€ì§€ í™˜ê²½ì—ì„œ ì‹¤í—˜. ëª¨ë¸ì˜ Runtimeì„ ìª¼ê°œë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì°¨íŠ¸ê°€ ê·¸ë ¤ì§. FLOPsëŠ” Convolution ì— ëŒ€í•´ ì„¤ëª…í•˜ê¸° ë–„ë¬¸ì— ë¹„êµ ì§€í‘œë¡œ ì ì ˆí•˜ì§€ ëª»í•¨ì„ ê°•ì¡°.     ìœ„ ë¬¸ì œë¥¼ ê·¼ê±°ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì—¬ëŸ¬ ê°œì˜ ê°€ì´ë“œ ë¼ì¸ì„ ì œì‹œ.  G1) Equal channel width minimizes memory access cost (MAC)  ê·¼ë˜ì— ë§ì´ ì‚¬ìš©ë˜ëŠ” depthwise separable convoltuionì—ì„œ ì—°ì‚°ëŸ‰ì˜ ëŒ€ë¶€ë¶„ì€ pointwise convolution ì´ ì°¨ì§€. 1x1 convolution ì´ ì°¨ì§€í•˜ëŠ” ì—°ì‚°ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ìŒ.  $$h, w: \\text{the spatial size of the input feature map} $$ $$c_1, c_2: \\text{Number of channels about input and output }$$ $$B=hwc_1c_2, \\text{ FLOPs of the }1 \\times 1 \\text{ convolution}$$\n í˜„ ìƒí™©ì—ì„œ MACì˜ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ.  $$MAC = hw(c_1+c_2) +c_1c_2 = hwc_1 + hwc_2 + c_1c_2$$ $$hwc_1: \\text{Number of input feature map\u0026rsquo;s element}$$ $$hwc_2: \\text{Number of output feature map\u0026rsquo;s element}$$ $$c_1c_2: \\text{Number of filter\u0026rsquo;s element}$$\n MACì˜ lower bound ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ.  $$MAC \\ge 2\\sqrt{hwB} + \\frac{B}{hw} \\to 2hw\\sqrt{c_1c_2} + c_1c_2$$\n \\(c_1 = c_2\\) ì´ë©´ MACê°€ ìµœì†Œ. ì „ì²´ ì—°ì‚°ëŸ‰ì€ ê³ ì •í•´ë†“ê³  \\(c_1:c_2\\)ì˜ ë¹„ìœ¨ì„ ë°”ê¿”ê°€ë©´ì„œ runtime ë¹„êµ. 1:1ì¼ë•Œê°€ ê°€ì¥ ë¹ ë¥¸ ì„±ëŠ¥ì„ ë³´ì„.     It reaches the lower bound when the numbers of input and output channels are equal.\n G2) Excessive group convolution increases MAC  Group convolution ì´ ë§ì€ ë„¤íŠ¸ì›Œí¬ì˜ í•µì‹¬ì´ì§€ë§Œ Groupsê°€ ì»¤ì§€ë©´ MACì„ ì¦ê°€ì‹œí‚´. â†’ ì•ˆì“¸ ìˆ˜ëŠ” ì—†ìœ¼ë‹ˆ ì ë‹¹íˆ ì“°ëŠ”ê²Œ ì¢‹ë‹¤. ê·¸ë£¹ì— ë”°ë¼ ì—°ì‚°ëŸ‰ì´ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì— BëŠ” ë‹¤ìŒê³¼ ê°™ìŒ.  $$B=\\frac{hwc_1c_2}{g}$$\n$$MAC = hw(c_1+c_2) + \\frac{c_1c_2}{g} $$ $$ = hwc_1 + hwc_2 + \\frac{c_1c_2}{g}$$ $$ = hwc_1 + \\frac{Bg}{c_1} + \\frac{B}{hw}$$\n Groups ì— ë”°ë¼ runtime ë¹„êµ.     The group number should be carefully chosen based on the target platform and task. It is unwise to use a large group number simply because this may enable using more channels, because the benet of accuracy increase can easily be outweighed by the rapidly increasing computational cost.\n G3) Network fragmentation reduces degree of parallelism  Inception ê³¼ ê°™ì´ ì—¬ëŸ¬ branchë¥¼ parallelí•˜ê²Œ êµ¬ì„±í•  ê²½ìš° ì„±ëŠ¥ì€ ì¢‹ì•„ì¡Œì§€ë§Œ íš¨ìœ¨ì„±ì€ ê°ì†Œì‹œí‚´. â†’ GPU ê°™ì€ ìì›ì—ëŠ” ì–´ìš¸ë¦¬ì§€ ì•ŠìŒ. Fragmentation ì— ë”°ë¥¸ runtime ë¹„êµ.       Fragmented structure has been shown benecial for accuracy, it could decrease eciency because it is unfriendly for devices with strong parallel computing powers like GPU. It also introduces extra overheads such as kernel launching and synchronization.\n G4) Element-wise operations are non-negligible   Activation, Add ì™€ ê°™ì€ Element-wise operationë“¤ì˜ ë¹„ìœ¨ì´ ê½¤ ì¡´ì¬. Figure 2 ì°¸ê³ \n  ì´ ì—°ì‚°ì€ FLOPsëŠ” ì ì§€ë§Œ ìƒëŒ€ì ìœ¼ë¡œ MACì€ í¼.\n  Depthwise convolution ë˜í•œ element-wise ì—¬ì„œ MAC/FLOPs ê°€ í´ ê²ƒì´ë¼ ìƒê°.\n  ê° ìƒí™©ì— ëŒ€í•œ runtime ë¹„êµ.\n     We observe around 20% speedup is obtained on both GPU and ARM, after ReLU and shortcut are removed.\n Conclusion and Discussions  use \u0026ldquo;balanced convolutions (equal channel width); be aware of the cost of using group convolution; reduce the degree of fragmentation; reduce element-wise operations.   ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ë“¤ì— ëŒ€í•œ ê³ ì°°   ShuffleNet V1\n Heavily group convolutions â†’ G2 Bottleneck-like building blocks â†’ G1 Residual Block â†’ G3 Element-wise operationâ†’ G4    MobileNet V2\n Inverted bottleneck structure â†’ G1    Depthwise convolution \u0026amp; ReLU\n Element-wise operation â†’ G4    NAS\n Highly fragmentation â†’ G3      ShueNet V2: an Ecient Architecture Review of ShueNet v1  G1, G2, G3, G4 ëª¨ë‘ ì§€í‚¤ì§€ ì•ŠìŒ. ì´ë¥¼ í•´ê²°í•œ êµ¬ì¡°ê°€ ShuffleNet V2 ì˜ ìœ ë‹› (Fig 3 (c), Fig 3 (d))    Channel Split and ShueNet V2   Fig 3 (c)\n Input feature ì„ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ  ë‘ê°œì˜ branch ìƒì„±. Left branchëŠ” ì•„ë¬´ ì—°ì‚°ë„ ì§„í–‰ X. â†’ G3 ì— ëŒ€í•œ íšŒí”¼ë²•. Right branchëŠ” ë™ì¼í•œ Number of filterë¡œ 1x1 Conv â†’ 3x3 DWConv â†’ 1x1 Conv ìˆ˜í–‰. â†’ G1ì— ëŒ€í•œ íšŒí”¼ë²•. 1x1 Conv ëŠ” Group ì„ ë‚˜ëˆ„ì§€ ì•ŠìŒ â†’ G2ì— ëŒ€í•œ íšŒí”¼ë²•. Residual Blockì˜ Add operation ì„ Concatenate ë¡œ ë³€ê²½ â†’ G4ì— ëŒ€í•œ íšŒí”¼ë²•.    Fig 3 (d)\n Downsampling block Input feature ê·¸ëŒ€ë¡œ ë‘ê°œì˜ branch ìƒì„±. Number of filterëŠ” ëª¨ë‘ ë™ì¼    ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°\n    Analysis of Network Accuracy  ShuffleNet V2ëŠ” íš¨ìœ¨ì ì´ë©° ì„±ëŠ¥ë„ ì¢‹ìŒ.  ë” ë§ì€ channel, ë” í° networkë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ. DenseNet ì´ë‚˜ CondenseNet ì²˜ëŸ¼ feature reuse ê³¼ ë§¤ìš° ìœ ì‚¬í•¨.   DenseNetì˜ feature reuse íŒ¨í„´ê³¼ ShuffleNet V2ì˜ feature reuse íŒ¨í„´ ë¹„êµ. ë¶‰ì„ ìˆ˜ë¡ Source layerì™€ Target layerì˜ ì—°ê²°ì„±ì´ ê°•í•˜ë‹¤ëŠ” ì˜ë¯¸. DenseNetê³¼ ê°™ì´ ShuffleNet V2ì—ì„œë„ Target layerê°€ ë©€ì–´ì§ˆ ìˆ˜ë¡ ì—°ê²°ì„±ì´ ì•½í•¨.    Experiment  ì´ 4ê°œì˜ ëª¨ë¸ê³¼ ë¹„êµ.  ShuffleNet V1 MobileNet V2 Xception DenseNet      Accuracy vs. FLOPs  ì—°ì‚°ëŸ‰ì„ 40 MFLOPs ë¡œ ê³ ì •ì‹œí‚¤ê³  Network ë¥¼ êµ¬ì„±í•œ í›„ ì„±ëŠ¥ ë¹„êµ. (Table 8 ìƒë‹¨)  Inference Speed vs. FLOPs/Accuracy  ì—°ì‚°ëŸ‰ì„ íŠ¹ì • ê°’ ë²”ìœ„ë¡œ ê³ ì •ì‹œí‚¤ê³  runtime ë¹„êµ. (Fig 1 ì°¸ì¡°)    Compared with MobileNet v1  MobileNet V1ì˜ ê²½ìš° Accuracyê°€ ì¢‹ì§€ ì•Šìœ¼ë‚˜ GPU runtimeì€ ê°€ì¥ ë¹ ë¦„. ì´ëŠ” ìœ„ì—ì„œ ì œì‹œí•œ ê°€ì´ë“œ ë¼ì¸ì„ ì–´ëŠ ì •ë„ ê°€ì¥ ì˜ ë§Œì¡±í•˜ê¸° ë•Œë¬¸.  Compared automatic model search  NAS ëŠ” ë§¤ìš° ëŠë¦¬ì§€ë§Œ ì œì‹œí•œ ê°€ì´ë“œ ë¼ì¸ì„ ë§Œì¡±í•˜ê³  speedì— ëŒ€í•œ metricì„ ì‚¬ìš©í•œë‹¤ë©´ ì¶©ë¶„íˆ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒ.  Compatibility with other methods  Squeeze-and-excitation ê³¼ ê°™ì€ moduleê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ. ì†ë„ëŠ” ë–¨ì–´ì§€ë‚˜ ì •í™•ë„ëŠ” ìƒìŠ¹. (Table 8 í•˜ë‹¨)  Generalization to Large Models  2GFLOPs ì´ìƒì˜ í° ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ. 50ê°œì˜ ë ˆì´ì–´ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ìƒì„±í•´ë„ ResNet-50 ê³¼ ë¹„êµí•˜ì—¬ ì ì€ ì—°ì‚°ëŸ‰, ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„. (Table 6 ìƒë‹¨) SE module, residual blockì„ ì‚¬ìš©í•˜ì—¬ ë”ìš± ê¹Šê²Œ ë§Œë“¤ì–´ë„ ìƒëŒ€ì ìœ¼ë¡œ ì—°ì‚°ëŸ‰ì´ ì ìœ¼ë©´ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„. (Table 6 í•˜ë‹¨)    Object Detection  Light-Head RCNN ì‚¬ìš©. Classification ì—ì„œ ì„±ëŠ¥ì´ ë³„ë¡œì˜€ë˜ Xception ì´ Detection ì—ì„  ì„±ëŠ¥ì´ ì¢‹ìŒ. â†’ Receptive Fieldê°€ í¬ê¸° ë•Œë¬¸ì´ë¼ê³  ìƒê°. 3x3 depthwise convolution ì„ ì¶”ê°€í•˜ì—¬ Receptive Fieldë¥¼ í‚¤ì›Œë³´ë‹ˆ (ShuffleNet V2*) runtimeì€ ëŠ˜ì—ˆìœ¼ë‚˜ ì„±ëŠ¥ì´ ì¦ê°€í•¨.    P.S  ì–´ë ¤ì› ë‹¤. í•­ìƒ ê°€ì´ë“œ ë¼ì¸ì„ ì§€í‚¤ë©´ì„œ ëª¨ë¸ì„ ì„¤ê³„í•  ìˆ˜ ìˆì„ê¹Œ?  ","permalink":"https://jjerry-test.github.io/blog/shufflenetv2/","tags":["Paper"],"title":"Review: ShuffleNetV2"},{"categories":["DeepLearning"],"contents":"Aggregated Residual Transformations for Deep Neural Networks Author: Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, Kaiming He\nDate: Nov 16, 2016\nURL: https://arxiv.org/abs/1611.05431\nIntroduction  Network desine ì— ê³ ë ¤í•´ì•¼í•  hyper-parameterê°€ ë„ˆë¬´ ë§ìŒ. (Width, filter size, Height, \u0026hellip;.) VGG, ResNetì€ ë¹„ìŠ·í•œ êµ¬ì¡°ì˜ ë ˆì´ì–´ë¥¼ ê³„ì† ìŒ“ëŠ” ë°©ë²•ì„ ì‚¬ìš©. Inception ì€ ì„±ëŠ¥ì€ ì´ì „ë³´ë‹¤ ë›°ì–´ë‚˜ì§€ë§Œ ì´ì „ ë°©ë²•ë“¤ê³¼ ë‹¤ë¥´ê²Œ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ìŒ“ëŠ” ë°©ë²• ì‚¬ìš©. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” VGG/ResNetê³¼ ê°™ì´ ë¹„ìŠ·í•œ ë ˆì´ì–´ë¥¼ ë°˜ë³µí•˜ì§€ë§Œ AlexNet ì—ì„œ ë‚˜ì˜¨ ì²˜ìŒ ì œì•ˆëœ Group Convolution ì„ ì ìš©í•˜ì—¬ split-transform-merge stretegy ë¥¼ ë„ì…. ì¼ë°˜ì ì¸ Reidual Blockê³¼ ResNeXtì˜ Residual Block ë¹„êµ. Cardinality = Number of Groups,    Method Template  ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” ê¸°ì¡´ì˜ VGG/ResNetê³¼ ê°™ì´ ì¼ì • Block ì„ ë°˜ë³µí•˜ì—¬ ìŒ“ëŠ” êµ¬ì¡°. ë°˜ë³µë˜ëŠ” ë¸”ëŸ­ì€ ë™ì¼í•œ hyper parameter ì‚¬ìš©.    Revisiting Simple Neurons  ê°€ì¥ ê¸°ë³¸ì ì¸ ë‰´ëŸ°ì˜ êµ¬ì¡°  $$\\sum_{i=1}^Dw_ix_i$$\n   ê¸°ë³¸ ë‰´ëŸ° ë˜í•œ split-transform-merge (Splitting, Transforming, Aggregating)ì˜ êµ¬ì¡°ë¥¼ ê°€ì§. Vector X ê°€ $x_i$ë¡œ Splitting, $x_iw_i$ë¡œ Transforming, $\\sum_{i=1}^D$ ë¡œ Aggregating  Aggregated Transformations  Networt-in-Networkì™€ ë‹¤ë¥´ê²Œ Network-in-Neuronì´ë¼ëŠ” ì»¨ì…‰ìœ¼ë¡œ ì°¨ì› í™•ì¥  $$\\mathcal{F}(x) = \\sum_{i=1}^C\\mathcal{T}_i(\\mathrm{x})$$\n ë‹¤ë¥¸ ë°©ì‹ì´ì§€ë§Œ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ ì„¤ëª…      Model Capacity  Complexity, Number of parameter ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì‹¤í—˜. ë‹¤ë¥¸ Hyper parameterëŠ” ìˆ˜ì •í•˜ê³  ì‹¶ì§€ ì•Šê¸° ë•Œë¬¸ì— Residual Blockì˜ Cardinality Cì™€ bottleneck dë¥¼ ìˆ˜ì • Cardinalityì™€ bottleneck dì˜ ê´€ê³„    Result On ImageNet-1K  Cardinalityë¥¼ 1~32 ì”© ì¦ê°€ì‹œí‚¤ë˜ complexity ëŠ” ìœ ì§€í•˜ë„ë¡ ì„¤ì •í•˜ê³  ì‹¤í—˜.       Increasing Cardinality ì™€ Increasing depth or width ë¹„êµ Cardinality ì˜ ì„±ëŠ¥ì´ ë” ì¢‹ìŒ.     Residual connections ì—¬ë¶€ì— ë”°ë¥¸ ë¹„êµ     State-of-the-art model ê³¼ ë¹„êµ    On ImageNet-5K  5000ê°œ í´ë˜ìŠ¤ì—ì„œë„ ì˜ ë˜ë”ë¼.      On CIFAR     On COCO object detection  Faster RCNNì— ì ìš©.    P.S  AlexNetì˜ ì„ ê²¬ì§€ëª…. í•˜ê¸´ ì•ˆì¢‹ìœ¼ë©´ ë…¼ë¬¸ìœ¼ë¡œ ì“¸ë¦¬ê°€ ì—†ì§€.  ","permalink":"https://jjerry-test.github.io/blog/resnext/","tags":["Paper"],"title":"Review: ResNeXt"},{"categories":["DeepLearning"],"contents":"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices Author: Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun\nDate: Jul 04, 2017\nURL: https://arxiv.org/abs/1707.01083\nIntroduction  Visual recognition ì—ì„œ deeper, larger CNNì„ ì„¤ê³„í•˜ëŠ” ê²ƒì´ íŠ¸ë Œë“œ. í•˜ì§€ë§Œ ì´ëŠ” ë§¤ìš° ë§ì€ ì—°ì‚°ëŸ‰ì„ í•„ìš”ë¡œí•¨. ë³¸ ë…¼ë¬¸ì€ ì •í•´ë†“ì€ ë²”ìœ„ì˜ ì—°ì‚°ëŸ‰ì—ì„œ ìµœê³ ë¡œ íš¨ìœ¨ì ì¸ êµ¬ì¡°ëŠ” ì°¾ì•„ë‚´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨. Xception, ResNeXt ì—ì„œ 1x1 Convolution ì„ ì‚¬ìš©í•˜ëŠ”ë° ë‘ ë„¤íŠ¸ì›Œí¬ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ì—°ì‚°ëŸ‰ì´ 1x1 Convolution ì´ ì°¨ì§€í•˜ê³  ìˆì–´ ë¹„íš¨ìœ¨ì . ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ AlexNetì—ì„œ ì²˜ìŒ ì œì•ˆí•œ group convolution ì ìš©. Group convolution ì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ channel shuffle operation ë˜í•œ ì œì•ˆ.  Method Channel Shuffle for Group Convolutions  ìƒëŒ€ì ìœ¼ë¡œ ì—°ì‚°ëŸ‰ì´ ë§ì€ 1x1 Convolutionì„ ResNeXt ì—ì„œ ì‚¬ìš©í•œ Group Convolution ìœ¼ë¡œ ì ìš©. í•˜ì§€ë§Œ Groupìœ¼ë¡œ ê³„ì† ì§„í–‰í•˜ë‹¤ë³´ë©´ íŠ¹ì • ì±„ë„ì— í¸í–¥ëœ ê²°ê³¼ë¥¼ ë³´ì´ëŠ” ë¬¸ì œê°€ ìƒê¸¸ ê²ƒì´ë¯€ë¡œ channelì„ shuffle í•´ì¤Œ.    ShuffleNet Unit  ShuffleNetì—ì„œ ì‚¬ìš©ëœ Bottle unitì€ Xceptionê³¼ MobileNetì—ì„œ ì‚¬ìš©ëœ Residual Blockì—ì„œ 1x1 Convolutionì„ Group Convolutionìœ¼ë¡œ ë³€ê²½í•˜ê³  Channel Shuffleì„ ì¶”ê°€í•œ ê²ƒ. Stride unit ì—ì„  element-wise additionì´ ì•„ë‹Œ concatenationìœ¼ë¡œ ëŒ€ì²´.    Network Architecture   Result Ablation Study Pointwise Group Convolutions   Groups ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ.\n  ShuffleNet s\\(\\times\\) ì—ì„œ sëŠ” í•„í„° ê°œìˆ˜ì— ëŒ€í•œ scaling factor.\n  ë¬´ì¡°ê±´ ë§ì´ ë‚˜ëˆˆë‹¤ê³  ì¢‹ì€ ê²ƒì€ ì•„ë‹˜.\n    Channel Shuffle vs. No Shuffle  Channel Shuffle ì—¬ë¶€ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ. Shuffle ì ìš©ì‹œ ì„±ëŠ¥ì´ ëšœë ·í•˜ê²Œ ì¦ê°€í•œ ê²ƒì„ í™•ì¸.    Comparison with Other Sturcture Units  ì œí•œëœ ì—°ì‚°ëŸ‰ ë‚´ì—ì„œ ë‹¤ë¥¸ Network ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ.     ê¸°ì¡´ì— ë¹„ìŠ·í•œ ì„±ëŠ¥ì˜ Networkë“¤ê³¼ ì—°ì‚°ëŸ‰ ë¹„êµ.    Comparison with MobileNets and Other Frameworks  Mobile devicesì— íŠ¹í™”ëœ MobileNetê³¼ ì„±ëŠ¥ ë¹„êµ    Generalization Ability  MS COCO Dataë¥¼ ì‚¬ìš©í•˜ì—¬ Object detection ì„±ëŠ¥ ë¹„êµ.    Actual Speedup Evaluation  Mobile deviceì—ì„œ Inference ì†ë„ ë¹„êµ.    P.S  GPUê°€ ë¶€ì¡±í•´ì„œ í–ˆë‹¤ë˜ Group Convolutionì˜ ë¶€í™œ..?  ","permalink":"https://jjerry-test.github.io/blog/shufflenetv1/","tags":["Paper"],"title":"Review: ShuffleNetV1"},{"categories":["DeepLearning"],"contents":"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features Author: Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo\nDate: May 13, 2019\nURL: https://arxiv.org/abs/1905.04899\nIntroduction  CNN ì€ computer vision ë¬¸ì œì— ë§ì´ ì‚¬ìš©ë˜ê³  ìˆìŒ. íš¨ìœ¨ì ì´ê³  ë†’ì€ ì„±ëŠ¥ì„ ìœ„í•´ data augmentation, regularization ë“± ê¸°ë²•ì„ ì ìš©. íŠ¹ì • ë¶€ë¶„ì— overfitting(?) ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ dropout, regional dropout ê³¼ ê°™ì€ ë°©ë²• ì‚¬ìš©. ê·¸ ì™¸ì—ë„ ì¼ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ìš´ë‹¤ê±°ë‚˜ ë…¸ì´ì¦ˆë¡œ ì±„ìš°ëŠ” ë°©ë²•, ì •ë³´ê°€ ìˆëŠ” ë¶€ë¶„ì˜ pixelì„ ì¤„ì´ëŠ” ë°©ë²• ë“±ì´ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì˜€ìœ¼ë‚˜ CNNì€ ë°ì´í„°ê°€ ë§ì´ ê³ í”ˆë°\u0026hellip;.ë°ì´í„°ë¥¼ ì—†ì•¤ë‹¤..? ë¼ëŠ” ë¶€ë¶„ì—ì„œ ì˜ë¬¸ì„ ê°€ì§. ì˜ìƒì˜ ì¼ë¶€ë¥¼ ìë¥´ê³  ë‹¤ë¥¸ ì˜ìƒìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” CutMix ë¥¼ ì œì•ˆ.    CutMix Algorithm  A, B ë‘ê°œì˜ í´ë˜ìŠ¤ë§Œ ì¡´ì¬.  $$(x, y): \\text{Training image, label}$$ $$(A, B): \\text{Training class}$$ $$(x_A, y_A), (x_B, y_B): \\text{Training sample}$$\n ì–´ëŠ ë¶€ë¶„ì„ ì„ì„ ê²ƒì¸ì§€ binary mask (M) ìƒì„± ìƒì„±ëœ maskë¥¼ í†µí•´ ì„ì„ ë¹„ìœ¨ lambda ì¶”ì¶œ. Labelì˜ ê²½ìš° ë¹„ìœ¨ì— One-hot encodingì´ í•©ì¹œ í›„ ì˜ìƒì—ì„œì˜ ê° í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ë¡œ ë³€ê²½.  $$\\mathrm{M}: \\text{Binary mask where to drop out and fill}$$ $$\\lambda: \\text{Combination ratio}$$ $$\\tilde{x} = \\mathrm{M} \\bigodot x_A + (1 - \\mathrm{M}) \\bigodot x_B$$ $$\\tilde{y} = \\lambda{y_A} + (1 - \\lambda)y_B$$\n Mì—ì„œ bounding box ì¢Œí‘œ (B) ì¶”ì¶œ. x, y ì¢Œí‘œëŠ” Uniform distribution. $x_B$ì—ì„œ B ë¥¼ ë§¤ì¹­ì‹œì¼œì„œ crop í›„ Bì— ë§¤ì¹­ë˜ëŠ” $x_A$ ì˜ ë¶€ë¶„ì— paste.  $$\\mathrm{B}: \\text{Bounding box coordinates } (r_x, r_y, r_w, r_h)$$ $$r_x \\sim \\text{Unif }(0, W), r_w = W\\sqrt{1-\\lambda},$$ $$r_y \\sim \\text{Unif } (0, H), r_h = H\\sqrt{1-\\lambda}$$\nDiscussion  CutMixë¥¼ ì´ìš©í–ˆì„ ë•Œ CNNì´ ì–´ëŠ ë¶€ë¶„ì„ í•™ìŠµí•˜ëŠ”ì§€ í™•ì¸.     ë‹¤ë¥¸ methodì™€ ë¹„êµí•˜ì—¬ CutMixì˜ ì£¼ìš” ì°¨ì´ì .     Validation Errorë¥¼ ë¹„êµí–ˆì„ ë•Œ ê¸°ì¡´ì˜ ëª¨ë¸ì— ë¹„í•´ CutMix ì ìš©ì‹œ Errorê°€ ë‚®ìŒ.    Experiments Image Classification ImageNet Classification  Baseline, ë‹¤ë¥¸ augmentation methodì™€ ë¹„êµ     ë‘ Modelì— CutMixë¥¼ ì ìš©í•˜ì—¬ ì„±ëŠ¥ ë¹„êµ.    CIFAR Classification  ë‹¤ë¥¸ Regularization ë“¤ê³¼ ë¹„êµ.     ê°€ë²¼ìš´ Model ì— ì ìš©í•˜ì—¬ ë¹„êµ.     CIFAR-10ì— ì ìš©í•œ ê²°ê³¼.    Ablation Studies  CutMix ì—ì„œ alphaê°€ ë­ì§€\u0026hellip;\u0026hellip;.     CutMixí•˜ëŠ” ë°©ë²•ì„ ë‹¤ì–‘í•˜ê²Œ ì ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ ë¹„êµ Center Gaussian: Uniform distribution â†’ Gaussian distribution Fixed-size: 16 x 16 ( \\(\\lambda = 0.75\\) )ë¡œ ê³ ì • Scheduled: í•™ìŠµì´ ì§„í–‰ë  ìˆ˜ë¡ CutMix í™•ë¥ ì„ 0ë¶€í„° 1ê¹Œì§€ ì¦ê°€ One-hot: íŒ¨ì¹˜ ë¹„ìœ¨ì— ë”°ë¼ Portion labelì´ ì•„ë‹Œ One-hot encodingìœ¼ë¡œ ì ìš© Complete-label: lambda ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  \\(y = 0.5y_A + 0.5y_B\\)ë¡œ ì ìš©    Weakly Supervised Object Localization  Localization ë¶€ë¶„ì— ëŒ€í•´ ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ë¹„êµ. í•™ìŠµ í›„ CAMì„ ì´ìš©í•´ì„œ bounding boxë¥¼ ê·¸ë¦° ê²ƒìœ¼ë¡œ ë³´ì„.      Transfer Learning of Pretrained Model  Object detection, Image captioning ì— ì ìš©í•˜ì—¬ ì„±ëŠ¥ ë¹„êµ.    Robustness and Uncertainty  Adversarial attack ì— ëŒ€í•´ Accuracy ë¹„êµ. Fast Gradient Sign Method (FGSM)ì„ ì´ìš©í•˜ì—¬ adversarial perturbation ìƒì„±.     Occlusion ìƒí™©ì— ëŒ€í•´ì„œ ì„±ëŠ¥ ë¹„êµ. ê°€ìš´ë° ë¶€ë¶„ í˜¹ì€ Boundary ì— 0~224 í¬ê¸° ì‚¬ì´ì˜ holeì„ ìƒì„±.     Uncertainty    CutMix Algorithm   P.S  Appendixì— ë‚´ìš©ì´ ë” ìˆì§€ë§Œ\u0026hellip; ê°„ë‹¨íˆ ì •ë¦¬í•˜ë ¤ë‹ˆ ë„£ê¸° ì¢€ í˜ë“¦. ë‹¹ì—°í•œ ì–˜ê¸°ì§€ë§Œ ëª¨ë“  ë°ì´í„°ì— ì ìš©í•˜ê¸°ì—” ì–´ë ¤ì›€ì´ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì„.  ","permalink":"https://jjerry-test.github.io/blog/cutmix/","tags":["Paper"],"title":"Review: CutMix"},{"categories":["Ubuntu"],"contents":"ì§€ê·¹íˆ ê°œì¸ì´ ì‚¬ìš©í•˜ê¸° ìœ„í•œ Dockerfile   í™˜ê²½ì„ ë§Œë“¤ ë•Œë§ˆë‹¤ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤. ë§ˆìŒê» í¸í•˜ì‹ ëŒ€ë¡œ Copy \u0026amp; Paste í•˜ì„¸ìš”!  TensorFlow FROMnvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04LABEL maintainer \u0026#34;Jerry Kim \u0026lt;jaeyeol2931@gmail.com\u0026gt;\u0026#34;ARG PYTHON_VERSION=3.7RUN apt-get updateRUN apt-get install -y \\  build-essential \\  cmake \\  git \\  curl \\  wget \\  ca-certificates \\  libjpeg-dev \\  libpng-devRUN apt-get update \u0026amp;\u0026amp; apt-get -y upgrade# 2020.10.11 ë‚´ìš© ì¶”ê°€# Docker containerì— sshë¡œ ì ‘ì†í•˜ê³  ì‹¶ë‹¤ë©´....# RUN mkdir /var/run/sshdRUN echo \u0026#39;root:{ê°œì¸ ë¹„ë°€ë²ˆí˜¸}\u0026#39; | chpasswdRUN sed -i \u0026#39;s/PermitRootLogin prohibit-password/PermitRootLogin yes/\u0026#39; /etc/ssh/sshd_config# SSH login fix. Otherwise user is kicked off after loginRUN sed \u0026#39;s@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g\u0026#39; -i /etc/pam.d/sshdENV NOTVISIBLE \u0026#34;in users profile\u0026#34;RUN echo \u0026#34;export VISIBLE=now\u0026#34; \u0026gt;\u0026gt; /etc/profileEXPOSE22CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;]RUN rm -rf /var/lib/apt/lists/*RUN curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o ~/miniconda.shRUN chmod +x ~/miniconda.sh \u0026amp;\u0026amp; \\  ~/miniconda.sh -b -p /opt/conda \u0026amp;\u0026amp; \\  rm ~/miniconda.shRUN /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include ninja cython typing opencv matplotlib tqdm \u0026amp;\u0026amp; \\  /opt/conda/bin/conda install -y jupyter jupyterlab seaborn pillow pandas pylint scikit-learn scikit-image tensorflow-gpu \u0026amp;\u0026amp; \\  /opt/conda/bin/conda update -y --all \u0026amp;\u0026amp; \\  /opt/conda/bin/conda clean -yaENV PATH /opt/conda/bin:$PATH","permalink":"https://jjerry-test.github.io/blog/dockerfile/","tags":["Docker"],"title":"ê°œì¸ì ì¸ ë„ì»¤ íŒŒì¼"},{"categories":["DeepLearning"],"contents":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks Author: Mingxing Tan, Quoc V. Le\nDate: May 28, 2019\nURL: https://arxiv.org/abs/1905.11946\nIntroduction  ConvNet ì˜ ì„±ëŠ¥ì„ ë†’ì´ëŠ”ë° Depth, Width, Image size ì¤‘ í•˜ë‚˜ë¥¼ ì¦ê°€ ì‹œí‚¤ëŠ”ê²Œ ì¼ë°˜ì ì¸ ë°©ë²•. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ConvNetì˜ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•œ ì›ë¡ ì ì¸ ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬. ì‹¤í—˜ì˜ ê²°ê³¼ë¡œ Compound scaling method ë¥¼ ì œì•ˆ.    image   Compound Model Scaling  Scaling ë¬¸ì œì— ëŒ€í•œ ì •ì˜, Approache ë³„ ì—°êµ¬, ìƒˆë¡œìš´ ë°©ë²•ì— ëŒ€í•œ ë‚´ìš© ì„œìˆ .  Problem Formulation  Model scalingì€ Baseline ì—ì„œ Length(Depth), Width, Resolution ë¥¼ í™•ì¥. í•˜ì§€ë§Œ ì‹¤ì œë¡  ë¦¬ì†ŒìŠ¤ì— ì œì•½ì´ ìˆìœ¼ë‹ˆ ì´ì— ë§ì¶° ë¬¸ì œë¥¼ ìƒˆë¡­ê²Œ, ë‹¨ìˆœí•˜ê²Œ ì •ì˜. Design spaceë¥¼ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë“  ë ˆì´ì–´ëŠ” ìƒìˆ˜ ê°’ì„ ì´ìš©í•˜ì—¬ ê·œì¹™ì ìœ¼ë¡œ ë³€í™”í•˜ë„ë¡ í•¨. ìµœì¢… ëª©ì ì€ ì œí•œëœ ë¦¬ì†ŒìŠ¤ì—ì„œ ì„±ëŠ¥ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒ.  $$\\mathcal{N}: \\text{ConvNet}$$ $$ \\mathcal{F}_i: \\text{Layer architecture}$$ $$L_i: \\text{Network length}$$ $$C_i: \\text{Width}$$ $$H_i, W_i: \\text{Input resolution}$$\n$$ {max}_{d, w, r} \\space\\space\\space\\space Accuracy(\\mathcal{N}(d, w, r)) $$ $$s.t. \\space\\space\\space\\space \\mathcal{N}(d, w, r) = \\bigodot_{i=1\u0026hellip;s}\\hat{\\mathcal{F}}_i^{d \\cdot \\hat{L}_i}(X_{\\langle r\\cdot \\hat{H}_i, r \\cdot \\hat{W}_i, w\\cdot \\hat{C}_i \\rangle} )$$ $$Memory(\\mathcal{N}) \\leq \\text{target memory}$$ $$FLOPS(\\mathcal{N}) \\leq \\text{target flops}$$\nScaling Dimensions  ë‘ë²ˆì§¸ ë¬¸ì œëŠ” d, w, r ì´ ì„œë¡œ dependent í•˜ê³  ì œí•œëœ ë¦¬ì†ŒìŠ¤ì— ë”°ë¼ ê°’ì´ ë³€í™”. ê·¸ë˜ì„œ ê¸°ì¡´ì—ëŠ” ë‹¤ìŒ ì„¸ ê°œì˜ ìš”ì†Œ ì¤‘ í•˜ë‚˜ë¥¼ ë³€ê²½í•¨.  Depth (d)  VGGNet, GoogLeNet, ResNet ë“±ë“± ë ˆì´ì–´ë¥¼ ë§ì´ ë§ì´ !  Width (w)  ì±„ë„ ìˆ˜ë¥¼ ëŠ˜ë¦¬ê³  ê¹Šì´ë¥¼ ì¤„ì´ëŠ” ë°©ì‹. í•˜ì§€ë§Œ higher level featureë¥¼ ì¡ê¸° í˜ë“¤ ìˆ˜ ìˆìŒ.  Resolution (r)  í´ìˆ˜ë¡ ë” ì–‘ì§ˆì˜ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ìˆìŒ.    image    Observation 1: ì´ë¥¼ í†µí•´ì„œ ì–´ë–¤ ìš”ì†Œë¥¼ ì¦ê°€ì‹œí‚¤ë˜ ì„±ëŠ¥ì´ ì˜¤ë¥´ëŠ” ê²ƒì„ í™•ì¸ í•˜ì§€ë§Œ Modelì´ ë¬´ê±°ì›Œì§.  Compound Scaling  ê²½í—˜ì ìœ¼ë¡œ ì„¸ ìš”ì†Œê°€ dependent í•˜ë‹¤ëŠ” ê²ƒì„ ì´ë¯¸ ì•Œê³  ìˆìŒ. ë‹¤ë¥¸ depth, resolution ì„ ì´ìš©í•˜ì—¬ ì„±ëŠ¥ ë¹„êµ.    image    Observation 2: ì„¸ ìš”ì†Œì˜ balanceê°€ ë§¤ìš° ì¤‘ìš”.. ë‹¤ìŒê³¼ ê°™ì€ compound scaling method ì œì•ˆ.  $$\\phi: \\text{Compound Coefficient} \\\\ depth: d = \\alpha^\\phi \\\\ width: w = \\beta^\\phi \\\\ resolution: r = \\gamma^\\phi \\\\ \\text{s.t. }\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\\\ \\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1$$\n ê° ê°’ì€ small grid searchë¡œ ê²°ì •ëœ ìƒìˆ˜ ê°’.  EfficientNet Architecture   image    EfficientNet-B0 ë¥¼ baseline networkë¡œ í•˜ì—¬ Accuracy, FLOPS ë‘˜ ë‹¤ ìµœì í™”í•˜ë„ë¡ multi-objective neural architecture search ì ìš©.  Step 1  \\(\\phi\\) =1 ë¡œ ê³ ì • ì‹ 2, 3ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ small grid search EfficientNet-B0ì— ê°€ì¥ ì í•©í•œ ê°’ì„ \\(\\alpha=1.2, \\beta=1.1, \\gamma=1.15\\)   Step 2  \\(\\alpha, \\beta, \\gamma\\)ë¥¼ ê³ ì •í•˜ê³  \\(\\phi\\)ë¥¼ ë³€ê²½í•˜ì—¬ ì‹¤í—˜. Result (ImageNet Result for EfficientNet) ì°¸ê³       Result Scaling Up MobileNets and ResNets  Compound scale ì„ ì¦ëª…í•˜ê¸° ìœ„í•´ MobileNetê³¼ ResNetì„ ì´ìš©í•˜ì—¬ ë¹„êµ. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ 3ê°œì¤‘ 1ê°œì˜ ìš”ì†Œë§Œ scaling.    image   ImageNet Result for EfficientNet   Training Setting\n Optimization  RMSProp Decay: 0.9 Momentum: 0.9   Batch normalization  Momentum: 0.99   Weight decay: 1e-5 Initial learning rate: 0.256  Decay: 0.97 (every 2.4 epochs)   Swish Activation AutoAugmentation: ë­”ì§€ ëª¨ë¥´ê² êµ° 1 Stochastic depth: ë­”ì§€ ëª¨ë¥´ê² êµ° 2  Drop connect ratio: 0.2   Dropout  0.2 ~ 0.5 (B0 ~ B7)      B0ë¶€í„° B7 ê¹Œì§€ ì„±ëŠ¥ ë¹„êµ.\n  GPipeì— ë¹„í•´ 8.4ë°° ì ê³  ì¢‹ì€ ì„±ëŠ¥.\n    image    CPUë¥¼ ì´ìš©í•œ Inference ì†ë„ ë¹„êµ.    image    ëª¨ë¸ë³„ FLOPS-Accuracy curve    image   Transfer Learning Result for EfficientNet  ImageNet pretrained model ì„ ì´ìš©í•˜ì—¬ ê°ì¢… Datasetì„ Transfer learning í•œ ì„±ëŠ¥ ë¹„êµ ì‚¬ìš©í•œ Dataset    image    Transfer learning ê²°ê³¼ ì „ì²´ì ìœ¼ë¡œ ëª¨ë¸ì´ ê°€ë²¼ì›€.    image    ê¸°ì¡´ì˜ ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì—¬ ê°€ë³ì§€ë§Œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„.    image   Discussion  EfficientNet-B0 ë¥¼ ì´ìš©í•˜ì—¬ ê°ê¸° ë‹¤ë¥¸ scaling methodë¥¼ ì´ìš©í•˜ì—¬ ì„±ëŠ¥ ë¹„êµ    image     image     image   P.S  ì´ëŸ° ì—°êµ¬ëŠ”..NAS(Network Architecture Search)ê°€ ë‹µ..ì¸ê±´ê°€ ê·¼ë° ì´ê²ƒë„ í•˜ë“œì›¨ì–´ê°€ ë¹µë¹µí•´ì•¼\u0026hellip;.. í¬í¡  ","permalink":"https://jjerry-test.github.io/blog/efficientnet/","tags":["Paper"],"title":"Review: EfficientNet"},{"categories":["DeepLearning"],"contents":"CBAM: Convolutional Block Attention Module Author: Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon\nDate: Jul 17, 2018\nURL: https://arxiv.org/abs/1807.06521\nIntroduction  BAM ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ìµœê·¼ CNN ì„±ëŠ¥ í–¥ìƒì— ì£¼ë¡œ ì—°êµ¬ë˜ëŠ” ìš”ì†ŒëŠ” depth, width, cardinality. ë³¸ ë…¼ë¬¸ì—ì„  Convolutional Block Attention Module(CBAM) ì œì•ˆ. Convolutionì„ ì´ìš©í•˜ì—¬ channel, spatial information ì„ ì¶”ì¶œí•˜ê³  ì„ì–´ì„œ ì‚¬ìš©. channel, spatial attention moduleì€ ê°ê° \u0026ldquo;what\u0026rdquo;, \u0026ldquo;where\u0026quot;ì— ëŒ€í•œ ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŒ.    Convolutional Block Attention Module  CBAM ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒ ì‚¬ì§„ê³¼ ê°™ìŒ.  $$F: \\text{Input feature map} \\\\ F':\\text{Channel attention module feature map} \\\\ F'': \\text{Spatial attention module feature map} \\\\ F' = M_c(F)\\bigotimes F \\\\ F'' = M_s(F')\\bigotimes F'$$\n  Channel attention branch $$M_c(F) = \\sigma(MLP(AvgPool(F)) + MLP(MaxPool(F))) \\\\ = \\sigma(W1(W0(F^c_{avg})) + W1(W0(F^c_{max})))$$\nW0 ì˜ output channel í¬ê¸°: Fì˜ ì±„ë„ ìˆ˜ / reduction ratio(r)\nW1 ì˜ output channel í¬ê¸°: Fì˜ ì±„ë„ ìˆ˜\nSpatial attention branch $$M_s(F)=\\sigma(f^{7\\times7}([AvgPool(F); MaxPool(F)])) \\\\ = \\sigma(f^{7 \\times 7}([F^s_{avg};F^s_{max}]))$$\n 7x7 Convolutionì˜ output channel í¬ê¸°: 1  Arrangement of attention modules  ë‘ branchì˜ ìˆœì„œë¥¼ ì–´ë–»ê²Œ ë°°ì—´í• ì§€ ê³ ë¯¼. ì‹¤í—˜ì ìœ¼ë¡œ Channel â†’ Spatial ë¡œ í•˜ê¸°ë¡œ í•¨.  Ablation study using ImageNet-1K Channel attention  Pooling ê¸°ë²•ë³„ ì„±ëŠ¥ ë¹„êµ    Spatial attention  Pooling, convolution kernel size ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ    Arrangement of the channel and spatial attention  Attention module ìˆœì„œì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ    Result Classification Result on ImageNet-1K       Object Detection on MS COCO and VOC 2007     Network Visualization with Grad-CAM   P.S  BAMê³¼ ë™ì¼í•˜ê²Œ Original Codeê°€ ìˆì§€ë§Œ\u0026hellip;.ë…¼ë¬¸ê³¼ ë‹¤ë¥¸ ë¶€ë¶„ì´ ë§¤ìš° ë§ìŒ.  ","permalink":"https://jjerry-test.github.io/blog/cbam/","tags":["Paper"],"title":"Review: CBAM"},{"categories":["DeepLearning"],"contents":"BAM: Bottleneck Attention Module Author: Jongchan Park, Sanghyun Woo, Joon-Young Lee, In So Kweon\nDate: Jul 17, 2018\nURL: https://arxiv.org/abs/1807.06514\nIntroduction   DLì€ Classification, Detection, Segmentation ë“± ë§ì€ íŒ¨í„´ ì¸ì‹ ë¶„ì•¼ì—ì„œ ê°•ë ¥í•œ Toolë¡œ ì‚¬ìš©.\n  ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•´ì„œ ì¢‹ì€ backboneì„ ì„¤ê³„í•˜ëŠ” ê²ƒì´ ê¸°ë³¸ì ì¸ ì ‘ê·¼ë²•.\n  ì§ê´€ì ì¸ ë°©ë²•ì€ ë” ê¹Šê²Œ ì„¤ê³„í•˜ëŠ” ê²ƒ.\n  VGGNetëŠ” AlexNet ë³´ë‹¤ ë‘ë°° ì´ìƒ.\n  ResNet ì€ VGGNetë³´ë‹¤ 22ë°° ì´ìƒì´ë©´ì„œ residual connections ì‚¬ìš©í•˜ì—¬ gradient flow ë¥¼ í–¥ìƒ.\n  GoogLeNet ì€ ë§¤ìš° ê¹Šê³  ê°™ì€ layerì—ì„œ ë‹¤ì–‘í•œ featureë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ.\n  DenseNet ì´ì „ layerì˜ feature map ë“¤ì„ concatenation í•˜ì—¬ ì‚¬ìš©.\n  WideResNet, PyramidNet layerì˜ channels ë¥¼ ì¦ê°€í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ.\n  ResNeXt, Xceptionê³¼ ê°™ì€ backboneì€ grouped convolutionsì„ ì´ìš©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ.\n  ë³¸ ë…¼ë¬¸ì—ì„  attention ì˜ íš¨ê³¼ë¥¼ ë³´ê¸° ìœ„í•´ ê¸°ì¡´ì˜ architecture ì— ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ê°€ë²¼ìš´ Bottle Attention Module(BAM) ì œì•ˆ\n    Bottleneck Attention Module  BAM ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒ ì‚¬ì§„ê³¼ ê°™ìŒ.  $$F: \\text{Input feature map} \\\\ M(F): \\text{Attention map} \\\\ F' = F + F\\bigotimes M(F) \\\\ M(F) = \\sigma(M_c(F) + M_s(F))$$\n  Channel attention branch $$M_c(F) = BN(MLP(AvgPool(F))) \\\\ = BN(W_1(W_0AvgPool(F) + b_0)+b_1)$$\nW0 ì˜ output channel í¬ê¸°: Fì˜ ì±„ë„ ìˆ˜ / reduction ratio(r)\nW1 ì˜ output channel í¬ê¸°: Fì˜ ì±„ë„ ìˆ˜\nSpatial attention branch $$M_s(F)=BN(f_3^{1\\times1}(f_2^{3\\times3}(f_1^{3\\times3}(f_0^{1\\times1}(F)))))$$\n ëª¨ë“  ì—°ì‚°ì€ convolution ì—°ì‚°. 3x3 Convolution ì—°ì‚° ìˆ˜í–‰ì‹œì—” dilation convolution ì‚¬ìš©. ì²«ë²ˆì§¸~ì„¸ë²ˆì§¸ Convolution ì˜ output channel í¬ê¸°: Fì˜ ì±„ë„ ìˆ˜ / reduction ratio(r) ë§ˆì§€ë§‰ Convolution ì˜ output channel í¬ê¸°: 1  Combine two attention branches $$M(F) = \\sigma(M_c(F) + M_s(F))$$\n Channel attention branch ì¶œë ¥: 1x1xR Spatial attention branch ì¶œë ¥: HxWx1 ë‘ attention branchë¥¼ í•©ì¹˜ëŠ” ë°©ë²•ìœ¼ë¡œ element-wise summation, multiplication, max operation ê³ ë ¤.  Ablation study using CIFAR-100 Dilation value and Reduction ratio  Dilation valueì™€ Reduction ratioì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ Table 1 (a)  Separate or Combined branches \u0026amp; Combining methods  ë‘ attention branch ì‚¬ìš© ë°©ë²•ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ Table 1 (b)  Comparison with placing original convblocks  BAM ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ Table 1 (c)    Bottleneck: The efficient point to place BAM  BAM ì‚¬ìš© ìœ„ì¹˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ.    Result Classification Result on CIFAR-100 and ImageNet-1K   Object Detection on MS COCO and VOC 2007   Comparison with Squeeze-and-Excitation   P.S  Original Codeê°€ ìˆì§€ë§Œ\u0026hellip;.ë…¼ë¬¸ê³¼ ë‹¤ë¥¸ ë¶€ë¶„ì´ ë§¤ìš° ë§ìŒ.  ","permalink":"https://jjerry-test.github.io/blog/bam/","tags":["Paper"],"title":"Review: BAM"},{"categories":["DeepLearning"],"contents":"Searching for MobileNetV3 Author: Andrew G. Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam\nDate: May 06, 2019\nURL: https://arxiv.org/abs/1905.02244\nIntroduction  Efficient neural network ëŠ” low latency, higher accuracy ì™€ ë”ë¶ˆì–´ ì „ë ¥ì†Œëª¨ê°€ ì¤„ì–´ë“¤ê²Œ í•˜ê¸° ë•Œë¬¸ì— ë°°í„°ë¦¬ ìˆ˜ëª… ë³´ì¡´ì—ë„ ê¸°ì—¬. ì´ì— í˜ì…ì–´ ë‹¤ìŒ ì„¸ëŒ€ì˜ ë” íš¨ìœ¨ì ì¸ ë„¤íŠ¸ì›Œí¬ ì œì•ˆ. ë³¸ ë…¼ë¬¸ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ ë„¤ ê°€ì§€.  Complementary search techniques New efficient versions of non-linearities practical New efficient network design New efficient segmentation decoder    Efficient Mobile Building Blocks  MobileNetV2ì˜ Inverted residual structureì— squeeze and excitation ì„ ì¶”ê°€.    Network Search Platform-Aware NAS for Block-wise Search  RNN-based controller, factorized hierarchical search space  NetAdapt for Layer-wise Search  platform-aware NASë¡œ ì°¾ì€ Seed network architecture ë¡œ ì‹œì‘. ë§¤ ìŠ¤í…ë§ˆë‹¤:\n(a) ì´ì „ì˜ proposal ì— ë¹„í•´ latencyê°€ ìµœì†Œ aë§Œí¼ ê°ì†Œëœ ìƒˆë¡œìš´ proposal ìƒì„±.\n(b) ê° proposalì€ ì´ì „ ìŠ¤í…ì˜ pre-trained modelì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œ ì œì•ˆëœ architectureë¥¼ ì±„ìš°ê³  ëˆ„ë½ëœ weightì— ëŒ€í•´ì„  ì ì ˆí•œ ê°’ìœ¼ë¡œ ì±„ì›€. ê° proposal ì€ T step ë™ì•ˆ finetuningí•˜ê³  ëŒ€ëµì ì¸ accuracy ì¶”ì¶œ.\n(c) ëª‡ëª‡ metricì„ ì´ìš©í•˜ì—¬ ìµœì ì˜ proposal ì„ ì„ íƒ. ëª©í‘œë¡œí•˜ëŠ” latencyì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ.  Network Improvements  Networkì˜ ì´ˆë°˜, í›„ë°˜ë¶€ì˜ expansive layer êµ¬ì¡° ìˆ˜ì •. ìƒˆë¡œìš´ non-linearity fuction ì œì•ˆ.  h-swish: swish ì˜ ë³€í˜• ë²„ì „, ë¹ ë¥¸ ê³„ì‚° ì†ë„, ê²½ëŸ‰í™”    Redesigning Expensive Layers   Nonlinearities   sigmoid â†’ h-swish\n$$hard\\text{-}swish(x) = x\\frac{ReLU6(x + 3)}{6}$$\n  h-swish ë¥¼ deeper layerì—ì„œë§Œ ì‚¬ìš©.\n      Large squeeze-and-excite  MnasNet: Platform-Aware Neural Architecture Search for Mobile ì—ì„œ Squeeze-and-Excite(SE) bottleneck í¬ê¸°ë§Œí¼ convolutional bottleneck ë°œìƒ. ë³¸ ë…¼ë¬¸ì—ì„  expansion layerì˜ ì±„ë„ì˜ 1/4ë¡œ ê³ ì •. íŒŒë¼ë¯¸í„° ë¯¸ì•½í•˜ê²Œ ì¦ê°€í•˜ë©´ì„œ ì •í™•ë„ ì¦ê°€.  Result     P.S  Batch sizeë¥¼ 4096ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\u0026hellip;. ì—­ì‹œ í•˜ë“œì›¨ì–´ ê¹¡íŒ¨ êµ¬ê¸€\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/mobilenetv3/","tags":["Paper"],"title":"Review: MobileNet V3"},{"categories":["DeepLearning"],"contents":"MobileNetV2: Inverted Residuals and Linear Bottlenecks Author: Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen\nDate: Jan 13, 2018\nURL: https://arxiv.org/abs/1801.04381\nAbstract  ìƒˆë¡œìš´ mobile architecture! mobileì—ì„œ Object detection, Semantic segmentation ì— ì ìš©í•  ìˆ˜ ìˆìŒ!  Introduction  MobileNetê³¼ ë¹„ìŠ·í•œ ì–˜ê¸° ìƒˆë¡œìš´ ëª¨ë“ˆ ì œì•ˆ! â†’ Inverted residual block  Preliminaries, discussion and intuition  ë…¼ë¬¸ì˜ ê°€ì¥ í° íŠ¹ì§•ì€ Depthwise Separable Convolution, Linear Bottlenecks, Inverted Residuals.  DepthwiseÂ SeparableÂ Convolutions  Xception ì—ì„œë¶€í„° ì œì•ˆëœ Convolution Efficient networkì˜ í•µì‹¬ Block  Linear Bottlenecks  MobileNetV1ì—ì„œ computation ê³¼ accuracyì˜ trade-off ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ width multiplier parameterë¥¼ ì‚¬ìš©. ë³¸ ë…¼ë¬¸ì—ì„  1x1 Convolution ì„ ì´ìš©í•˜ì—¬ dimension reduction ìˆ˜í–‰.  Inverted Residuals  ê¸°ì¡´ì˜ Residual Block ê³¼ ë¹„ìŠ·í•œ êµ¬ì¡° (Bottleneck â†’ Expansion \u0026amp; Skip connection). ë³¸ ë…¼ë¬¸ì—ì„  Expansion â†’ Bottleneck \u0026amp; Skip connection êµ¬ì¡°ì˜ Inverted Residual Block ì‚¬ìš©.      Model Architecture   Ablation study Inverted residual connections  Skip connection ì„ bottleneck í›„ì— í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ë” ì¢‹ìŒ. Figure 6 (b) ì°¸ê³   Importance of linear bottleneck  Bottleneck ì—ì„œëŠ” activation ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ë” ì¢‹ìŒ. Figure 6 (a) ì°¸ê³     ","permalink":"https://jjerry-test.github.io/blog/mobilenetv2/","tags":["Paper"],"title":"Review: MobileNet V2"},{"categories":["Living"],"contents":"After MacOS Install 1. Install applications  Kakao Talk Between Snap  Add Terminal shortcut   Speedtest Magnet MenuBar Stats  Add MenuBar Stats Plugins Manager https://www.seense.com/menubarstats/plugins3/   Buddy for Youtube Bandizip  https://kr.bandisoft.com/bandizip/x/   Notion  https://www.notion.so/desktop    2. Install brew /usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#34; 3. Install oh-my-zsh sh -c \u0026#34;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\u0026#34; 4. Install pyenv Python Installation for mac\n5. Edit .zshrc # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # Path to your oh-my-zsh installation. export ZSH=\u0026#34;/Users/jerry/.oh-my-zsh\u0026#34; # Set name of the theme to load --- if set to \u0026#34;random\u0026#34;, it will # load a random theme each time oh-my-zsh is loaded, in which case, # to know which specific one was loaded, run: echo $RANDOM_THEME # See https://github.com/robbyrussell/oh-my-zsh/wiki/Themes ZSH_THEME=\u0026#34;jreese\u0026#34; # Set list of themes to pick from when loading at random # Setting this variable when ZSH_THEME=random will cause zsh to load # a theme from this variable instead of looking in ~/.oh-my-zsh/themes/ # If set to an empty array, this variable will have no effect. # ZSH_THEME_RANDOM_CANDIDATES=( \u0026#34;robbyrussell\u0026#34; \u0026#34;agnoster\u0026#34; ) # Uncomment the following line to use case-sensitive completion. # CASE_SENSITIVE=\u0026#34;true\u0026#34; # Uncomment the following line to use hyphen-insensitive completion. # Case-sensitive completion must be off. _ and - will be interchangeable. # HYPHEN_INSENSITIVE=\u0026#34;true\u0026#34; # Uncomment the following line to disable bi-weekly auto-update checks. # DISABLE_AUTO_UPDATE=\u0026#34;true\u0026#34; # Uncomment the following line to automatically update without prompting. # DISABLE_UPDATE_PROMPT=\u0026#34;true\u0026#34; # Uncomment the following line to change how often to auto-update (in days). # export UPDATE_ZSH_DAYS=13 # Uncomment the following line if pasting URLs and other text is messed up. # DISABLE_MAGIC_FUNCTIONS=true # Uncomment the following line to disable colors in ls. # DISABLE_LS_COLORS=\u0026#34;true\u0026#34; # Uncomment the following line to disable auto-setting terminal title. # DISABLE_AUTO_TITLE=\u0026#34;true\u0026#34; # Uncomment the following line to enable command auto-correction. # ENABLE_CORRECTION=\u0026#34;true\u0026#34; # Uncomment the following line to display red dots whilst waiting for completion. # COMPLETION_WAITING_DOTS=\u0026#34;true\u0026#34; # Uncomment the following line if you want to disable marking untracked files # under VCS as dirty. This makes repository status check for large repositories # much, much faster. # DISABLE_UNTRACKED_FILES_DIRTY=\u0026#34;true\u0026#34; # Uncomment the following line if you want to change the command execution time # stamp shown in the history command output. # You can set one of the optional three formats: # \u0026#34;mm/dd/yyyy\u0026#34;|\u0026#34;dd.mm.yyyy\u0026#34;|\u0026#34;yyyy-mm-dd\u0026#34; # or set a custom format using the strftime function format specifications, # see \u0026#39;man strftime\u0026#39; for details. # HIST_STAMPS=\u0026#34;mm/dd/yyyy\u0026#34; # Would you like to use another custom folder than $ZSH/custom? # ZSH_CUSTOM=/path/to/new-custom-folder # Which plugins would you like to load? # Standard plugins can be found in ~/.oh-my-zsh/plugins/* # Custom plugins may be added to ~/.oh-my-zsh/custom/plugins/ # Example format: plugins=(rails git textmate ruby lighthouse) # Add wisely, as too many plugins slow down shell startup. plugins=(git) source $ZSH/oh-my-zsh.sh # User configuration # export MANPATH=\u0026#34;/usr/local/man:$MANPATH\u0026#34; # You may need to manually set your language environment # export LANG=en_US.UTF-8 # Preferred editor for local and remote sessions # if [[ -n $SSH_CONNECTION ]]; then # export EDITOR=\u0026#39;vim\u0026#39; # else # export EDITOR=\u0026#39;mvim\u0026#39; # fi # Compilation flags # export ARCHFLAGS=\u0026#34;-arch x86_64\u0026#34; # Set personal aliases, overriding those provided by oh-my-zsh libs, # plugins, and themes. Aliases can be placed here, though oh-my-zsh # users are encouraged to define aliases within the ZSH_CUSTOM folder. # For a full list of active aliases, run `alias`. # # Example aliases # alias zshconfig=\u0026#34;mate ~/.zshrc\u0026#34; # alias ohmyzsh=\u0026#34;mate ~/.oh-my-zsh\u0026#34; # Set pyenv export PYENV_ROOT=\u0026#34;${HOME}/.pyenv\u0026#34; export PATH=\u0026#34;${PYENV_ROOT}bin:$PATH\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; # \u0026gt;\u0026gt;\u0026gt; conda initialize \u0026gt;\u0026gt;\u0026gt; # !! Contents within this block are managed by \u0026#39;conda init\u0026#39; !! __conda_setup=\u0026#34;$(\u0026#39;/Users/jerry/.pyenv/versions/miniconda3-latest/bin/conda\u0026#39; \u0026#39;shell.zsh\u0026#39; \u0026#39;hook\u0026#39; 2\u0026gt; /dev/null)\u0026#34; if [ $? -eq 0 ]; then eval \u0026#34;$__conda_setup\u0026#34; else if [ -f \u0026#34;/Users/jerry/.pyenv/versions/miniconda3-latest/etc/profile.d/conda.sh\u0026#34; ]; then . \u0026#34;/Users/jerry/.pyenv/versions/miniconda3-latest/etc/profile.d/conda.sh\u0026#34; else export PATH=\u0026#34;/Users/jerry/.pyenv/versions/miniconda3-latest/bin:$PATH\u0026#34; fi fi unset __conda_setup # \u0026lt;\u0026lt;\u0026lt; conda initialize \u0026lt;\u0026lt;\u0026lt; 6. Create conda environment #!/bin/bash  conda install -y ipykernel jupyter jupyterlab pylint conda update --all -y conda create -y -n tf python=3.7 conda create -y -n tc python=3.7 source ~/.pyenv/versions/miniconda3-latest/etc/profile.d/conda.sh # Create TensorFlow Environment conda activate tf conda install -y numpy scipy pandas matplotlib pylint conda install -y seaborn pillow scikit-image opencv scikit-learn tqdm ipython ipykernel ipywidgets conda install -y tensorflow conda update --all -y python -m ipykernel install --user --name tf --display-name TensorFlow conda deactivate # Create PyTorch Environment conda activate tc conda install -y numpy scipy pandas matplotlib pylint conda install -y seaborn pillow scikit-image opencv scikit-learn tqdm ipython ipykernel ipywidgets conda install -y -c pytorch pytorch torchvision conda update --all -y python -m ipykernel install --user --name tc --display-name PyTorch conda deactivate 7. Install VSCode  https://code.visualstudio.com/docs/?dv=osx Install plugin  python pylance Rainbow Brackets indent-rainbow Remote - SSH kite  https://kite.com/download/   c/c++ markdown-all-in-one leetcode  brew install node Sign in using github      ","permalink":"https://jjerry-test.github.io/blog/macbook-setup/","tags":["Macbook"],"title":"ê°œì¸ì ì¸ ë§¥ë¶ ì„¸íŒ…ë²•"},{"categories":["Living"],"contents":"Raspberry pi 4 \u0026amp; Transmission Container  1. Docker Install curl -fsSL get.docker.com -o get-docker.sh sudo bash get-docker.sh sudo usermod -aG docker pi ê·¸ë¦¬ê³  ì¬ë¶€íŒ…\n2. Pull Transmission Image docker pull linuxserver/transmission 3. Create Transmission Container docker create \\  --name=transmission \\  -e PUID=1000 \\  -e PGID=1000 \\  -e TZ=Asia/Seoul \\  -e USER={username} \\  -e PASS={password} \\  -p 9091:9091 \\  -p 51413:51413 \\  -p 51413:51413/udp \\  -v {path to data}:/config \\  -v {path to downloads}:/downloads \\  -v {path to watch folder}:/watch \\  --restart unless-stopped \\  linuxserver/transmission 4. Run Container docker start transmission ","permalink":"https://jjerry-test.github.io/blog/rpi_transmission/","tags":["Hardware"],"title":"Raspberry pi 4 ì— Transmission ì„¸íŒ…í•˜ê¸°"},{"categories":["DeepLearning"],"contents":"Raspberry pi 4 \u0026amp; Google Coral USB Accelerator í‰ì†Œì— ë¼ì¦ˆë² ë¦¬íŒŒì´ 4ë¥¼ NASë¡œ ì‚¬ìš©í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ° ëŠë‚Œìœ¼ë¡œ..\ní•˜ì§€ë§Œ\u0026hellip;í•­ìƒ ë§ˆìŒ ì†ì—ëŠ” \u0026ldquo;í \u0026hellip;.ë¼ì¦ˆë² ë¦¬íŒŒì´ë¡œ ë”¥ëŸ¬ë‹ì„ ëŒë ¤ë³´ê³  ì‹¶ë‹¤\u0026hellip;\u0026rdquo; ë¼ëŠ” ìƒê°ì„ í•˜ê³  ìˆì—ˆì£ .\ní‰ì†Œì™€ ê°™ì´ í‰í™”ë¡œìš´ ì¤‘ê³ ë‚˜ë¼ ë¥¼ íƒìƒ‰í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. (ëª¨ë‹ˆí„°ê°€ ì‚¬ê³  ì‹¶ì–´ì„œ \u0026hellip;.)\nê·¼ë° . . ê°‘ìê¸° . . ? ì™œ ì¸ì§€ ëª¨ë¥´ê² ì§€ë§Œ Google Coral USB Accelerator ë¥¼ ê²€ìƒ‰í•˜ê³  ì‹¶ë”êµ°ìš”.\nê·¸ë˜ì„œ ë°”ë¡œ ê²€ìƒ‰ì„ í–ˆê³  7ë§ˆë„Œ(ë‚˜ë¦„ ì €ë ´)ì— ì˜¬ë¼ì™€ìˆê¸¸ë˜ ì¼ìš”ì¼ì— ì£¼ë¬¸ì„ í–ˆìŠµë‹ˆë‹¤.\nê·¸ë¦¬ê³  ì˜¤ëŠ˜ ì§‘ì— ë„ì°©ì„ í–ˆì£ .\nì´ì œ ì‚¬ìš©ì„ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.\nì¤€ë¹„ë¬¼ì„ ì†Œê°œí•˜ë„ë¡ í•˜ì£ .\nì¤€ë¹„ë¬¼  ë¼ì¦ˆë² ë¦¬ íŒŒì´ Coral Accelerator Webcam    Step 1. ì—°ê²°     ëª¨ë‘ ì—°ê²° í›„ ë¼ì¦ˆë² ë¦¬íŒŒì´ë¥¼ ì¼œë©´ ìœ„ ì‚¬ì§„ê³¼ ê°™ì´ USBì— í°ìƒ‰ ë¶ˆì´ ë“¤ì–´ì˜µë‹ˆë‹¤.\nStep 2. ë¼ì¦ˆë² ë¦¬íŒŒì´ ì„¸íŒ… #!/bin/bash echo \u0026quot;deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo apt-get update sudo apt-get install libedgetpu1-std # Check Your Platform # https://www.tensorflow.org/lite/guide/python sudo pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl #!/bin/bash git clone https://github.com/google-coral/examples-camera.git cd examples-camera sh download_models.sh  download_models.shë¥¼ ì‹¤í–‰í•˜ì‹œë©´ ë‹¤ìŒê³¼ ê°™ì´ all_modelsë¼ëŠ” ë””ë ‰í† ë¦¬ ì•ˆì— ê° ë°ì´í„°ì…‹ ë³„ labelmap.txtì™€ í•™ìŠµëœ ëª¨ë¸ì˜ tflite íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤.\nì—¬ê¸°ì„œ ë°‘ì¤„ ì¹œ íŒŒì¼ì„ ì´ìš©í•´ì„œ object detection ì„ í•´ë³¼ê²ë‹ˆë‹¤! ì¼ë‹¨ ê·¸ ë‹¤ìŒ ì„¸íŒ…ìœ¼ë¡œ ë„˜ì–´ê°€ì£ .\n  #!/bin/bash cd opencv sh install_requirements.sh  opencvë¥¼ ì´ìš©í•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê²ƒë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.\nStep 3. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì´ì œ opencv ë””ë ‰í† ë¦¬ ì•ˆì— detect.pyë¥¼ ì‹¤í–‰ ì‹œì¼œì¤„ê±´ë°ìš”. ì˜µì…˜ì´ ëª‡ê°œ ìˆìŠµë‹ˆë‹¤. í•œë²ˆ ì‚´í´ë³´ë„ë¡ í•˜ì£ .\nusage: detect.py [-h] [--model MODEL] [--labels LABELS] [--top_k TOP_K] [--camera_idx CAMERA_IDX] [--threshold THRESHOLD] optional arguments: -h, --help show this help message and exit --model MODEL .tflite model path --labels LABELS label file path --top_k TOP_K number of categories with highest score to display --camera_idx CAMERA_IDX Index of which video source to use. --threshold THRESHOLD classifier score threshold  í•œë²ˆ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.\npython detect.py \\ --model ../all_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite \\ --labels ../all_models/coco_labels.txt \\ --top_k 3 \\ --threshold 0.7    ë§‰ ë¹ ë¥¼ ì¤„ ì•Œì•˜ëŠ”ë° Webcam ì˜ í•œê³„ë¼ ê·¸ëŸ°ì§€\u0026hellip; FPSê°€ ë‚®ë„¤ìš”..\nì¶”í›„ì— ì¹´ë©”ë¼ ëª¨ë“ˆì„ ì´ìš©í•´ì„œ í•´ë´ì•¼ê² ìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ ê°„ë‹¨í•œ ì´ìš©ê¸°ë¥¼ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤.\n","permalink":"https://jjerry-test.github.io/blog/coral/","tags":["Tools"],"title":"Google Coral USB ì‚¬ìš©ê¸°"},{"categories":["DeepLearning"],"contents":"StarGAN v2: Diverse Image Synthessis for Multiple Domains Author: Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha\nDate: Dec 04, 2019\nURL: https://arxiv.org/abs/1912.01865\nAbstract   Image translation ì„ ì˜ í•˜ëŠ” Modelì„ í•™ìŠµí•˜ë ¤ë©´ ë‹¤ìŒ ì‚¬í•­ì„ ë§Œì¡±í•´ì•¼í•¨\n Diversity of generated images Scalability over multiple domains    ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ limited diversity, multiple models(networks)ë¥¼ ë‹¤ë£¸.\n  StarGAN v2ëŠ” ë‘ ì¡°ê±´ ëª¨ë‘ ë§Œì¡±.\n  Introduction  Domain: ì‹œê°ì ìœ¼ë¡œ êµ¬ë³„ë˜ëŠ” ë²”ì£¼ Style: ê° ì˜ìƒì´ ê°€ì§€ëŠ” ë…íŠ¹í•œ ì™¸ê´€ì  íŠ¹ì„±    StarGAN v2 Proposed framework     4ê°œì˜ Network ë¡œ êµ¬ì„±.\n  Generator (G)\n Image xì™€ Style code së¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìƒˆë¡œìš´ ì˜ìƒì„ ìƒì„±. adaptive instance normalization (AdaIN) ì‚¬ìš©.    Mapping network (F)\n Latent code zì™€ Domain code yë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ Style code sìƒì„±. Multi Layer Perceptron êµ¬ì¡°.    Style encoder (E)\n Image xì™€ Domain code yë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ xì—ì„œ Style code së¥¼ ì¶”ì¶œ.    Discriminator (D)\n Image xë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ Domain code yì™€ Real/Fake ë¶„ë¥˜.    Training objectives  Adversarial objective  GAN ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” Loss    $$\\mathcal{L}_{adv}=\\mathbb{E}_{\\mathrm{x},y}[\\log{D_y}(\\mathrm{x})] + \\mathbb{E}_{\\mathrm{x}, \\tilde{y}, \\mathrm{z}}[\\log{(1-D_{\\tilde{y}}(G(\\mathrm{x}, \\tilde{\\mathrm{s}})))}$$\n Style reconstruction  G(x, s) ë¥¼ Style encoder E ì— ë„£ì–´ s ì¶”ì¶œ í›„ ì…ë ¥ sì™€ ë¹„êµ    $$\\mathcal{L}_{sty}=\\mathbb{E}_{\\mathrm{x},\\tilde{y}, \\mathrm{z}}[\\parallel\\tilde{\\mathrm{s}}-E_{\\tilde{y}}(G(\\mathrm{x}, \\tilde{\\mathrm{s}}))\\parallel_1]$$\n Style diversification  Gê°€ ë‹¤ì–‘í•œ Imageë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ Regularization í•˜ëŠ” ì—­í• . z1, z2 ê°€ Fì— ì˜í•´ ìƒì„±ëœ s1, s2ì™€ ì…ë ¥ xë¥¼ Gì˜ ì…ë ¥ìœ¼ë¡œ ìƒˆë¡œìš´ ì˜ìƒ ìƒì„±. L1 Norm ê³„ì‚°.    $$\\mathcal{L}_{ds}=\\mathbb{E}_{\\mathrm{x},\\tilde{y}, \\mathrm{z}_1, \\mathrm{z}_2}[\\parallel G(\\mathrm{x}, \\tilde{\\mathrm{s}}_1) - G(\\mathrm{x}, \\tilde{\\mathrm{s}}_2) \\parallel_1]$$\n Preserving source characteristics  Cycle GAN ì˜ cycle consistency loss. target domainì˜ style ì„ ì ìš©í•œ ì˜ìƒì„ ë‹¤ì‹œ E(x)ë¡œ ì¶”ì¶œëœ së¥¼ ì´ìš©í•˜ì—¬ x'ë¡œ reconstruction í•œ í›„ L1 Norm ê³„ì‚°.    $$\\mathcal{L}_{cyc}=\\mathbb{E}_{\\mathrm{x}, y, \\tilde{y}, \\mathrm{z}}[\\parallel \\mathrm{x} - G(G(\\mathrm{x}, \\tilde{\\mathrm{s}}), \\hat{\\mathrm{s}})\\parallel_1]$$\n  Full objective\n$$\\mathcal{L}_D = -\\mathcal{L}_{adv} \\ \\mathcal{L}_{F, G, E}=\\mathcal{L}_{adv} + \\lambda_{sty} \\mathcal{L}_{sty} - \\lambda_{ds} \\mathcal{L}_{ds} + \\lambda_{cyc} \\mathcal{L}_{cyc}$$\n About \\(\\lambda\\)       Dataset sty ds cyc     CelebA-HQ 1 1 1   AFHQ 0.3 1 0.1    Experiments   Baselines\n MUNIT DRIT MSGAN StarGAN    Datasets\n CelebA-HQ AFHQ    Evaluation metrics\n Frechet inception distance (FID) Learned perceptual image patch similarity (LPIPS)    Analysis of individual components  StarGANì—ì„œ ë³¸ ì—°êµ¬ì—ì„œ ì œì•ˆí•˜ëŠ” ë°©ë²•ë“¤ì„ í•˜ë‚˜í•˜ë‚˜ ë„£ì–´ê°€ë©´ì„œ ì„±ëŠ¥ ì‹¤í—˜. ì •ëŸ‰ì  í‰ê°€ë¥¼ ë³´ë©´ ì¶”ê°€í•  ë•Œë§ˆë‹¤ ì¢‹ì•„ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ.     ê° ë‹¨ê³„ë³„ ìƒì„±í•œ ì˜ìƒì˜ ê²°ê³¼    Comparison on diverse image synthesis  ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ë¹„êµ Latent-guided synthesis ( Latent code ë§Œì„ ì´ìš©í•˜ì—¬ ìƒì„± )       Referenc-guided synthesis ( Style code ë¥¼ ì´ìš©í•œ ìƒì„± )       Human evaluation  ë°©ë²• ë³„ë¡œ 100ê°œì˜ sample ìƒì„± í›„ ì‚¬ëŒì´ íŒë‹¨.      Result  ê²ë‚˜\u0026hellip;.ì˜ ìƒì„±í•¨\u0026hellip;      ","permalink":"https://jjerry-test.github.io/blog/starganv2/","tags":["Paper"],"title":"Review: StarGAN v2"},{"categories":["DeepLearning"],"contents":"StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation Author: Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha\nDate: Dec 21, 2019\nURL: https://arxiv.org/abs/1711.09020\nIntroduction  Multi domain image translation ì´ë¼ê³  í•˜ë©´ ë‹¤ìŒ ì‚¬ì§„ê³¼ ê°™ì´ ë¨¸ë¦¬ ìƒ‰, ì„±ë³„, ì—°ë ¹ëŒ€ ë“±ê³¼ ê°™ì´ ì—¬ëŸ¬ condition ì— ëŒ€ì‘í•˜ëŠ” ì˜ìƒì„ ìƒì„±.     ê¸°ì¡´ì˜ image-to-image translation ì€ nê°œì˜ domain ì„ ì ìš©í•  ì‹œì— n*(n-1) ê°œì˜ generatorê°€ í•„ìš”í–ˆìŒ. StarGANì€ í•œ 1ê°œì˜ generatorë¡œ nê°œì˜ domain ì„ ì ìš©.    Star Generative Adversarial Networks   Loss functions  Adversarial Loss  $$\\mathcal{L}_{adv} = \\mathbb{E}_x [\\log D_{src}(x)] + \\mathbb{E}_{x, c} [\\log (1-D_{src}(G(x,c)))] - \\lambda_{gp}\\mathbb{E}_{\\hat{x}}[(\\Vert \\nabla_{\\hat{x}}D_{src}(\\hat{x})\\Vert_2 -1)^2]$$\n$$\\lambda_{gp} = 10$$\n Domain Classification Loss  $$\\mathcal{L}^r_{cls} = \\mathbb{E}_{x, c'}[-\\log D_{cls}(c'|x)]$$\n$$\\mathcal{L}^f_{cls} = \\mathbb{E}_{x, c'}[-\\log D_{cls}(c|G(x, c)]$$\n Reconstruction Loss  $$\\mathcal{L}_{rec} = \\mathbb{E}_{x, c, c'}[\\Vert x-G(G(x, c), c')\\Vert_1]$$\n Full Objective  $$\\mathcal{L}_D = -\\mathcal{L}_{adv} + \\lambda_{cls}\\mathcal{L}^r_{cls}$$\n$$\\mathcal{L}_G = \\mathcal{L}_{adv} + \\lambda_{cls}\\mathcal{L}^f_{cls}+\\lambda_{rec}\\mathcal{L}_{rec}$$\n$$\\lambda_{cls} = 1, \\lambda_{rec} = 10$$\nResult     Training Detail     ","permalink":"https://jjerry-test.github.io/blog/starganv1/","tags":["Paper"],"title":"Review: StarGAN v1"},{"categories":["DeepLearning"],"contents":"ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„  Weights \u0026amp; Biases ë¼ëŠ” Tool ì„ ì†Œê°œë“œë¦¬ë ¤ í•©ë‹ˆë‹¤.\në‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°•ì¡°í•˜ë„¤ìš”.\n Store hyper-parameters used in a training run Search, compare, and visualize training runs Analyze system usage metrics alongside runs Collaborate with team members Replicate historic results Run parameter sweeps Keep records of experiments available forever  ê·¸ëŸ¼ ê°„ë‹¨ ì‚¬ìš©ë²•ì— ëŒ€í•´ì„œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\nSign Up ì„ í–ˆë‹¤ëŠ” ê°€ì •í•˜ì— ì§„í–‰í•©ë‹ˆë‹¤. íŒ¨í‚¤ì§€ ì„¤ì¹˜ wandb íŒ¨í‚¤ì§€ë¥¼ ë¨¼ì € ì„¤ì¹˜í•´ì¤ë‹ˆë‹¤.\npip install wandb wandb ë¡œê·¸ì¸ ì„¤ì¹˜ í›„ Terminal (or ëª…ë ¹ í”„ë¡¬í”„íŠ¸) ì— ë‹¤ìŒê³¼ ê°™ì´ ì…ë ¥í•©ë‹ˆë‹¤.\nwandb login ê·¸ëŸ¬ë©´ ì•„ë˜ì™€ ê°™ì€ í™”ë©´ì´ ë‚˜ì˜¤ë©´ì„œ ì›¹ ë¸Œë¼ìš°ì €ê°€ ì¼œì§‘ë‹ˆë‹¤.\n    Browser ì—ì„œ APIí‚¤ë¥¼ ë³µì‚¬í•´ì£¼í•´ì„œ Terminal (or ëª…ë ¹ í”„ë¡¬í”„íŠ¸) ì— ë¶™ì—¬ë„£ê¸°í•©ë‹ˆë‹¤.\n  ê·¸ëŸ¼ wandb ì— ë¡œê·¸ì¸ ì™„ë£Œ!\n  Code ì‹¤í–‰ ê°„ë‹¨í•œ MLPë¥¼ ì´ìš©í•œ MNIST Classification ë¬¸ì œì…ë‹ˆë‹¤.\n# wandb initialization import wandb wandb.init(project=\u0026#34;test_project\u0026#34;) import os import numpy as np from matplotlib import pyplot as plt import tensorflow as tf from tensorflow.keras import models, layers, optimizers, losses, utils, datasets # Import callback function from wandb.keras import WandbCallback print(\u0026#34;Packge Loaded!\u0026#34;) # Data Loading (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data() train_x, test_x = np.reshape(train_x/255., [-1, 784]), np.reshape(test_x/255., [-1, 784]) print(\u0026#34;Train Data\u0026#39;s Shape : \u0026#34;, train_x.shape, train_y.shape) print(\u0026#34;Test Data\u0026#39;s Shape : \u0026#34;, test_x.shape, test_y.shape) # Network Building ## Using Sequential mlp = models.Sequential() mlp.add(layers.Dense(256, activation=\u0026#39;relu\u0026#39;, input_shape=(784,))) mlp.add(layers.Dense(128, activation=\u0026#39;relu\u0026#39;)) mlp.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) print(\u0026#34;Network Built!\u0026#34;) mlp.compile(optimizer=optimizers.Adam(), loss=losses.sparse_categorical_crossentropy, metrics=[\u0026#39;accuracy\u0026#39;]) history = mlp.fit(train_x, train_y, epochs=10, batch_size=16, validation_data=(test_x, test_y), callbacks=[WandbCallback()]) # callbacks ì— Wandbcallback ì¶”ê°€ Check training í•´ë‹¹ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œí‚¤ê³  W\u0026amp;B ì— ì ‘ì†ì„ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í™”ë©´ì´ ë‚˜ì˜µë‹ˆë‹¤.\nìœ„ì—ì„œ test_project ë¼ëŠ” í”„ë¡œì íŠ¸ì— ì„¸íŒ…ì„ í–ˆê¸° ë•Œë¬¸ì— í™•ì¸ì„ í•´ë´…ë‹ˆë‹¤.\ní•˜ë‚˜ì˜ í”„ë¡œì íŠ¸ì—ì„œ í•œë²ˆ ì‹¤í–‰í•˜ë©´ ë­”ì§€ ëª¨ë¥¼ ì¡°í•©ì˜ Nameìœ¼ë¡œ ì‹¤í–‰ ìƒíƒœë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì €ëŠ” í•œë²ˆë§Œ í–ˆê¸° ë•Œë¬¸ì— lyric-dream-3 ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ í•˜ë‚˜ë§Œ ìƒê²¨ìˆìŠµë‹ˆë‹¤.\n  lyric-dream-3 ê°™ì´ ì´ë¦„ì„ í´ë¦­í•´ì„œ ë¬´ì—‡ì„ ë³¼ ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\nChart íƒ­ Chart íƒ­ì—ì„  loss, epoch, metric ë“±ê³¼ ê°™ì€ í•™ìŠµ ê´€ë ¨ ì§€í‘œë¥¼ ë³¼ ìˆ˜ ìˆëŠ” CHARTSì™€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ëœ ì‹œìŠ¤í…œì„ ëª¨ë‹ˆí„°ë§ í•  ìˆ˜ ìˆëŠ” SYSTEMì´ ìˆìŠµë‹ˆë‹¤.\n          System íƒ­ System íƒ­ì—ì„œëŠ” Chart íƒ­ì—ì„œ SYSTEM ê³¼ ë™ì¼í•œ ê·¸ë˜í”„ë¥¼ ë³´ì—¬ì£¼ë„¤ìš”.   Model íƒ­ ì´ë¦„ ê·¸ëŒ€ë¡œ í•™ìŠµí•˜ëŠ” Modelì˜ graph êµ¬ì¡°ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   Logs íƒ­ ì—¬ê¸°ì„  í•™ìŠµ ë¡œê·¸ë“¤ì´ ì €ì¥ë©ë‹ˆë‹¤.   Files íƒ­ ì†”ì§íˆ ë³´ì—¬ì£¼ê¸°ë§Œ í•´ë„ ì¢‹ì§€ë§Œ ì €ì¥ë„ í•´ì£¼ë©´ ì¢‹ê² ì£ .\nFiles íƒ­ì—ëŠ” í˜„ì¬ ëª¨ë¸ì„ ëŒë¦´ë•Œ ì‚¬ìš©ëœ ì •ë³´, ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ì„ epoch ì˜ model, ëŒë¦¬ëŠ” í™˜ê²½ì— ì„¤ì¹˜ëœ package ë¦¬ìŠ¤íŠ¸ ë“±ì˜ ì •ë³´ê°€ ì €ì¥ë©ë‹ˆë‹¤.\në¬¼ë¡  ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥!\nmodel-best.h5ëŠ” graph ì •ë³´ë„ ì €ì¥ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë‹¤ìš´ë¡œë“œ í›„ loadí•´ì„œ ë°”ë¡œ Inference ê°€ëŠ¥í•©ë‹ˆë‹¤!   Weights \u0026amp; Biases ì— ëŒ€í•´ ì •~~~ë§ ê°„ë‹¨íˆ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤.\në”±íˆ ì´ëŸ°ì €ëŸ° ê¸°ëŠ¥ì€ ì¶”ê°€ë¥¼ í•˜ì§€ ì•Šê³  ê¸°ë³¸ì ì¸ ê¸°ëŠ¥ë§Œ ë´¤ìŠµë‹ˆë‹¤.\nì¶”í›„ì—ëŠ” ì¢€ ë” ë‹¤ì–‘í•œ ê¸°ëŠ¥ê³¼ í”„ë ˆì„ì›Œí¬ì— ì ìš©í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n 200401 PyTorch ì˜ˆì œ ì¶”ê°€ # wandb initialization import wandb wandb.init(project=\u0026#34;test_project\u0026#34;) # Importing Modules import numpy as np from matplotlib import pyplot as plt import torch from torch import nn, optim import torch.nn.functional as F from torchvision import utils, datasets, transforms # Loading Data # MNIST dataset mnist_train = datasets.MNIST(root=\u0026#39;./\u0026#39;, train=True, transform=transforms.ToTensor(), download=True) print(\u0026#34;Downloading Train Data Done ! \u0026#34;) mnist_test = datasets.MNIST(root=\u0026#39;./\u0026#39;, train=False, transform=transforms.ToTensor(), download=True) print(\u0026#34;Downloading Test Data Done ! \u0026#34;) # Defining Model # our model class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.linear1 = nn.Linear(784, 256) self.linear2 = nn.Linear(256, 10) def forward(self, X): X = F.relu((self.linear1(X))) X = self.linear2(X) return X model = Model() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Logging model, Gradient, parameters on dashboard wandb.watch(model) # Training Phase batch_size = 100 data_iter = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True, num_workers=1) print(\u0026#34;Iteration maker Done !\u0026#34;) # Training loop for epoch in range(10): avg_loss = 0 total_batch = len(mnist_train) // batch_size for i, (batch_img, batch_lab) in enumerate(data_iter): X = batch_img.view(-1, 28*28) Y = batch_lab y_pred = model.forward(X) loss = criterion(y_pred, Y) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() avg_loss += loss if (i+1)%100 == 0 : print(\u0026#34;Epoch : \u0026#34;, epoch+1, \u0026#34;Iteration : \u0026#34;, i+1, \u0026#34; Loss : \u0026#34;, avg_loss.data.numpy()/(i+1)) # Logging metrics  wandb.log({\u0026#39;epoch\u0026#39;: epoch+1, \u0026#34;loss\u0026#34;: avg_loss.data.numpy()/(i+1)}) print(\u0026#34;Epoch : \u0026#34;, epoch+1, \u0026#34; Loss : \u0026#34;, avg_loss.data.numpy()/total_batch) print(\u0026#34;Training Done !\u0026#34;) # Save final model torch.save(model.state_dict(), \u0026#34;model.h5\u0026#34;) wandb.save(\u0026#39;model.h5\u0026#39;) # Evaluation test_img = mnist_test.data.view(-1, 28*28).type(torch.FloatTensor) test_lab = mnist_test.targets outputs = model.forward(test_img) pred_val, pred_idx = torch.max(outputs.data, 1) correct = (pred_idx == test_lab).sum() print(\u0026#39;Accuracy : \u0026#39;, correct.data.numpy()/len(test_img)*100) # Testing r = np.random.randint(0, len(mnist_test)-1) X_single_data = mnist_test.data[r:r + 1].view(-1,28*28).float() Y_single_data = mnist_test.targets[r:r + 1] single_prediction = model(X_single_data) plt.imshow(X_single_data.data.view(28,28).numpy(), cmap=\u0026#39;gray\u0026#39;) print(\u0026#39;Label : \u0026#39;, Y_single_data.data.numpy()[0]) print(\u0026#39;Prediction : \u0026#39;, torch.max(single_prediction.data, 1)[1].numpy()[0]) ","permalink":"https://jjerry-test.github.io/blog/wandb/","tags":["Tools"],"title":"Weights \u0026 Biases ê°€ ë­ì§€??"},{"categories":["DeepLearning"],"contents":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications Author: Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam\nDate: Apr 17, 2017\nURL: https://arxiv.org/abs/1704.04861\nAbstract -Â ëª¨ë°”ì¼,Â ì„ë² ë””ë“œì—Â ì ìš©í• Â ìˆ˜Â ìˆëŠ”Â ë„¤íŠ¸ì›Œí¬.\n  1. Introduction  CNNÂ ìœ ëª…ì„¸Â ì§±ì§±ë§¨ ì¢‹ì€Â ì„±ëŠ¥(ì •í™•ë„)ë¥¼Â ìœ„í•´Â ì—°êµ¬ê°€Â ê³„ì†Â ì§„í–‰ë˜ì—ˆì§€ë§ŒÂ ì†ë„ì™€Â ë©”ëª¨ë¦¬Â ì¸¡ë©´ì—ì„œÂ ë¹„íš¨ìœ¨ì . ê²½ëŸ‰í™”,Â íš¨ìœ¨í™”Â í•„ìš”!!  2. Prior Work  ìµœê·¼ ê²½ëŸ‰í™”, íš¨ìœ¨ì ì¸ ë„¤íŠ¸ì›Œí¬ ê°œë°œì— ì´ˆì . Depthwse separable convolution,  3. MobileNetÂ Architecture  í•µì‹¬ì ì¸ ë ˆì´ì–´ ì„¤ëª….  3.1 DepthwiseÂ SeparableÂ Convolution  Depthwise Convolution ìˆ˜í–‰ í›„ Pointwise Convolution ìˆ˜í–‰.       MobileNet ì—ì„œëŠ” 3 x 3 ì˜ Depthwise saparable convolution ì‚¬ìš©. ì—°ì‚°ëŸ‰ì´ ì•½ 8~9 ë°°ë¡œ í¬ê²Œ ê°ì†Œ.  3.2 NetworkÂ StructureÂ andÂ Training  ì´ 28ê°œì˜ ë ˆì´ì–´     MobileNetì˜ Convolution Block êµ¬ì¡°     MobileNetì˜ ëŒ€ë¶€ë¶„ íŒŒë¼ë¯¸í„°, ì—°ì‚° ì‹œê°„ì€ 1x1 Conv     RMSprop, asynchronous gradient descent ì‚¬ìš©. Tensorflow ë¡œ êµ¬í˜„.  3.3 WidthÂ Multiplier:Â ThinnerÂ Models  MobileNetì˜ ê¸°ë³¸ êµ¬ì¡°ëŠ” ì‘ê³  ë¹ ë¥´ì§€ë§Œ ë” ì‘ê³  ë¹ ë¥¸ ëª¨ë¸ì„ í•„ìš”ë¡œ í•¨. Width multiplier ë¼ëŠ” íŒŒë¼ë¯¸í„° ì œì•ˆ.  3.4 ResolutionÂ Multiplier:Â ReducedÂ Representation   4. Experiments  íš¨ìœ¨ì ì´ë‹¤ ë¼ëŠ” ë‚´ìš©ì´ ì „ë¶€..  4.1 ModelÂ Choices     4.2 ModelÂ ShrinkingÂ Hyperparemeters             4.3 FineÂ GrainedÂ Recognition   4.4 LargeÂ ScaleÂ Geolocalization   4.5 FaceÂ Attributed     4.6 ObjectÂ Detection     4.7 FaceÂ Embeddings   5. Conclusion  ì—°ì‚°ëŸ‰ì´ ì ê³  íŒŒë¼ë¯¸í„° ìˆ˜ë„ ì ìŒ! ë‹¤ì–‘í•œ task ì— ì‚¬ìš©ê°€ëŠ¥!  ","permalink":"https://jjerry-test.github.io/blog/mobilenetv1/","tags":["Paper"],"title":"Review: MobileNet V1"},{"categories":["DeepLearning"],"contents":"Xception: Deep Learning with Depthwise Separable Convolution Author: FrancÂ¸ois Chollet\nDate: Dec 19, 2016\nURL: https://arxiv.org/abs/1610.02357\n Abstract  CNNÂ ì—ì„œÂ ë§ì´Â ì‚¬ìš©ë˜ëŠ”Â InceptionÂ ì€Â RegularÂ ConvolutionÂ ê³¼Â DepthwiseÂ SeparableÂ ConvolutionÂ ì˜Â ì¤‘ê°„Â ë‹¨ê³„. DepthwiseÂ SeparableÂ ConvolutionÂ ì€Â ìµœëŒ€Â ê°œìˆ˜ì˜Â íƒ€ì›Œë¥¼Â ê°€ì§„Â InceptionÂ moduleÂ (?) InceptionÂ moduleì„Â DepthwiseÂ SeparableÂ ConvolutionÂ ìœ¼ë¡œÂ ëŒ€ì²´í•œÂ ìƒˆë¡œìš´Â DeepÂ ConvolutionalÂ NeuralÂ NetworkÂ ì œì•ˆ. XceptionÂ ì´ë¼ê³ Â ì¹­í•¨. ImageNetÂ ìœ¼ë¡œÂ í•™ìŠµëœÂ InceptionÂ V3Â ë³´ë‹¤Â ì‚´ì§Â ì„±ëŠ¥ì´Â ì¢‹ê³ Â 350,000,000ê°œì˜Â ì´ë¯¸ì§€ì™€Â 17,000ê°œì˜Â í´ë˜ìŠ¤ë¡œÂ êµ¬ì„±ëœÂ LargerÂ imageÂ datasetÂ ì—ì„ Â InceptionÂ V3Â ë³´ë‹¤Â ì›”ë“±íˆÂ ë›°ì–´ë‚œÂ ì„±ëŠ¥ì„Â ë³´ì„. InceptionÂ V3ì™€Â XceptionÂ ì´Â ë™ì¼í•œÂ parameterÂ ìˆ˜ë¥¼Â ê°€ì§. ê·¸ëŸ¬ë¯€ë¡œÂ ì—°ì‚°ëŸ‰,Â ë©”ëª¨ë¦¬ì˜Â ì¦ê°€ê°€Â ì•„ë‹ŒÂ ëª¨ë¸Â parameterë¥¼Â íš¨ê³¼ì ìœ¼ë¡œÂ ì‚¬ìš©í•´ì„œÂ ì„±ëŠ¥Â í–¥ìƒì´Â ëœÂ ê²ƒ.  1.Â Introduction   ConvolutionalÂ NeuralÂ NetworkÂ (ì´í•˜Â CNN)Â ì€Â ComputerÂ VisionÂ ì—ì„œÂ ê°€ì¥Â ì£¼ìš”í•œÂ ì•Œê³ ë¦¬ì¦˜ì´Â ë˜ì—ˆê³ ,Â ì´ë¥¼Â ì„¤ê³„í•˜ëŠ”Â ë°©ë²•ì—Â ëŒ€í•´Â ê°œë°œí•˜ëŠ”ë°Â ë§ì€Â ê´€ì‹¬ì„Â ê°€ì§€ê²ŒÂ ë¨.\n  LenetÂ -\u0026gt;Â AlexNetÂ (2012)Â -\u0026gt;Â ZFNetÂ (2013)Â -\u0026gt;Â VGGÂ (2014)Â -\u0026gt;Â InceptionÂ ì¢…ë¥˜Â \u0026hellip;Â -\u0026gt;Â Inception-ResNetÂ (2015)\n  InceptionÂ ìŠ¤íƒ€ì¼ì˜Â ê¸°ë³¸Â êµ¬ì„±Â ìš”ì†ŒëŠ”Â InceptionÂ module.\n  FigureÂ 1ì€Â InceptionÂ V3Â ì˜Â í‘œì¤€Â InceptionÂ Module.\n  InceptionÂ ëª¨ë¸ì€Â ìœ„ì™€Â ê°™ì€Â ëª¨ë“ˆì„Â StackÂ í•œÂ ê²ƒ.Â VGG-StyleÂ ë„¤íŠ¸ì›Œí¬ëŠ”Â ë‹¨ìˆœíˆÂ ConvolutionÂ layerë¥¼Â Stack.\n  ì‹¤í—˜ì ìœ¼ë¡œÂ Inception-styleÂ ì´Â VGG-styleë³´ë‹¤Â ì ì€Â parameterë¡œÂ ë‹¤ì–‘í•œ,Â ë§ì€Â featureë¥¼Â í•™ìŠµÂ í• Â ìˆ˜Â ìˆë‹¤ëŠ”Â ê²ƒì„Â ë³´ì„.\n  1.1Â TheÂ InceptionÂ hypothesis   ConvolutionÂ layerëŠ”Â 3ì°¨ì›Â ê³µê°„ì—ì„œÂ filterë¥¼Â í•™ìŠµí•˜ë ¤ê³ Â í•¨.\n  SingleÂ ConvolutionÂ kernelÂ ì€Â ì±„ë„ì˜Â correlationê³¼Â ê³µê°„ì˜Â correlationÂ ì„Â ë™ì‹œì—Â mappingÂ í•¨.\n  InceptionÂ ëª¨ë“ˆì€Â ì±„ë„,Â ê³µê°„ì˜Â correlationÂ ì„Â ë…ë¦½ì ìœ¼ë¡œÂ ë‚˜íƒ€ë‚¼Â ìˆ˜Â ìˆë„ë¡Â ì—°ì‚°ì„Â ë¶„í•´.Â -\u0026gt;Â ì‰½ê³ Â íš¨ìœ¨ì ì¸Â í”„ë¡œì„¸ìŠ¤ë¥¼Â ë§Œë“¦.\n  Inceptionì˜Â ê°€ì„¤ì€Â ì±„ë„Â ì±„ë„,Â ê³µê°„ì˜Â correlationÂ ì´Â ë¶„ë¦¬ë˜ì–´Â ìˆìœ¼ë¯€ë¡œÂ ë™ì‹œì—Â ë§¤í•‘í•˜ëŠ”Â ê²ƒì€Â ì¢‹ì§€Â ì•Šë‹¤ëŠ”Â ê²ƒ.\n  FigureÂ 2ëŠ”Â 3x3Â convì™€Â 1x1Â convë§ŒÂ ì‚¬ìš©í•œÂ ë‹¨ìˆœí™”í•œÂ InceptionÂ ëª¨ë“ˆ.\n  FigureÂ 3ì€Â FigureÂ 2ì˜Â InceptionÂ ëª¨ë“ˆì—ì„œÂ í•˜ë‚˜ì˜Â í°Â 1x1Â Convolutionê³¼Â 3x3Â Convolutionë“¤ë¡œÂ ì¬êµ¬ì„±í•œÂ ê²ƒ.\n  ì´Â ë°©ë²•ì´Â InceptionÂ ì˜Â ê°€ì„¤ë³´ë‹¤Â ë›°ì–´ë‚œÂ ê°€ì„¤ì„Â ë§Œë“œëŠ”Â ê²ƒì´Â í•©ë¦¬ì ì¸Â ê²ƒì¼ì§€,Â ì±„ë„ê³¼Â ê³µê°„ì„Â ë…ë¦½ì ìœ¼ë¡œÂ ë§¤í•‘í• Â ìˆ˜Â ìˆëŠ”ì§€Â ì˜ë¬¸.\n  1.2Â TheÂ continuumÂ betweenÂ convolutionsÂ andÂ separableÂ convolutions   FigureÂ 4Â ì²˜ëŸ¼Â InceptionÂ ëª¨ë“ˆÂ êµ¬ì„±.\n  1x1Â ConvolutionÂ ì ìš©í•˜ì—¬Â ì±„ë„ì˜Â correlationÂ ë§¤í•‘,Â ê·¸Â í›„Â ê°ê°ì˜Â channelë³„ë¡œÂ ê³µê°„ì˜Â correlationÂ ë§¤í•‘\n  ì´ë¥¼Â AnÂ â€œextremeâ€Â versionÂ ofÂ anÂ InceptionÂ moduleÂ ì´ë¼ê³ Â ì¹­í•¨.\n  TensorFlowÂ í”„ë ˆì„ì›Œí¬ì—Â DepthwiseÂ SeparableÂ ConvolutionÂ ì—°ì‚°ê³¼Â ê±°ì˜Â ë™ì¼í•¨.\n  TensorFlowë‚˜Â KerasÂ í”„ë ˆì„ì›Œí¬ì—Â ìˆëŠ”Â DepthwiseÂ SeparableÂ ConvolutionÂ (SeparableÂ ConvolutionÂ ë¼ê³ ë„Â ë¶ˆë¦¼.)Â ì€Â ê°Â channelÂ ë³„ë¡œÂ 3x3Â ConvolutionÂ ì ìš©Â í›„Â ì±„ë„ê°„ì˜Â 1x1Â ConvolutionÂ ì ìš©.\n  ì˜ìƒì²˜ë¦¬Â ë¶„ì•¼ì—ì„œÂ ì‚¬ìš©í•˜ëŠ”Â SeparableÂ ConvolutionÂ ê³¼Â í˜¼ë™í•˜ë©´Â ì•ˆë¨,Â ì´Â ì—°ì‚°ì€Â ê³µê°„ì Â ë¶„ë¦¬ë¥¼Â í•˜ëŠ”Â Convolution.\n     ë¹„êµ Extream Depthwise Separable     ì—°ì‚°ìˆœì„œ pointwise\u0026ndash;\u0026gt;channelwise channelwise\u0026ndash;\u0026gt;pointwise   ë¹„ì„ í˜•ì„± Presence Absence    2.Â PriorÂ work  VGG-16Â ê³¼Â ê°™ì€Â êµ¬ì¡°ê°€Â xceptionÂ ê³¼Â ìœ ì‚¬. InceptionÂ êµ¬ì¡°ëŠ”Â ê°€ì§€ì¹˜ê¸°ì˜Â ì´ì ì„Â ë³´ì—¬ì¤Œ. DepthwiseÂ separableÂ convolutionëŠ”Â ê²½ëŸ‰í™”ì—ë„Â ì í•©. TensorFlowì—ëŠ”Â ì´ë¯¸Â êµ¬í˜„ë˜ì–´ìˆìŒ. ResidualÂ connectionÂ ì„Â ê´‘ë²”ìœ„í•˜ê²ŒÂ ì‚¬ìš©.  3.Â TheÂ XceptionÂ architecture    FigureÂ 5Â ì™€Â ê°™ì€Â êµ¬ì¡°Â ì œì•ˆ. ì²˜ìŒê³¼Â ë§ˆì§€ë§‰ì„Â ì œì™¸í•˜ê³¤Â linearÂ residualÂ moduleÂ ì‚¬ìš©. ì´Â 36ê°œì˜Â convolutionÂ layerë¡œÂ êµ¬ì„±. ë§¤ìš°Â ë‹¨ìˆœí•œÂ êµ¬ì¡°.  4.Â ExperimentalÂ evaluation  Xceptionê³¼Â InceotionÂ V3Â ë¹„êµ. Parametersê°€Â ë¹„ìŠ·.Â ë„¤íŠ¸ì›Œí¬Â ê·œëª¨ì—Â ëŒ€í•œÂ ì°¨ì´ë¥¼Â ì—†ì• ê¸°Â ìœ„í•¨. ImageNetê³¼Â JFTÂ datasetÂ ì´ìš©.  4.1Â TheÂ JFTÂ dataset  ê·¸ëƒ¥\u0026hellip;JFTÂ ë°ì´í„°Â ì„¤ëª…\u0026hellip; GoogleÂ ë°ì´í„°Â ì¤‘Â í•˜ë‚˜ JFTÂ ë¡œÂ í•™ìŠµ,Â FastEval14kÂ datasetìœ¼ë¡œÂ ì„±ëŠ¥Â ë¹„êµ.  4.2Â OptimizationÂ configuration  ê°Â ë°©ë²•ì—Â ëŒ€í•´ì„œÂ ë‹¤ìŒê³¼Â ê°™ì€Â ì„¤ì •ìœ¼ë¡œÂ Xception,Â InceptionÂ V3Â ëª¨ë‘Â í•™ìŠµ OnÂ ImageNet  Optimizer:Â SGD Momentum:Â 0.9 InitialÂ learningÂ rate:Â 0.045 LearningÂ rateÂ decay:Â 0.94Â (everyÂ 2Â epochs)   OnÂ JFT  Optimizer:Â RMSprop Momentum:Â 0.9 InitialÂ learningÂ rate:Â 0.001 LearningÂ rateÂ decay:Â 0.9Â (everyÂ 3,000,000Â samples)    4.3Â RegularizationÂ configuration  WeightÂ decay Dropout AuxiliaryÂ lossÂ tower  4.4Â TrainingÂ infrastructure  80ê°œì˜Â NVIDIAÂ K80Â GPUÂ \u0026hellip;. ImageNetÂ í•™ìŠµì‹œÂ synchronousÂ gradientÂ descentì„Â ì ìš©í•˜ì—¬Â dataÂ parallelismÂ ì´ìš©.Â â†’Â 3ì¼Â ì†Œìš” JFTÂ í•™ìŠµì‹œÂ asynchronousÂ gradientÂ descentì„Â ì ìš©í•˜ì—¬Â dataÂ parallelismÂ ì´ìš©.Â â†’í•œë‹¬Â ì†Œìš”  4.5Â ComparisonÂ withÂ InceptionÂ V3 4.5.1Â ClassificationÂ performance   ë‘Â ë°ì´í„°Â ëª¨ë‘Â Xceptionì´Â ì¢‹ì€Â ì„±ëŠ¥ì„Â ë³´ì„.\n  4.5.2Â SizeÂ andÂ speed   Parameterê°€Â ëŠ˜ì§€Â ì•Šìœ¼ë©´ì„œÂ ì„±ëŠ¥Â í–¥ìƒì„Â ë³´ì´ê¸°ì—Â Xceptionì´Â íš¨ìœ¨ì ì¸Â ëª¨ë¸.\n  4.6Â EffectÂ ofÂ theÂ residualÂ connections  ResidualÂ connectionì—Â ëŒ€í•œÂ ablationÂ studyÂ ì§„í–‰. ResidualÂ connectionì˜Â ì¤‘ìš”ì„±ì„Â ë³´ì—¬ì¤Œ.    4.7Â EffectÂ ofÂ anÂ intermediateÂ activationÂ afterÂ pointÂ wiseÂ convolutions   DepthwiseÂ separableÂ convolutionì€Â depthwiseÂ â†’Â pointwiseÂ convolutionìœ¼ë¡œÂ êµ¬ì„±ë˜ì–´ìˆìŒ.\n  ê·¸Â ì¤‘ê°„ì—Â activationÂ functionì—Â ëŒ€í•œÂ ablationÂ studyÂ ì§„í–‰.\n  InceptionÂ moduleì—Â ëŒ€í•œÂ ì—°êµ¬ì™€Â ë°˜ëŒ€ë˜ëŠ”Â ê²°ê³¼Â ë„ì¶œ.\n  5.Â FutureÂ directions  DepthwiseÂ separbleÂ convolutionÂ ì´Â ë§ŒëŠ¥ì´ë¼ëŠ”Â ë³´ì¥ì€Â ì—†ìŒ.  ","permalink":"https://jjerry-test.github.io/blog/xception/","tags":["Paper"],"title":"Review: Xception"},{"categories":["Python"],"contents":"íŒŒì´ì¬, ë”¥ëŸ¬ë‹ì„ í•˜ì‹œëŠ” ë¶„ë“¤ ì¤‘ Jupyter ë¶€ë¥˜ë¥¼ ì´ìš©í•˜ì‹œëŠ” ë¶„ë“¤ì´ ê½¤ ë§ìŠµë‹ˆë‹¤. Jupyter Notebook, Jupyter Lab,...\nì €ëŠ” ë‘˜ ë‹¤ ì‚¬ìš©í•´ë³´ê¸´ í•˜ì§€ë§Œ ì£¼ë¡œ Jupyter Lab ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\nì´ìœ ëŠ” ê·¸ëƒ¥\u0026hellip;ì¢€ ë” ë³´ê¸° í¸í•´ì„œ..?\në‘ í™˜ê²½ì˜ ì°¨ì´ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.\n Jupyter Notebook Jupyter Lab  ì°¸ê³ ì‚¬í•­ https://github.com/jupyterlab/jupyterlab/issues/7122\në©°ì¹  ì „ ì´ìŠˆì— ì˜¬ë¼ì˜¨ ìƒí™©ì…ë‹ˆë‹¤.\njupyter lab 1.0.2 ë²„ì „ì— extensionì„ ì„¤ì¹˜í•˜ë©´ sidebar ë¶€ë¶„ì´ ê¹¨ì§€ëŠ” ë²„ê·¸ê°€ ìˆë„¤ìš”\u0026hellip;\n1.1.1 ì—ì„  ê³ ì³ì¡Œë‹¤ê³  í•©ë‹ˆë‹¤.\nì, ë‹¤ì‹œ ë³¸ë¡ ìœ¼ë¡œ ëŒì•„ê°€ë´…ë‹ˆë‹¤.\nJupyter Lab ì—ì„œ ë‹¨ì¶•í‚¤ë¡œ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\u0026lsquo;notebook:create-new\u0026rsquo;\n\u0026lsquo;notebook:interrupt-kernel\u0026rsquo;\n\u0026lsquo;notebook:restart-kernel\u0026rsquo;\n\u0026lsquo;notebook:restart-clear-output\u0026rsquo;\n\u0026lsquo;notebook:restart-run-all\u0026rsquo;\n\u0026lsquo;notebook:reconnect-to-kernel\u0026rsquo;\n\u0026lsquo;notebook:change-kernel\u0026rsquo;\n\u0026hellip; (ë„ˆë¬´ ë§ë‹¤\u0026hellip;.)\n\u0026lsquo;notebook:hide-all-cell-outputs\u0026rsquo;\n\u0026lsquo;notebook:show-all-cell-outputs\u0026rsquo;\n\u0026lsquo;notebook:enable-output-scrolling\u0026rsquo;\n\u0026lsquo;notebook:disable-output-scrolling\u0026rsquo;\n\u0026lsquo;notebook:save-with-view\u0026rsquo;\në„ˆë¬´ ë§ì€ ê´€ê³„ë¡œ ë§í¬ ì°¸ê³ í•´ì£¼ì„¸ìš”.\nì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„  ì˜ˆì‹œë¡œ Restart and Clear ì— ëŒ€í•´ì„œ shortcutì„ ì„¤ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.\nì„¤ì • ë°©ë²• ë¨¼ì € Jupyter Lab ì„ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.\n  Advanced Setting Editor ë¥¼ í´ë¦­í•´ì£¼ì„¸ìš”.\n  ì¢Œì¸¡ì— Keyboard Shortuts ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\n  ì—¬ê¸°ì„œ ì™¼ìª½ì˜ ì°½ì€ default settingì— ëŒ€í•œ ì„¤ëª…ì´ê³  ì˜¤ë¥¸ìª½ì„ Custom ì„ ìœ„í•´ ê¸°ì…í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.\nê·¸ëŸ¼ ì˜ˆì‹œëŒ€ë¡œ Restart and Clear ì— ëŒ€í•´ì„œ shortcutì„ ì„¤ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.\nìœ„ì— ê¸°ëŠ¥ì— ëŒ€í•´ì„œ ë§í¬ ì˜¬ë ¤ë“œë ¸ì—ˆì£ ? ê·¸ ê¹ƒí—™ì—ì„œ ë§í¬ì— ëŒ€í•œ ë‚´ìš©ì„ ë¨¼ì € ì°¾ì•„ë´…ë‹ˆë‹¤.\n  ê±°ê¸°ì„œ \u0026lsquo;{ê¸°ëŠ¥ ì´ë¦„}\u0026rsquo; ë¶€ë¶„ì„ ë³µì‚¬í•´ì£¼ì„¸ìš”.\nê·¸ë¦¬ê³  ë‹¤ìŒê³¼ ê°™ì´ ê¸°ì…í•´ì¤ë‹ˆë‹¤.\n  ê·¸ë¦¬ê³  ìš°ì¸¡ ìƒë‹¨ì— ì €ì¥ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ë…¸íŠ¸ë¶ì—ì„œ Shift + Command + C ë¥¼ ëˆŒë €ì„ë•Œ Restart and Clear ì´ ì‘ë™í•˜ê²Œ ë©ë‹ˆë‹¤.\nì‚¬ì§„ë§Œ ë³´ê³  ë”°ë¼ ì ìœ¼ì‹œê¸° ê·€ì°®ìœ¼ì‹¤í…Œë‹ˆ code block ìœ¼ë¡œ ë‚¨ê¹ë‹ˆë‹¤.\n\u0026quot;shortcuts\u0026quot;: [ { \u0026quot;command\u0026quot;: \u0026quot;kernelmenu:restart-and-clear\u0026quot;, \u0026quot;keys\u0026quot;: [\u0026quot;Shift + Ctrl + C\u0026quot;], \u0026quot;selector\u0026quot;: \u0026quot;[data-jp-kernel-user]:focus\u0026quot; }] ê³¼ì •ì„ ìš”ì•½í•´ë“œë¦¬ë©´ (Jupyter Lab ì¼œê¸° ~ Editor ì—´ê¸° ê¹Œì§€ëŠ” ìƒëµ)\n ê¸°ëŠ¥ì— ëŒ€í•œ ë§í¬ì—ì„œ ìì‹ ì´ ì›í•˜ëŠ” ê¸°ëŠ¥ ì°¾ê¸° ìœ„ì— code block ì—ì„œ \u0026ldquo;command\u0026rdquo;: ë¶€ë¶„ì— \u0026lsquo;{ê¸°ëŠ¥ ì´ë¦„}\u0026rsquo; ê¸°ì… \u0026ldquo;keys\u0026rdquo;: ì— ì›í•˜ëŠ” ì»¤ë§¨ë“œ ë„£ê¸° (ë³µìˆ˜ê°œ ê°€ëŠ¥) \u0026ldquo;selector\u0026rdquo;: ì— .jp-Notebook:focus\u0026quot; í˜¹ì€ \u0026ldquo;[data-jp-kernel-user]:focus\u0026rdquo; ê¸°ì… ì €ì¥ í›„ ì¦ê²ê²Œ ì‚¬ìš©!  shortcut ì„¤ì •ì€ ì—¬ê¸°ê¹Œì§€ ì…ë‹ˆë‹¤.\ní˜¹ì‹œ ì´ì™¸ì— ë‹¤ë¥¸ ì„¤ì •ì— ëŒ€í•´ ê¶ê¸ˆí•œê±° ìˆìœ¼ì‹œë©´ comment ë‚¨ê²¨ì£¼ì„¸ìš”!\nì°¸ê³ ì‚¬í•­  ë§Œì•½ A ê¸°ëŠ¥ì€ Shift + Alt + C ì´ê³  B ê¸°ëŠ¥ì€ Alt + Shift + C ì¼ë•Œ\u0026hellip; ì „ì²´ì ìœ¼ë¡œ ë³´ë©´ ê°™ì€ í‚¤ ì…ë ¥ì´ì§€ë§Œ ìˆœì„œê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë³„ê°œì˜ shortcutìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.\n ","permalink":"https://jjerry-test.github.io/blog/jupyter_shortcut/","tags":["Setting"],"title":"jupyter Lab ì—ì„œ ë‹¨ì¶•í‚¤ ì„¤ì •í•˜ëŠ” ë°©ë²•!"},{"categories":["Python"],"contents":"Anaconda ë¥¼ ì‚¬ìš©í•˜ë‹¤ë³´ë©´ ì—¬ëŸ¬ ê°€ìƒí™˜ê²½ì„ ë§Œë“¤ê²Œ ë©ë‹ˆë‹¤. (ì•„ë‹ìˆ˜ë„ ìˆêµ¬ìš”\u0026hellip;)\nê·¸ í›„ì— jupyter ë¥¼ ì‚¬ìš©í•˜ì‹œëŠ” ë¶„ë“¤ì´ë¼ë©´ ëŒ€ë¶€ë¶„ ì´ë ‡ê²Œ ì‚¬ìš©í•˜ì‹¤ ê²ë‹ˆë‹¤.\nconda activate í™˜ê²½ì´ë¦„ jupyter notebook conda activate í™˜ê²½ì´ë¦„ ì´ë¼ëŠ”ê±¸ ë¬´.ì¡°.ê±´ ì¨ì¤˜ì•¼í•˜ì£ ..\nì´ê²Œ ë§¤ìš° ê·€ì°®ì•˜ìŠµë‹ˆë‹¤\u0026hellip;\n \u0026ldquo;activate ì—†ì´ baseì—ì„œ jupyter notebookì„ ì‹¤í–‰í•´ë„ ê°€ìƒí™˜ê²½ì„ ì¡ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ë‚˜..\u0026rdquo;\n ì´ëŸ° ìƒê° ë§ì´ë“¤ í•˜ì‹¤ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\në‹¹ì—°íˆ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤!\nê·¸ ë°©ë²•ì— ëŒ€í•´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\nconda activate {í™˜ê²½ì´ë¦„} python -m ipykernel install --user --name {í™˜ê²½ì´ë¦„} --display-name {Jupyterì— í‘œì‹œë  ì´ë¦„} ì´ë ‡ê²Œ í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ì˜ˆì‹œë¥¼ ì§ì ‘ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n  ì €ëŠ” base í™˜ê²½ë§Œ ì¼ìŠµë‹ˆë‹¤. ê°€ìƒí™˜ê²½ì„ ê°€ë³ê²Œ í•˜ë‚˜ ë§Œë“¤ë„ë¡ í• ê²Œìš”!\n  testë¼ëŠ” ê°€ìƒí™˜ê²½ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ë¨¼ì € kernelì„ ì¶”ê°€í•˜ì§€ ì•Šê³  jupyter notebookì„ ì‹¤í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.\n    ì²«ë²ˆì§¸ ì‚¬ì§„ì„ ë³´ì‹œë©´ Python 3 ë§Œ ë‚˜ì˜¤ëŠ”ê±¸ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì € Python 3 ëŠ” baseì˜ Pythonì„ ê°€ë¦¬í‚µë‹ˆë‹¤.\nbaseì—ëŠ” ì œê°€ tensorflowë¥¼ ì„¤ì¹˜í•´ë†¨ê¸° ë•Œë¬¸ì— import ê°€ ì˜ ì‘ë™í•˜ëŠ”êµ°ìš”..\nê·¸ëŸ¼ kernelì„ ì¶”ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤. (ì¼ë‹¨ test í™˜ê²½ì— ipython ì´ ì•ˆê¹”ë ¤ìˆì–´ì„œ ì„¤ì¹˜í•¨..)\n    ì»¤ë„ì„ ì¶”ê°€í•˜ê³  ë‚˜ë©´ Installed ~~~~ ë¼ëŠ” ë©”ì„¸ì§€ë¥¼ ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”!\nê·¸ëŸ¼ ë‹¤ì‹œ baseë¡œ ëŒì•„ê°€ì„œ jupyter notebookì„ ì‹¤í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.\n    ì²«ë²ˆì§¸ ì‚¬ì§„ì„ ë³´ì‹œë©´ ì¶”ê°€í•˜ê¸° ì „ê³¼ëŠ” ë‹¤ë¥´ê²Œ test ë¼ëŠ” í•­ëª©ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!\nì‹¤í–‰ì„ í•´ë´ë„ tensorflow ëª¨ë“ˆì´ ì—†ë‹¤ê³  ë‚˜ì˜¤ëŠ”ê±¸ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\nì•ìœ¼ë¡œëŠ” conda activate ~~ë¥¼ ì•ˆí•˜ì…”ë„ ë˜ìš”! (ë¿Œë“¯)\nì´ë²ˆ í¬ìŠ¤íŒ…ì€ ì—¬ê¸°ê¹Œì§€ ì…ë‹ˆë‹¤.\në§ì€ ë¶„ë“¤ê»˜ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ë„¤ìš”!\nPS. jupyter notebook ìš°ì¸¡ ìƒë‹¨ìª½ì— ì–´ë–¤ ì»¤ë„ë¡œ ì‹¤í–‰í•˜ê³  ìˆëŠ”ì§€ í‘œì‹œê°€ ë©ë‹ˆë‹¤. (Logout ì•„ë˜)\n","permalink":"https://jjerry-test.github.io/blog/multiple_kernel/","tags":["Setting"],"title":"jupyter (ipython) ì—¬ëŸ¬ ì»¤ë„ ì‚¬ìš©í•˜ê¸°!"},{"categories":["Python"],"contents":"ì˜ˆ~~ì „ì— NIfTI íŒŒì¼ì„ load í•˜ëŠ” ë°©ë²•ì„ ì˜¬ë ¸ì—ˆìŠµë‹ˆë‹¤!\nì´ë²ˆì—ëŠ” Pythonì—ì„œ DICOM í¬ë§·ì˜ ë°ì´í„°ë¥¼ load í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ í¬ìŠ¤íŒ…ì„ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.\nê°€ì¥ ë¨¼ì € ê´€ë ¨ íŒ¨í‚¤ì§€ì¸ Pydicom ì„ ì„¤ì¹˜ë¥¼ í•´ì¤ë‹ˆë‹¤.\nconda install -c conda-forge pydicom ì €ë²ˆê³¼ ë˜‘ê°™ì´ ë‹¨ìˆœí•œ ì„¤ì¹˜ë°©ë²•!\nì´ì œ ì½”ë”©ìœ¼ë¡œ ì½ì–´ë³´ê² ìŠµë‹ˆë‹¤.\nì˜ˆì‹œ ë°ì´í„°ë¡œ ë‹¤ìŒ ë§í¬ì— ìˆëŠ” ì˜ìƒì„ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\nimport pydicom as di from matplotlib import pyplot as plt data = di.read_file(\u0026#34;.dcm ê²½ë¡œ\u0026#34;) #data = di.dcmread(\u0026#34;.dcm ê²½ë¡œ\u0026#34;) # í¸í•œê±° ì“°ì‹œë©´ ë©ë‹ˆë‹¤. img = data.pixel_array plt.imshow(img) # ìŠ¬ë¼ì´ìŠ¤ 1ì¥ì¼ ê²½ìš° #plt.imshow(img[:,:,\u0026#34;slice ë²ˆí˜¸\u0026#34;]) # ìŠ¬ë¼ì´ìŠ¤ê°€ ì—¬ëŸ¬ ì¥ì¼ ê²½ìš° plt.show() ì´ëŸ° ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\nì˜ˆì‹œë¥¼ ë³´ì—¬ë“œë¦¬ë©´\n  ì˜ˆì‹œ ì˜ìƒì´ ì¢€ ì‘ë„¤ìš”\u0026hellip;\nì–´ì¨Œë“  ì´ëŸ° ì‹ìœ¼ë¡œ ì½ìŠµë‹ˆë‹¤.\ní \u0026hellip;.NIfTIì™€ DICOMì„ í–ˆìœ¼ë‹ˆ\u0026hellip;ë‹¤ìŒì—” Insight Meta-Image ë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/dicom/","tags":["Usage"],"title":"Pythonìœ¼ë¡œ DICOM ì˜ìƒì„ ì½ì–´ë³´ì!"},{"categories":["DeepLearning"],"contents":"Deep Generative Adversarial Networks for Thin-Section Infant MR Image Reconstruction  Jiaqi Gu1, Zezu Li1, YuanYuan Wans1, 3, Haowei Yang2, Zhongwei Qiao2, and Jinhua Yu1, 3 1School of Information Science and Technology, Fudan University, Shanghai 200433, China\n2The Children\u0026rsquo;s Hospital of Fudan University, Shanghai 201102, China\n3Key Laboratory of Medical Imaging Computing and Computer Assisted Intervention of Shanghai, Department of Electronic Engineering, Institute of Functional and Molecular Medical Imaging, Fudan University, Shanghai 200433, China   Abstract  Thin section magnetic resonance images (Thin MRI) ëŠ” ë‡Œìˆ˜ìˆ , ë‡Œ êµ¬ì¡° ë¶„ì„ì— ì¢‹ì€ ì˜ìƒ. í•˜ì§€ë§Œ Thick section magnetic resonance images (Thick MRI) ì— ë¹„í•´ imaging costê°€ ë§ì´ ë“¤ê¸° ë•Œë¬¸ì— ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ. Thick MRI 2 Thin MRI ì œì•ˆ. Two stage( GAN -\u0026gt; CNN )ë¡œ êµ¬ì„±í•˜ì˜€ê³  Thick MRIì˜ Axial, Sigittal planeì„ ì´ìš©í•˜ì—¬ Thin MRIì˜ Axial reconstruction. 3D-Y-Net-GAN ì€ Axial, Sagittal Thick MRI ë¥¼ ì´ìš©í•˜ì—¬ Fusion. 3D-Dense U-Netì€ Sagittal planeì— ëŒ€í•´ ì„¸ë¶€ì ì¸ calibrations, structual correction ì œê³µ. Loss function ì€ structual detailì„ Networkê°€ capture í•  ìˆ˜ ìˆë„ë¡ ì œì•ˆ. bicubic, sparse representation, 3D-SRU-Net ê³¼ ë¹„êµ. 35ë²ˆì˜ Cross-validation, 114ê°œë¥¼ ì´ìš©í•˜ì—¬ ë‘ê°œì˜ testset êµ¬ì„±.  PSNR : 23.5 % ì¦ê°€. SSIM : 90.5 % ì¦ê°€. MMI : 21.5 % ì¦ê°€.    Introduction  Thin MRI ëŠ” slice thicknessê°€ 1mmì´ê³  sapcing gapì´ 0mm. í•˜ì§€ë§Œ í•­ìƒ Thin MRIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ. ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” Thick MRIëŠ” slice thicknessê°€ 4~6mm ì´ê³  sapcing gapì´ 0.4~1mm.  í•´ìƒë„ : Thin MRI \u0026gt; Thick MRI   ì¸ê°„ì˜ ë‡Œ ë°œë‹¬ì— ëŒ€í•œ insightë¥¼ ì£¼ê¸° ë•Œë¬¸ì— ìœ ì•„ì˜ brain MR imageëŠ” ì–´ë¥¸ì˜ brain MR image ë³´ë‹¤ ì—°êµ¬ì— ê°€ì¹˜ê°€ ìˆìŒ í•˜ì§€ë§Œ ìœ ì•„ì˜ MR imageë¥¼ ì–»ëŠ”ê²Œ ì‰½ì§€ ì•ŠìŒ. ê·¸ë˜ì„œ Thick to Thin ì œì•ˆ. ê¸°ì¡´ traditional interpolation algorithm  ì‹œê°ì ìœ¼ë¡œëŠ” ì„±ëŠ¥ì´ ì¢‹ì•„ë³´ì„. í•˜ì§€ë§Œ ì„±ì¸ì˜ brain ì— ì´ˆì ì„ ë§ì¶¤. Interpolation-based super-resolution reconstruction: effects of slice thickness Evaluation of interpolation effects on upsampling and accuracy of cost functions-based optimized automatic image registration   Frame interpolation ë°©ë²•ê³¼ ê°™ì´ ì ìš© ê°€ëŠ¥.  Slice Interpolation in MRI Using a Decomposition-reconstruction Method   Super-resolution ë¬¸ì œë¡œ ì ìš©í•  ìˆ˜ë„ ìˆìŒ.  Image super-resolution via sparse representation   CNN, GAN ì´ ë°œì „í•˜ë©´ì„œ super-resolution ì´ íƒ„ë ¥ì„ ë°›ìŒ.  [Context-Sensitive Super-Resolution for Fast Fetal Magnetic Resonance Imaging] [Deep Generative Adversarial Neural Networks for Compressive Sensing MRI]   ì´ì „ì— ì„±ì¸ì˜ Thick MRIë¥¼ Thin MRI ë¡œ reconstruction í•˜ëŠ” 3D-SRGAN ì œì•ˆí–ˆìœ¼ë‚˜ axial planeë§Œ ê³ ë ¤í–ˆìŒ. [Reconstruction of Thin-Slice Medical Images Using Generative Adversarial Network] Deep Learning ì´ reconstruction performance ë¿ ì•„ë‹ˆë¼ reconstruction time ê°ì†Œì—ë„ ë§¤ìš° íš¨ê³¼ì ì¸ê±¸ ë³´ì„.  Proposed Method A. Overview  CNNì€ ê¸°ì¡´ì—ë„ super-resolutionì—ì„œ ë§ì´ ì‚¬ìš©ë¨. í•˜ì§€ë§Œ ìµœê·¼ê¹Œì§€ ì œì•ˆëœ NetworkëŠ” ëŒ€ë¶€ë¶„ 2D imageì— ëŒ€í•œ upscaling. ëª‡ëª‡ NetworkëŠ” 3D imageë¡œ í™•ì¥í–ˆì§€ë§Œ ê·¸ë ‡ê²Œ íš¨ê³¼ë¥¼ ë³´ì§€ ëª»í–ˆìŒ. ì´ ë…¼ë¬¸ì˜ Flow       B. Network Architecture  First stageëŠ” 3D-Y-Net-GAN ìœ¼ë¡œ Thick MRIë¥¼ Thin MRIë¡œ ìƒì„± í›„ 3D-DenseU-Netìœ¼ë¡œ recalibration.  3D-Y-Net-GAN  Input : Axial, Sagittal Thick MRI Output : Thin MRI r : Upscaling Factor ( r = 8 ì¼ ê²½ìš°ì˜ ì˜ˆì‹œ )      Feature Extraction Branches\n ê° inputì— ëŒ€í•œ feature ì¶”ì¶œ. Maxpooling layerì—ì„œ [1, 2, 1], [2, 1, 1]ì˜ ë‹¤ë¥¸ strides factor ì ìš©. 3D convolutional layer ëŠ” Convolution + Batch Normalization + Swish ë¡œ êµ¬ì„±.  SwishëŠ” Activation ì˜ ì¢…ë¥˜ë¡œ ReLUë¡œ ì¸í•´ ìƒê¸°ëŠ” Dead neuronì„ ê·¹ë³µí•  ìˆ˜ ìˆìŒ. -\u0026gt; ê·¼ë° êµ³ì´ ì™œ swishì¼ê¹Œ\u0026hellip;   layers ë¥¼ ê±°ì¹œ í›„ shape ì˜ ë³€í™”  Axial : [H, W, S, 1] -\u0026gt; [H/2, W/2, S, 32] Sagittal : [H, W, r*S, 1] -\u0026gt; [H/2, W/2, S, 32]   Axialê³¼ Sagittalì˜ shapeì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— Sagittal ì— ëŒ€í•´ì„œ preprocessingìœ¼ë¡œ 3ê°œì˜ 3d convolution layer ì ìš©.    Feature Fusion Branch\n ë‘ featureë¥¼ channel ë°©í–¥ìœ¼ë¡œ Concatanation. W ë°©í–¥ìœ¼ë¡œ Upsampling í›„ H ë°©í–¥ìœ¼ë¡œ Downsampling feature ë¥¼ Concatanation. H ë°©í–¥ìœ¼ë¡œ Upsampling í›„ ì²«ë²ˆì§¸ Blockì˜ Feature mapì„ Concatanation U-Net ì—ì„œ ì•„ì´ë””ì–´ë¥¼ ì–»ì—ˆê³  structual alignment, gradient-vanishing ë“±ì„ ì™„í™”.    Reconstruction Branch\n Figure 3 (b) ì™€ ê°™ì€ êµ¬ì¡°. Upsampling layer 3ê°œë¥¼ ì—°ì†ìœ¼ë¡œ ë¶™ì—¬ì„œ 8ë°° í™•ì¥í•˜ëŠ” êµ¬ì¡° ëŒ€ì‹ ì— Multipath upscaling strategy ì ìš©. -\u0026gt; Artifact ì™„í™” íš¨ê³¼\u0026hellip;?    Discriminator    Axial Image, Saggital Image, Combination Image ê°€ Real Pairì¸ì§€ Fake Pairì¸ì§€ ê°ë³„. Input : \\((I^A, I^Y, I^S), (I^A, I^{GT}, I^S)\\) Output : Real, Fake    3D-DenseU-Net    ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” U-Netì´ì§€ë§Œ 2ê°œì˜ Enhanced residual block ì„ ì ìš©í•˜ì—¬ detail recalibration. Input : \\(I^Y, I^S, I^{YA}\\) -\u0026gt; ì–´ë–»ê²Œ 3ê°œê°€ inputìœ¼ë¡œ\u0026hellip;? Output : Thin MR Image \\(I^A\\) ë¥¼ \\(I^Y\\) ì˜ í•´ë‹¹ ìœ„ì¹˜ì— insertion í•˜ì—¬ \\(I^{YA}\\) ìƒì„±. -\u0026gt; ì•„ì§ ì´í•´ X..  Axial Information ì„ ì´ìš©í•˜ì—¬ ì •í™•í•œ axial ì„ ë§Œë“¤ê¸° ìœ„í•´\u0026hellip; \\(I^S\\) ë¥¼ \\(I^Y\\) ì— insertioní•˜ê²Œ ë˜ë©´ Sagittal ì— ëŒ€í•œ information ì´ ê³¼í•´ì§€ê¸° ë•Œë¬¸ì— Reconstrtion Axial Imageì˜ Quality ê°€ ì•ˆì¢‹ì•„ ì§ˆ ê²ƒ!   End-to-End ê°€ ì•„ë‹ˆë¼ ê°ê° ë”°ë¡œë”°ë¡œ í•™ìŠµ. -\u0026gt; Faster RCNN ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ í• ëŸ°ì§€\u0026hellip;.?  Loss Function  \\(G\\) ëŠ” generator ë¼ëŠ” ì˜ë¯¸. Self-Adaptive Charbonnier Loss  ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ëŠ” \\(\\ell2\\) ì „ë°˜ì ìœ¼ë¡œ Smoothing í•˜ê²Œ ë§Œë“¤ì–´ì§€ê³  \\(\\ell1\\) ì€ GTì™€ Prediction ì˜ ì°¨ì´ë¡œ indiscriminate í•˜ê²Œ í•™ìŠµ. Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolutionì— ë”°ë¥´ë©´ Charbonnier loss(ë¯¸ë¶„ê°€ëŠ¥í•œ $\\ell1$ì˜ ë¶„ì‚°)ê°€ \\(\\ell1, \\ell2\\) ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨. Deep Learning for Isotropic Super-Resolution from Non-Isotropic 3D Electron Microscopy ì— ë”°ë¥´ë©´ Cubic-weighted mean square error ê°€ Generated ì˜ìƒê³¼ Ground truth ê°„ì˜ ì°¨ì´ê°€ í° \u0026ldquo;ì–´ë ¤ìš´\u0026rdquo; ë¶€ë¶„ì˜ ì„±ëŠ¥ì„ ê°•ì¡°. ë‹¤ìŒê³¼ ê°™ì€ Loss ì œì•ˆ. \\(\\epsilon\\) ì€ defaultë¡œ \\(10^{-6}\\)    $$L^G_{SC} = \\frac{1}{rLWH}\\sum_{x,y,z=1,1,1}^{L,W,rH}\\sqrt{(I^{GT}_{x,y,z}-I^Y_{z,y,z})^2+\\epsilon}\\cdot\\bigg(\\frac{1}{2}+\\frac{(I^{GT}_{x,y,z}-I^Y_{z,y,z})^2}{2max((I^{GT}-I^Y)^2)}\\bigg)$$\n 3-D Gradient Correction Loss  Charbonnier LossëŠ” Pixelwise differenceì— ëŒ€í•œ Loss, Gradientì— ëŒ€í•œ ì†ì‹¤ì„ ì¤„ ìˆ˜ ìˆìŒ. ë‹¤ìŒê³¼ ê°™ì´ ê° axisì— ëŒ€í•œ Gradient ë¥¼ ì´ìš©í•˜ì—¬ Loss ì œì•ˆ.    $$L^G_{GC} = \\mathbb{E}[(\\nabla_{x}I^{GT}_{x,y,z} - \\nabla_{x}I^Y_{x,y,z})^2] \\\\ + \\mathbb{E}[(\\nabla_{y}I^{GT}_{x,y,z} - \\nabla_{y}I^Y_{x,y,z})]^2 \\\\ + \\mathbb{E}[(\\nabla_{z}I^{GT}_{x,y,z} - \\nabla_{z}I^Y_{x,y,z})^2]$$\n Adversarial Loss  LSGAN Loss ì‚¬ìš©.    $$L^D=\\frac{1}{2}\\mathbb{E}[(D(I^{GT}, I^A, I^S)-1)^2+D(I^Y, I^A, I^S)^2]$$\n$$L^G_{AD}=\\mathbb{E}[(D(I^Y, I^A, I^S)-1)^2]$$\n \\(\\ell_2\\) Weight Regularization Loss  (LossëŠ” ì•„ë‹ˆì§€ë§Œ\u0026hellip;) Overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©.    $$L^G_{WR} = \\sum\\Vert W_G\\Vert^2_2$$\n  3D-Y-Net-GAN Loss\n \\(L_G = L^G_{SC} + \\lambda_1L^G_{GC} + \\lambda_2L^G_{AD} + \\lambda_3L^G_{WR}\\)    3D-DenseU-Net Loss\n \\(L = L_{SC} + \\lambda_1L_{GC} + \\lambda_3L_{WR}\\)    Experimental Result   Multiplanar ì˜ íš¨ìœ¨ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ì„¸ ê°€ì§€ ê²½ìš°ë¡œ ë‚˜ëˆ”.\n Axial, Sagittal ë‘˜ ë‹¤ ì´ìš©. Axial ë§Œ ì´ìš©. Saigittal ë§Œ ì´ìš©.    Loss functionì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ë„¤ ê°€ì§€ ê²½ìš°ë¡œ ë‚˜ëˆ”.\n \\(\\ell1norm + L_{GC} + L_{AD} + L_{WR}\\) (pixelwise lossë¥¼ \\(\\ell1norm\\)ìœ¼ë¡œ ëŒ€ì²´.) \\(L_{SC} + L_{GC} + L_{WR}\\) \\(L_{SC} + L_{AD} + L_{WR}\\) \\(L_{SC} + L_{GC} + L_{AD} + L_{WR}\\)    Evalutaion Method ë¡œëŠ” ì•„ë˜ì™€ ê°™ì´ ë„¤ ê°€ì§€ ê¸°ë²•ê³¼ ìì‹ ë“¤ì˜ Network\n Bicubic interpolation Sparse representation 3D-SRU-Net 3D-Y-Net-GAN 3D-Y-Net-GAN + 3D-DenseU-Net    Metricsìœ¼ë¡œëŠ” ë‹¤ìŒ ì„¸ ê°€ì§€ ì‚¬ìš©.\n PSNR(Peak Signal-to-Noise Ratio)    $$ \\begin{alignedat}{2} MAX_I = 255 \\\\ PSNR = 20\\cdot\\log_{10}\\Bigg(\\frac{MAX_I}{\\sqrt{\\frac{1}{rLWH}\\sum_{x, y, z}(I^R_{x,y,z}-I^{GT}_{x,y,z})^2}}\\Bigg) \\end{alignedat} $$\n SSIM(Structural SIMilarity)  $$L=255(\\text{dynamic\\ range})$$ $$\\mu:\\text{Variance} $$ $$\\mu_{ab}:\\text{Covariance} $$ $$c_1=(k_1L)^2 $$ $$c_2=(k_2L)^2 $$ $$SSIM=\\frac{(2\\mu_a\\mu_b+c_1)(2\\sigma_{ab}+c_2)}{(\\mu_a^2+\\mu_b^2+c_1)(\\sigma_a^2+\\sigma_b^2+c_2)}$$\n NMI(Normalized Mutual Information)  $$H(X) = -\\sum_{x_i}\\in{X} p(x_i)\\log {p(x_i)}$$ $$H(X, Y) = -\\sum_{y_i\\in{Y}} \\sum_{x_i\\in{X}}p(x_i, y_i)\\log{p(x_i, y_i)}$$ $$NMI(X, Y) = 2\\frac{H(X) + H(Y) - H(X, Y)}{H(X)+H(Y)}$$\n pixel ê°’ì„ [-1, 1]ë¡œ clipping -\u0026gt; ë‹¤ì‹œ 8-bit gray scaleë¡œ ë³€í™˜. Generated MR images ì™€ Ground truthê°€ ë¹„ìŠ·í•  ìˆ˜ë¡ ë†’ì€ ê°’ì„ ê°€ì§.  A. Data and Preprocessing  ì´ 154 samplesì˜ 2~5ì„¸ ìœ ì•„ Axial, Sagittal Thick MRI, Axial Thin MRI     Table 1. ê³¼ ê°™ì€ parameter ì‚¬ìš©. Dataset ë¶„í•   Cross Validation Dataset : 40 samples Test 1 Dataset : 65 samples Test 2 Dataset : 49 samples   Preprocessing  ê° ì˜ìƒë³„ë¡œ ë‹¤ë¥¸ parameterë¥¼ ê°€ì§€ê³  ìˆê³  intensities ë„ ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì— spatial misalignment, intensity imblanceë¥¼ ë°œê²¬. Registrationì„ ìœ„í•´ SPM12 ë¥¼ ì´ìš©í•˜ì—¬ unified spatial normalization ìˆ˜í–‰.  DICOM to NIfTI Segment gray matter, white matter, cerebrospinal fluid, skull, scalp, and air mask. Nonlinear deformation field ICBM Asian brain template in affine regularization   Grayscale Normalization  MRI ëŠ” 16 bit.. ë‹¨ìˆœ linear transformation ìœ¼ë¡œ [-1, 1]ë¡œ mapping.   Histogram Matching  ê³ ì •ëœ ìƒ˜í”Œì„ referenceë¡œ histogram matching ì ìš©. histogram imbalance ì œê±°.     Data Augmentation  Radial Transformation  Image Augmentation using Radial Transform for Training Deep Neural Networks   Mirror Reflection    B. Experimental Settings   5-fold cross-validation ì ìš©.\n  35 ê°œì¤‘ ëœë¤ìœ¼ë¡œ 28:7ë¡œ training:validation . -\u0026gt; ì•ì—ì„  40ê°œë¼ë”ë‹ˆ..?\n  Training 3D-Y-Net-GAN\n Batch Size : 16 Epochs : 200 Adam Optimizer Parameter  \\(\\beta_1\\): 0.9 Learning rate schedule  Initial value : 5*10-4 Decay Step : 252 Decay rate : 0.989     $\\lambda_1, \\lambda_2, \\lambda_3$ : 0.2, 0.02, 0.1 He initializer    Training 3D-DenseU-Net\n Batch Size : 12 Epochs : 300 Adam Optimizer Parameter  \\(\\beta_1\\): 0.9 Learning rate schedule  Initial value : 5*10-4 Decay Step : 373 Decay rate : 0.989     \\(\\lambda_1, \\lambda_3\\) : 1, 0.001 He initializer    SR Parameter\n Dictionary size = 512 Patch number = 100,000 Patch size = 13 x 13 Sparsity Regularization = 0.15 Overlap = 12.    Training 3D-SRU-Net\n Batch Size : 32 Epochs : 300 Adam Optimizer Parameter  \\(\\beta_1\\): 0.9 Initial value : 5*10-4      C. Ablation Experiment On Input Data  Inputì„ ë³€ê²½í•˜ë©´ì„œ ì‹¤í—˜ ì§„í–‰.      Axial ê³¼ Sagittal ì„ ê°™ì´ ì‚¬ìš©í–ˆì„ ë•Œê°€ ì¢€ ë” ì„¸ë¶€ì ì¸ êµ¬ì¡°, ì ì€ ì™œê³¡ì„ ë³´ì„.  ë‘ ì¶•ì˜ ì˜ìƒì´ ì„œë¡œ ì¡°í•©í•˜ì—¬ reconstruction taskë¥¼ í–¥ìƒ.   Quantitive evaluation ì—ì„œë„ ë” ë†’ì€ ì§€í‘œë¥¼ ì‚°ì¶œ.  D. Ablation Experiment On Loss Function  Lossë¥¼ ë³€ê²½í•˜ë©´ì„œ ì‹¤í—˜ ì§„í–‰.      Self-Adaptive Charbonnier Lossì— ë¹„í•´ \\(\\ell1 norm\\) ì´ íë¦° ì˜ìƒì„ ìƒì„±. Without Gradient Correction Loss  ëœ ì„ ëª…í•œ ì˜ìƒì„ ìƒì„±.   Without Adversarial Loss  ëœ realistic ì˜ìƒì„ ìƒì„±. -\u0026gt; ?????ê·¸ëƒ¥ ì“´ ë§ì¸ê°€..   Table3 \u0026hellip;ì§€í‘œ ì¢€ ì´ìƒ..  E. Comparison With Other Methods  ë‹¤ë¥¸ Methodë“¤ê³¼ ë¹„êµ.      ì œì•ˆí•œ methodë¡œ ìƒì„±ëœ imageê°€ ê°€ì¥ Realisticí•˜ê³  Ground truth ì™€ ê°€ì¥ ë¹„ìŠ·í•˜ë‹¤ê³  í•¨. ëŒ€ë¶€ë¶„ Quantitative evaluation ì—ì„œ ì œì•ˆí•œ methodê°€ ë‹¤ë¥¸ ê²ƒë“¤ì„ ë‹¤ ë›°ì–´ë„˜ìŒ.  Conclusion  ì œì•ˆí•œ Method ì—ì„  Data preprocessingì´ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤\u0026hellip;\u0026hellip;  ","permalink":"https://jjerry-test.github.io/blog/thin/","tags":["Paper"],"title":"Review: MRI interpolation using Deep Learning"},{"categories":["Python"],"contents":"ì´ë²ˆ í¬ìŠ¤íŒ…ì€ ì œê°€ ë§ì´ ì“°ëŠ” íŒ¨í‚¤ì§€ë“¤ì— ëŒ€í•´ì„œ ì ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.\nì¶”í›„ì— í¬ë§·í•˜ê³  ë‹¤ì‹œ ì„¸íŒ…í•  ìˆ˜ë„ ìˆìœ¼ë‹ˆê¹Œ\u0026hellip;.\nì €ëŠ” Anacondaê°€ ì•„ë‹Œ Minicondaë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì–´ì§€ê°„í•˜ë©´ í•˜ë‚˜ í•˜ë‚˜ ì„¤ì¹˜ë¥¼ í•´ì¤˜ì•¼í•´ìš”. ë¬¼ë¡  ë¨¸ë¦¬ë¡œëŠ” ê¸°ì–µí•˜ê³  ìˆì§€ë§Œ ì»¤ë§¨ë“œ ì“°ê¸°ê°€ ê·€ì°®ìœ¼ë‹ˆ\u0026hellip;ì ì–´ë†“ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤!\nê°ê° ì„¤ëª…ì€\u0026hellip;ìƒëµí• ê±°ì—ìš”. ë³´ì‹œëŠ” ë¶„ë“¤ êµ¬ê¸€ë§ ì‹¤ë ¥ì„ ë¯¿ìŠµë‹ˆë‹¤.\nScientific uses  numpy, scipy, pandas conda install numpy scipy pandas  Visualization  matplotlib, seaborn conda install matplotlib seaborn  Image Processing  pillow, scikit-image, opencv conda install pillow scikit-image opencv  ML \u0026amp; DL  scikit-learn, tensorflow, pytorch (kerasë„ ì¼ì—ˆëŠ”ë°\u0026hellip;ì´ë²ˆì— ëºì–´ìš”..) conda install scikit-learn tensorflow conda install -c pytorch pytorch torchvision  Medical Image Processing  pydicom, nibabel, simpleitk conda install -c conda-forge pydicom nibabel conda install -c SimpleITK SimpleITK  Tools  tqdm, jupyter, jupyter-lab conda install tqdm jupyter jupyterlab  ì´\u0026hellip;.ì •ë„ë„¤ìš”!\nì†”ì§íˆ ì œê°€ ì´ë ‡ê²Œ í¬ìŠ¤íŒ…ì„ í•˜ëŠ” ì´ìœ ëŠ”\u0026hellip;.\nì´ë¯¸ ì œ ë…¸íŠ¸ë¶ì„\u0026hellip;í•œë²ˆ ê°ˆì•„ì—ì—ˆìŠµë‹ˆë‹¤\u0026hellip;\nconda ë¥¼ 4.7.5 ë¡œ ì˜¬ë¦¬ëŠ” ìˆœê°„\u0026hellip;ëª¨ë“ ê²Œ ë‚ ì•„ê°”ê±°ë“ ìš”.\ní•˜\u0026hellip;ì–¼ë¥¸ ì„¸íŒ…í•˜ëŸ¬ ê°‘ë‹ˆë‹¤..\nP.S conda ë¥¼ ì¡°ì‹¬í•˜ì„¸ìš”\u0026hellip;\n","permalink":"https://jjerry-test.github.io/blog/conda_package/","tags":["Setting"],"title":"(ê°œì¸ì ìœ¼ë¡œ) ë¬´.ì¡°.ê±´ ì„¤ì¹˜í•˜ëŠ” Anaconda íŒ¨í‚¤ì§€"},{"categories":["Python"],"contents":"Index  Index  A. Visualizing statistical relationships  Relating variables with scatter plots Emphasizing continuity with line plots  Aggregation and representing uncertainty Plotting subsets of data with semantic mappings Plotting with date data   Showing multiple relationships with facets      A. Visualizing statistical relationships Relationships ì‹œê°í™”ì—ì„  relplot() ì´ë¼ëŠ”ê±¸ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. relplot() ì—ëŠ” kindë¼ëŠ” ì˜µì…˜ìœ¼ë¡œ scatter, line ì„ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì¼ë‹¨! ë‹¤ìŒê³¼ ê°™ì´ ê¸°ë³¸ì ì¸ íŒ¨í‚¤ì§€ë¥¼ import í•˜ê³  ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤!\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026#34;darkgrid\u0026#34;) # ì¶”í›„ Part 3 ì—ì„œ ë‹¤ë£° ë‚´ìš©! Relating variables with scatter plots Scatter plotì„ í•´ë³¼ê²ë‹ˆë‹¤!\nscatterplot()ì„ ì‚¬ìš©í•´ë„ ë˜ì§€ë§Œ relplot()ì„ ì‚¬ìš©í•´ì„œ ê·¸ë ¤ë³´ê² ìŠµë‹ˆë‹¤.\nê°€ì¥ ë¨¼ì € \u0026hellip; ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ê² ìŠµë‹ˆë‹¤.\ntips = sns.load_dataset(\u0026#34;tips\u0026#34;) tips.head(5) ìœ„ë¥¼ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒ ì‚¬ì§„ê³¼ ê°™ì´ ì¶œë ¥ì´ ë‚˜ì˜µë‹ˆë‹¤.\n  ì •í™•íˆ ë¬´ìŠ¨ ë°ì´í„°ì¸ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ\u0026hellip; ì–´ë–¤ ê°€ê²Œì˜ ê°€ê³„ë¶€\u0026hellip;? ê°™ìŠµë‹ˆë‹¤. ì´ ê¸ˆì•¡, íŒ, ì„±ë³„, í¡ì—° ì—¬ë¶€, ìš”ì¼, ì‹œê°„ëŒ€, í¬ê¸°(\u0026hellip;?) ë“±ì˜ ì¹´í…Œê³ ë¦¬ê°€ ìˆë„¤ìš”.\nê·¸ëŸ¼ í•œë²ˆ plot í•´ë³´ê² ìŠµë‹ˆë‹¤.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, data=tips);   ë‹¤ìŒê³¼ ê°™ì€ figureê°€ ë‚˜ì˜¤ë„¤ìš”!\nx, y, dataì— ë“¤ì–´ê°„ ì˜ë¯¸ë¥¼ ì•Œì•„ë³¼ê¹Œìš”? x= ëŠ” xì¶•ì„ ì–´ë–¤ ë°ì´í„°ë¡œ í• ì§€ ì •í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. y= ëŠ” ë‹¹ì—°íˆ yì¶•ì´ê² ì£ ? data= ëŠ” ì–´ë–¤ dataë¥¼ plotì— ì‚¬ìš©í• ì§€ ì ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.\nê·¸ë˜ì„œ plotí•˜ëŠ” ì½”ë“œë¥¼ í’€ì–´ì„œ ì–˜ê¸°í•´ë³´ìë©´\ntipsë¼ëŠ” DataFrameì—ì„œ total_billì„ xì¶•ìœ¼ë¡œ tipì„ yì¶•ìœ¼ë¡œ ê³¨ë¼ì„œ scatter plot í•˜ë¼ëŠ” ì–˜ê¸°ì…ë‹ˆë‹¤.\nì¢€ ë” í•´ë³¼ê¹Œìš”?\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, data=tips);   x, y, dataëŠ” ìœ„ë‘ ë‹¤ë¥¼ê²Œ ì—†ëŠ”ë°\u0026hellip; hueë¼ëŠ” ì˜µì…˜ì´ ì¶”ê°€ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\nhue='smoke'ë¼ê³  í¡ì—° ì—¬ë¶€ì— ëŒ€í•´ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œë¥¼ í•´ì¤ë‹ˆë‹¤!\ní•˜ë‚˜ ë˜ ìˆìŠµë‹ˆë‹¤.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, style=\u0026#34;smoker\u0026#34;, data=tips);   style ì˜µì…˜ì´ ì¶”ê°€ë˜ì—ˆì–´ìš”! style='smoker'ë¥¼ ì¶”ê°€í•´ì£¼ë©´ì„œ plot ìŠ¤íƒ€ì¼ì´ ì¶”ê°€ë˜ì—ˆë„¤ìš”!\në‹¤ìŒ ì‚¬ì§„ì€ style=\u0026quot;time\u0026quot; ì˜µì…˜ì„ ì¤€ê±°ì—ìš”!\n  ìƒ‰ìœ¼ë¡œëŠ” í¡ì—°ì—¬ë¶€ë¥¼ í‘œì‹œí•˜ê³  plot ìŠ¤íƒ€ì¼ë¡œëŠ” ì‹œê°„ëŒ€ë¥¼ í‘œì‹œí•´ì£¼ë„¤ìš”!\nì, hue ì˜µì…˜ì˜ ë˜ ë‹¤ë¥¸ ì˜ˆì‹œì…ë‹ˆë‹¤.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;size\u0026#34;, data=tips);   ì´ì „ ì˜ˆì‹œì—ì„  Yes or No ì˜€ì£ .\nì´ë²ˆ ìˆ˜ì¹˜ì— ëŒ€í•´ ìƒ‰ë³„ë¡œ ì˜ë¯¸ë¥¼ ì¤¬ìŠµë‹ˆë‹¤!\në‹¤ìŒê³¼ ê°™ì´ pallette ì˜µì…˜ì„ ì¤˜ì„œ ìƒ‰ì„ ë°”ê¿€ ìˆ˜ë„ ìˆì–´ìš”!\npalletteì— ëŒ€í•œ ìì„¸í•œê±´ Part 3 ì—ì„œ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;size\u0026#34;, palette=\u0026#34;ch:r=-.5,l=.75\u0026#34;, data=tips);   ì˜¤\u0026hellip;scatter plotì˜ ëì´ ë³´ì—¬ìš”\u0026hellip;\nsize ì˜µì…˜ì…ë‹ˆë‹¤.\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, size=\u0026#34;size\u0026#34;, data=tips);   size=\u0026quot;size\u0026quot;ë¼ê³  ì˜µì…˜ì„ ì¤¬ì–´ìš”!\nplotì˜ í¬ê¸°ì— ë”°ë¼ ì˜ë¯¸ê°€ ë‚˜ë‰˜ì–´ì¡ŒìŠµë‹ˆë‹¤!\në‹¤ìŒê³  ê°™ì´ sizesë¥¼ ì´ìš©í•´ì„œ í¬ê¸° ë²”ìœ„ë¥¼ ì •í•´ì¤„ìˆ˜ë„ ìˆë„¤ìš”!\nsns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, size=\u0026#34;size\u0026#34;, sizes=(15, 200), data=tips);   Emphasizing continuity with line plots ì´ íŒŒíŠ¸ëŠ” ì—°ì†ì ì¸ ê°’ì„ ê°€ì§„ ë°ì´í„°ì— ëŒ€í•œ plotì…ë‹ˆë‹¤.\në‹¤ìŒê³¼ ê°™ì´ DataFrameì„ í•˜ë‚˜ ë§Œë“¤ì–´ ë´…ì‹œë‹¤!\ndf = pd.DataFrame(dict(time=np.arange(500), value=np.random.randn(500).cumsum())) df.head(5)   ê²°ê³¼ëŠ” ì‚¬ì§„ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ìš”!\ntime, value ë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê±°ì— ì´ˆì ì„ ë§ì¶”ì‹œë©´ ë©ë‹ˆë‹¤!\nì¼ë³„ë¡œ ë­”ê°€ ê°’ì´ ë“¤ì–´ê°€ ìˆë„¤ìš”.\nì´ë¥¼ relplot() ì„ ì´ìš©í•˜ì—¬ ê·¸ë ¤ë³´ê² ìŠµë‹ˆë‹¤. kind ì˜µì…˜ì— \u0026quot;line\u0026quot; ë¼ê³  ì£¼ë©´ ë©ë‹ˆë‹¤!\nautofmt_xdata() ë¼ëŠ”ê±´ ì´ë¦„ì—ì„œ ìœ ì¶”í•  ìˆ˜ ìˆë“¯ì´ x ì¶• ë ˆì´ë¸”ì„ ë°ì´í„°ì— ë§ê²Œ formatì„ ìë™ìœ¼ë¡œ ë§ì¶°ì£¼ëŠ”ê±°ì—ìš”!\nì§€ê¸ˆì€ x ì¶•ì´ ë‚ ì§œë‹ˆê¹Œ ë‚ ì§œì— ëŒ€í•´ì„  x ì¶• ë ˆì´ë¸”ì„ ì‚´ì§ì¿µ ê¸°ìš¸ì¸ê±°ì—ìš”!\nì´ê±¸ ì“°ì§€ ì•Šìœ¼ë©´ ë‚ ì§œê°€ ê²¹ì³ì§ˆê±°ì—ìš”\u0026hellip;(í•œë²ˆ í•´ë³´ì„¸ìš”..)\ng = sns.relplot(x=\u0026quot;time\u0026quot;, y=\u0026quot;value\u0026quot;, kind=\u0026quot;line\u0026quot;, data=df) g.fig.autofmt_xdate()   ìœ„ ì˜ˆì‹œëŠ” x ì¶•ì´ ì—°ì†ì ì´ì§€ë§Œ ì‹œê³„ì—´ ë°ì´í„°ì˜€ìŠµë‹ˆë‹¤!\në§Œì•½ ì—°ì†ì ì´ì§€ë§Œ ì´ë™ê²½ë¡œì™€ ê°™ì´ x, y ì¶• ì¢Œí‘œë§Œ ê°€ì§„ ë°ì´í„°ë¼ë©´?\nì˜ˆì‹œë¥¼ ë“¤ì–´ë³¼ê²Œìš”! ë‹¤ìŒê³¼ ê°™ì´ x, y ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\ndf = pd.DataFrame(np.random.randn(500, 2).cumsum(axis=0), columns=[\u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;]) df.head(5)   ì´ëŸ° ìœ„ì¹˜ì— ëŒ€í•œ ë°ì´í„°ëŠ” sort = False ì˜µì…˜ì„ ì¤˜ì•¼í•©ë‹ˆë‹¤!\nê·¸ë ‡ì§€ ì•Šìœ¼ë©´ \u0026hellip;. x ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ sorting í•´ì„œ plot í•´ë²„ë¦½ë‹ˆë‹¤..\nsns.relplot(x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;, sort=False, kind=\u0026#34;line\u0026#34;, data=df);   Aggregation and representing uncertainty ì´ íŒŒíŠ¸ëŠ” ì§‘ê³„ ë° ì‹ ë¢°êµ¬ê°„ í‘œì‹œì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\nseabornì€ ê¸°ë³¸ ê°’ìœ¼ë¡œ 95% ì‹ ë¢°êµ¬ê°„ì„ í‘œì‹œí•´ì¤€ë‹¤ê³  í•˜ë…œìš”!\në‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ load í•´ì¤ë‹ˆë‹¤!\nfmri = sns.load_dataset(\u0026#34;fmri\u0026#34;) fmri.head(5)   fmri data êµ°ìš”! ì¼ë‹¨ ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€ ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤!\nê° í™˜ìë³„ë¡œ timepoint ë¥¼ ê°€ì§€ê³  signalì„ ê°€ì§€ëŠ”ê±´ ì•Œê² ë„¤ìš”..\nì´ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì´ plotì„ í•´ë´…ì‹œë‹¤!\në­”ê°€ ì„ ì´ë‘ ë²”ìœ„(?)ë¡œ ë³´ì´ëŠ” ë¶ˆíˆ¬ëª…í•œ ë¶€ë¶„ì´ ìƒê²¼ìŠµë‹ˆë‹¤!\nì„ ì€ í•´ë‹¹ timepoint ì—ì„œ signal ì˜ mean ê°’ì´ê³  ë¶ˆíˆ¬ëª…í•œ ë¶€ë¶„ì€ ì‹ ë¢°êµ¬ê°„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, kind=\u0026#34;line\u0026#34;, data=fmri);     \u0026ldquo;ê·¸ëŸ¼\u0026hellip;ì´ëŸ° ë°ì´í„°ëŠ” í•­ìƒ ì‹ ë¢°êµ¬ê°„ì„ ê°™ì´ ë´ì•¼í•˜ëŠ”ê°€?\u0026rdquo;\n  ì•„ë‹™ë‹ˆë‹¤. ci(Confidence Intervals) ì˜µì…˜ì„ ë°”ê¿”ì£¼ë©´ ë©ë‹ˆë‹¤!\nci=None ì´ë¼ê³  ì˜µì…˜ì„ ì£¼ê³  plotì„ í•˜ë©´ ì‹ ë¢°êµ¬ê°„ì´ í‘œí˜„ì´ ì•ˆë©ë‹ˆë‹¤!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, ci=None, kind=\u0026#34;line\u0026#34;, data=fmri);   ë˜ ë‹¤ë¥¸ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤.\në°ì´í„°ê°€ ë§¤ìš° í° ê²½ìš°! ci ì˜µì…˜ì— í‘œì¤€ í¸ì°¨ë¥¼ ë„£ì–´ì£¼ë©´ ê° timepoint ì—ì„œ ë¶„ì‚°ì„ í‘œí˜„í•´ì¤ë‹ˆë‹¤!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, kind=\u0026#34;line\u0026#34;, ci=\u0026#34;sd\u0026#34;, data=fmri);   ë§ˆì§€ë§‰ìœ¼ë¡œ\u0026hellip; ì´ëŸ° ì§‘ê³„ë§ê³  ì •ë§ ì›ë³¸ ê·¸ëŒ€ë¡œ plot í•´ë³´ê³  ì‹¶ë‹¤ë©´!\nestimator=Noneì´ë¼ê³  ì£¼ë©´ ë©ë‹ˆë‹¤.\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, estimator=None, kind=\u0026#34;line\u0026#34;, data=fmri);   Plotting subsets of data with semantic mappings ì´ íŒŒíŠ¸ì—ì„  relplot() ì„ ì´ìš©í•´ì„œ scatter plot, line plotì„ ë™ì‹œì— í‘œí˜„í•©ë‹ˆë‹¤.\në°ì´í„°ëŠ” ìœ„ íŒŒíŠ¸ì—ì„œ ì¼ë˜ fmri ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!\nê·¸ë¦¬ê³  ìœ„ íŒŒíŠ¸ì™€ ê°™ì´ relplot(kind=\u0026quot;line\u0026quot;)ì„ ì“°ì§€ë§Œ hue=\u0026quot;event\u0026quot;ë¼ëŠ” ì˜µì…˜ì„ ì£¼ë„ë¡í•©ë‹ˆë‹¤!\ní•œë²ˆì— scatter plotê³¼ line plotì´ í‘œí˜„ë˜ë„¤ìš”!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;event\u0026#34;, kind=\u0026#34;line\u0026#34;, data=fmri);   ê·¸ëŸ¼\u0026hellip;styleë„ ì¶”ê°€í•´ë³´ë„ë¡ í•˜ì£ !\nhue=\u0026quot;region\u0026quot;, style=\u0026quot;event\u0026quot;ë¼ê³  ì˜µì…˜ì„ ì¤ë‹ˆë‹¤!\nì ì  í‘œí˜„í•´ì£¼ëŠ”ê²Œ ë§ì•„ì§€ì£ ?\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;region\u0026#34;, style=\u0026#34;event\u0026#34;, kind=\u0026#34;line\u0026#34;, data=fmri);   ë˜í•œ ì„  ìŠ¤íƒ€ì¼ê³¼ ë§ˆì»¤ ì˜µì…˜ì„ ì¤„ ìˆ˜ë„ ìˆì–´ìš”!\ndashes=False, markers=Trueë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”!\nì ì„ ì´ ëª¨ë‘ ì‹¤ì„ ì´ ë˜ì—ˆê³  ë§ˆì»¤ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;region\u0026#34;, style=\u0026#34;event\u0026#34;, dashes=False, markers=True, kind=\u0026#34;line\u0026#34;, data=fmri);   ê´œíˆ ì •ë³´ê°€ ë§ì•„ì§€ë©´\u0026hellip;í•´ì„ì´ ê²ë‚˜ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤..\në‹¤ìŒê³¼ ê°™ì´ ë‹¨ìˆœí™”í•˜ëŠ”ê²Œ ì¢‹ì„ ë•Œë„ ìˆìŠµë‹ˆë‹¤!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;event\u0026#34;, style=\u0026#34;event\u0026#34;, kind=\u0026#34;line\u0026#34;, data=fmri);   í•„ìš”ì— ë”°ë¼ ëª¨ë“  ìƒ˜í”Œ ê°ê°ì„ plot í•´ì•¼í•˜ê¸°ë„ í•˜ì£ ..\në‹¤ìŒ ì˜ˆì‹œëŠ” eventê°€ stim ì¸ ë°ì´í„°ë“¤ì„ subject ë³„ë¡œ plotì„ í•©ë‹ˆë‹¤!\n(ì € plotì´ ë˜ëŠ” ë°©ë²•ì€ ì¶”í›„ì— ì¢€ ë” ìì„¸íˆ ì„¤ëª…ì„ ì ì„ê²Œìš”!)\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;region\u0026#34;, units=\u0026#34;subject\u0026#34;, estimator=None, kind=\u0026#34;line\u0026#34;, data=fmri.query(\u0026#34;event == \u0026#39;stim\u0026#39;\u0026#34;));   ì—¬ê¸°ì„œëŠ” Colormapì„ ì¡°ê¸ˆì”© ë‹¤ë¤„ë³´ëŠ” êµ¬ê°„ì…ë‹ˆë‹¤.\në‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ load í•©ë‹ˆë‹¤!\n(ë­”ì§€ëŠ” ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\u0026hellip;)\ndots = sns.load_dataset(\u0026#34;dots\u0026#34;).query(\u0026#34;align == \u0026#39;dots\u0026#39;\u0026#34;) dots.head(5)   ì§€ê¸ˆê¹Œì§€ í•´ì™”ë˜ê±° ì²˜ëŸ¼ plotì„ í•´ë´…ë‹ˆë‹¤.\nsns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, hue=\u0026#34;coherence\u0026#34;, style=\u0026#34;choice\u0026#34;, kind=\u0026#34;line\u0026#34;, data=dots);   ìŒ\u0026hellip;.ê·¼ë° ìœ„ì— ì‚¬ì§„ì´ ë­”ê°€ ë¶€ì¡±í•˜ë‹¤ê³  ëŠê»´ì§‘ë‹ˆë‹¤.\nìì„¸íˆ ë³´ì‹œë©´ ê·¸ë ¤ì§€ëŠ” ì‹¤ì„ , ì ì„ ì€ ê°ê° 6ê°œì¸ë° í‘œí˜„ëœ ìƒ‰ìƒì€ 4ê°œë„¤ìš”\u0026hellip;\në‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•˜ë©´ palletteë¥¼ customí•˜ì—¬ ìƒ‰ìƒì„ 6ê°œë¡œ ëŠ˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\npalette = sns.cubehelix_palette(light=.8, n_colors=6) sns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, hue=\u0026#34;coherence\u0026#34;, style=\u0026#34;choice\u0026#34;, palette=palette, kind=\u0026#34;line\u0026#34;, data=dots);   ë˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Colormapì„ Normalization í•  ìˆ˜ë„ ìˆì£ !\nfrom matplotlib.colors import LogNorm palette = sns.cubehelix_palette(light=.7, n_colors=6) sns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, hue=\u0026#34;coherence\u0026#34;, style=\u0026#34;choice\u0026#34;, hue_norm=LogNorm(), kind=\u0026#34;line\u0026#34;, data=dots);   ì•„ë‹ˆë©´\u0026hellip; ë‹¤ìŒê³¼ ê°™ì´ sizeì— ì˜ë¯¸ë¥¼ ë¶€ì—¬í•  ìˆ˜ë„ ìˆê² ì£ .\nsns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, size=\u0026#34;coherence\u0026#34;, style=\u0026#34;choice\u0026#34;, kind=\u0026#34;line\u0026#34;, data=dots);   ì—¬ê¸°ì„  Colormap ë³€ê²½ì´ ì£¼ìš” ë‚´ìš©ì´ì˜€ìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ ë‹¤ìŒê³¼ ê°™ì´ Colormapë„ ì£¼ë©´ì„œ\u0026hellip;sizeë¥¼ ì´ìš©í•´ì„œ ì˜ë¯¸ë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ê² ì£ !?\nsns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;firing_rate\u0026#34;, hue=\u0026#34;coherence\u0026#34;, size=\u0026#34;choice\u0026#34;, palette=palette, kind=\u0026#34;line\u0026#34;, data=dots);   Plotting with date data ì´ì „ì— ë‚ ì§œ í˜¹ì€ ì‹œê°„ê³¼ ì—°ê´€ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ê²½ìš° autofmt_xdata()ë¼ëŠ” ë¶€ë¶„ì´ ìˆì—ˆìŠµë‹ˆë‹¤.\nì´ëŠ” xì¶• ë ˆì´ë¸”ì˜ formatì„ ìë™ìœ¼ë¡œ ë§ì¶°ì£¼ëŠ” ë¶€ë¶„ì´ë¼ê³  í–ˆì—ˆëŠ”ë°ìš”.\nê°€ë ¹ í¬ë§·ì„ ë³€ê²½í•´ì•¼í•˜ëŠ” ìƒí™©ì´ ë°œìƒí•œë‹¤ë©´..?\nmatplotlib ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë³€ê²½í•˜ì…”ì•¼í•©ë‹ˆë‹¤..\nì™œëƒí•˜ë©´ seabornì˜ ë¼ˆëŒ€ê°€ matplotlib ì´ë‹ˆê¹Œìš”!\ndf = pd.DataFrame(dict(time=pd.date_range(\u0026#34;2017-1-1\u0026#34;, periods=500), value=np.random.randn(500).cumsum())) df.head(5)   g = sns.relplot(x=\u0026#34;time\u0026#34;, y=\u0026#34;value\u0026#34;, kind=\u0026#34;line\u0026#34;, data=df) g.fig.autofmt_xdate()   Showing multiple relationships with facets ì§€ê¸ˆê¹Œì§€ëŠ” ë‹¨ 1ê°œì˜ figure ë§Œ plot í–ˆìŠµë‹ˆë‹¤!\ní•˜ì§€ë§Œ ë³´í†µ..ë‹¤ìˆ˜ì˜ figureê°€ í•„ìš”í•˜ì£ ..\nì´ë²ˆì—” ê·¸ ë¶€ë¶„ì— ëŒ€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\nrelplot()ì€ FacetGrid ê¸°ë°˜ì´ê¸° ë•Œë¬¸ì— ì‰½ê²Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\u0026ldquo;facet\u0026rdquo; ì´ë¼ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜¬í…ë°ìš”.\nì´ëŠ” ì „ì²´ figure í•˜ë‚˜ë¥¼ facetì´ë¼ê³  ì¹­í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ load í•˜ê³  plot í•´ì£¼ì„¸ìš”! col=\u0026quot;time\u0026quot; ì´ë¼ëŠ” ì˜µì…˜ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ Lunchì— ëŒ€í•œ figureì™€ Dinnerì— ëŒ€í•œ figure ë‘ ê°œê°€ plot ë©ë‹ˆë‹¤!\nì—¬ê¸°ì„  1x2 facet ì´ë¼ê³  í•  ìˆ˜ ìˆì–´ìš”!\ntips = sns.load_dataset(\u0026#34;tips\u0026#34;) tips.head(5) sns.relplot(x=\u0026#34;total_bill\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;smoker\u0026#34;, col=\u0026#34;time\u0026#34;, data=tips);   ë˜í•œ ë‹¤ìŒê³¼ ê°™ì´ ë‘ ë³€ìˆ˜ì— ëŒ€í•´ figureë¥¼ ë‚˜ëˆŒ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\ncol=\u0026quot;region\u0026quot;, row=\u0026quot;event\u0026quot;ë¼ê³  í•˜ë©´ í–‰ì€ **\u0026ldquo;event\u0026rdquo;**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—´ì€ **\u0026ldquo;region\u0026rdquo;**ìœ¼ë¡œ ì´ 4ê°œê°€ plot ë©ë‹ˆë‹¤!\nì´ê±´ 2x2 facet!\nfmri = sns.load_dataset(\u0026#34;fmri\u0026#34;) fmri.head(5) sns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;subject\u0026#34;, col=\u0026#34;region\u0026#34;, row=\u0026#34;event\u0026#34;, height=3, kind=\u0026#34;line\u0026#34;, estimator=None, data=fmri);   fmri ë°ì´í„°ì˜ ê²½ìš° ì—¬ëŸ¬ subjectê°€ ìˆì—ˆìŠµë‹ˆë‹¤.\nê·¸ ê°ê° subject ë³„ë¡œ figureë¥¼ ë§Œë“¤ê³  ì‹¶ì„ë•\u0026hellip;?\në‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•©ë‹ˆë‹¤.\ncol=\u0026quot;subject\u0026quot;, col_wrap=5 ëŠ” ì—´ì„ subjectë¥¼ ê¸°ì¤€ìœ¼ë¡œ 5ê°œì”© ëŠì–´ì„œ plot í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\nheightëŠ” facetì˜ ë†’ì´(ì¸ì¹˜)ë¥¼ ì •í•©ë‹ˆë‹¤.\naspectëŠ” heightì— ë§ì¶° ë¹„ìœ¨ì„ ì •í•˜êµ¬ìš”.\nê·¸ëŸ¼ ì´ 14ê°œì˜ figureê°€ plotë˜ê³  3x5 facetì´ ìƒì„±ë©ë‹ˆë‹¤!\nsns.relplot(x=\u0026#34;timepoint\u0026#34;, y=\u0026#34;signal\u0026#34;, hue=\u0026#34;event\u0026#34;, style=\u0026#34;event\u0026#34;, col=\u0026#34;subject\u0026#34;, col_wrap=5, height=3, aspect=.75, linewidth=2.5, kind=\u0026#34;line\u0026#34;, data=fmri.query(\u0026#34;region == \u0026#39;frontal\u0026#39;\u0026#34;));   ì—¬ê¸°ê¹Œì§€ Part 1ì˜ A ë‚´ìš©ì´ì˜€ìŠµë‹ˆë‹¤.\nê·¸ëŸ¼ \u0026ldquo;ì–´ë–¤ ë‚´ìš©ì´ ë¶€ì¡±í•´ìš”!\u0026rdquo;, \u0026ldquo;ì´ ë‚´ìš© ì˜ ëª¨ë¥´ê² ì–´ìš”!\u0026rdquo; í•˜ëŠ” ë¶€ë¶„ì„ ë‚¨ê²¨ì£¼ì‹œë©´ ì¶”ê°€í•˜ë„ë¡ í• ê²Œìš”!\nê°ì‚¬í•©ë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/part1a/","tags":["Usage"],"title":"Seaborn Tutorial Part 1-A"},{"categories":["Living"],"contents":"ì•ˆë…•í•˜ì„¸ìš”! Jerry ì…ë‹ˆë‹¤!\nì˜¤ëŠ˜ì€ .. Atom ì—ë””í„°ë¥¼ ì™„ë²½íˆ ì§€ìš°ëŠ” ë²•ì— ëŒ€í•´ ê°„ë‹¨ í¬ìŠ¤íŒ…ì„ í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\n Applications ë””ë ‰í† ë¦¬ì—ì„œ Atomì„ ì§€ìš´ë‹¤. home ë””ë ‰í† ë¦¬ì—ì„œ .atom ë””ë ‰í† ë¦¬ë¥¼ ì§€ìš´ë‹¤. /usr/local/bin/atom ì„ ì§€ìš´ë‹¤. /usr/local/bin/apm ì„ ì§€ìš´ë‹¤. ~/Library/Preferences/com.github.atom.plist ì„ ì§€ìš´ë‹¤. ~/Library/Preferences/com.github.atom.helper.plist ì„ ì§€ìš´ë‹¤.  ëì…ë‹ˆë‹¤..\nAtom ì— ëŒ€í•œ ëª¨~~ë“ ê±¸ ì§€ìš´ê±°ì—ìš”!\nê°ì‚¬í•©ë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/macbook_auto_start/","tags":["Macbook"],"title":"ë§¥ë¶ Atom ì™„ë²½ ì œê±°"},{"categories":["DeepLearning"],"contents":"You Only Look Once: Unified, Real-Time, Object detection   í¸ì˜ìƒ bounding box -\u0026gt; bbox\n Abstract  Object detectionì„ bboxes, class probabilities regression ë¬¸ì œë¡œ ì ‘ê·¼. Single Neural Network ë¡œ bboxesì™€ class probabilities ë‘˜ ë‹¤ ì˜ˆì¸¡. Localization Errorê°€ ë‹¤ì†Œ ë†’ì§€ë§Œ background errorê°€ ë‚®ìŒ. End-to-End, Extremely fast  1. Introduction  Current Detection ì‹œìŠ¤í…œ  DPM : Sliding Window ë°©ì‹ R-CNN ê³„ì—´ : potential bboxes ì¶”ì¶œ -\u0026gt; image ì—ì„œ bboxes ë¶€ë¶„ ë‹¤ì‹œ classify -\u0026gt; Post-processing   Slow, Hard to optimize(ê° ìš”ì†Œë³„ë¡œ ë”°ë¡œë”°ë¡œ í•™ìŠµì„ í•´ì•¼í•¨. RPN -\u0026gt; Classifier -\u0026gt; RPN -\u0026gt; \u0026hellip;)      ë³µì¡í•˜ì§€ ì•Šì€ pipelineê³¼ ë¹ ë¥¸ inferenc time.\n 45 fps on a Titan X .    ì´ë¯¸ì§€ ì „ì²´ë¥¼ ì´ìš©í•œ prediction\n Fast R-CNN ë³´ë‹¤ ì ì€ background error    Objectì˜ generalí•œ representations í•™ìŠµ.\n natural imageë¡œ í•™ìŠµí•˜ê³  art worksë¡œ test í–ˆì„ ë•Œ ê¸°ì¡´ì˜ DPM ì´ë‚˜ R-CNN ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•˜ë‹¤. ìƒˆë¡œìš´ ë„ë©”ì¸ì´ë‚˜ ë­”ê°€ ëª¨ë¥¼ ì…ë ¥ì— ëŒ€í•´ ì¼ë°˜í™” í•  ìˆ˜ ìˆë‹¤.    2. Unfied Detection  Single Neural Networkë¡œ í†µí•©. ì´ë¯¸ì§€ ì „ì²´ë¥¼ ì´ìš©í•´ì„œ ê°ê°ì˜ bbox ì˜ˆì¸¡.      ì…ë ¥ ì´ë¯¸ì§€ë¥¼ S x S gridë¡œ ë‚˜ëˆ”.\n Feature mapì´ S x S ë¼ê³  ì´í•´í•˜ë©´ ë¨.    ê° grid cellì€ Bê°œì˜ bboxì˜ ì •ë³´(x, y, w, h, confidence score), í•´ë‹¹ grid cellì˜ Class probabilities ì •ë³´ë¥¼ ê°€ì§.\n  bboxì˜ ì •ë³´\n x, y : bboxì˜ center ì¢Œí‘œ w, h : ì´ë¯¸ì§€ í¬ê¸°ì— ëŒ€ë¹„í•œ ìƒëŒ€ì ì¸ ê°’. confidence score : bboxê°€ objectë¥¼ ê°€ì¡ŒëŠ”ì§€ boxê°€ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì˜ˆì¸¡í–ˆëŠ”ì§€ì— ëŒ€í•œ score Confidence ScoreëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜. $${Confidence; Score}= Pr(Object) * IOU^{truth}_{pred}$$ No objectì˜ ê²½ìš°  Confidence ScoreëŠ” 0 ì´ì–´ì•¼í•¨.   Confidence Scoreê°€ IOUì™€ ê°™ì•„ì§€ê¸¸ ì›í•¨.    Class Probabilities ì •ë³´\n  Cê°œì˜ classì— ëŒ€í•œ conditional class probabilities, \\(Pr(Class_i \\mid Object)\\)\n  Test ì‹œì—ëŠ” Conditional class probabilitiesì™€ individual box confidence scoreë¥¼ ê³±í–ˆë‹¤ê³  í•¨.\n\\(Pr(Class_i|Object) = Pr(Object) * IOU^{truth}_{pred} = Pr(Class_i) * IOU^{truth}_{pred}\\)\n  bbox ë³„ë¡œ class confidence scoreë¥¼ ì•Œ ìˆ˜ ìˆìŒ.\n      PASCAL VOC ë¡œ í‰ê°€. S = 7, B = 2, C = 20\n  ìµœì¢… ì¶œë ¥ì€ 7 x 7 x 30 ì˜ tensor.\n  2.1 Design  24ê°œì˜ Convolution layer, 2ê°œì˜ Fully Connected layer. GoogLeNetì˜ inception ëª¨ë“ˆ ëŒ€ì‹ ì— 1 x 1 Convolution layerë¥¼ ì´ìš©í•˜ì—¬ reduction.   Tiny modelì€ 9ê°œì˜ Convolution layer, 2ê°œì˜ Fully Connected layer.  2.2 Training   ì• ë‹¨ì˜ 20ê°œì˜ Convolution layer(Feature Extractor)ë¥¼ ImageNet 1000-class competition ë°ì´í„°(224 x 224)ë¡œ Pretrain.\n  20ë²ˆì§¸ Convolution layer ë’¤ì— Average Pooling, Fully Connected Layer.\n  ImageNet 2012 validation setìœ¼ë¡œ top-5 accuracy 88% ì •ë„..     Pretrain í›„ Detector ë¶€ë¶„ ì¶”ê°€ í›„ ì…ë ¥ í¬ê¸°ë¥¼ 448 x 448 ë¡œ ë†’ì—¬ì„œ í•™ìŠµ ì§„í–‰.\n  Bounding Boxì˜ width, height ê°’ì€ ì´ë¯¸ì§€ì˜ width, heightë¡œ normalize í•˜ì—¬ 0 ~ 1 ì‚¬ì´ ê°’ì„ ê°™ë„ë¡ í•¨.\n  Bounding Boxì˜ x, y ê°’ì€ íŠ¹ì • grid cellì˜ left topìœ¼ë¡œë¶€í„° offset ê°’. 0 ~ 1 ì‚¬ì´ ê°’ì„ ê°™ë„ë¡ í•¨.\n  ë§ˆì§€ë§‰ layerëŠ” linear activation function ì‚¬ìš©.\n  ë‹¤ë¥¸ layerëŠ” leaky ReLU ì‚¬ìš©. $$\\phi(x)=\\begin{cases}x,\u0026amp;if;x \u0026gt;0\\\\ {0.1}x, \u0026amp; otherwise\\end{cases}$$\n  Optimizationì´ ì‰¬ìš´ Sum-Squared Error ë¥¼ ì‚¬ìš©.\n  ì´ë¯¸ì§€ì˜ ëŒ€ë¶€ë¶„ grid cellì´ object ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— Confidence Scoreê°€ 0 ì— ìˆ˜ë ´.\n  ì´ ìƒí™©ì—ì„  objectë¥¼ ê°€ì§€ê³  ìˆëŠ” grid cellì˜ gradientë¥¼ ì••ë„í•  ìˆ˜ ìˆìŒ.\n  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Bbox coordinate lossì™€ No objectì˜ confidence loss ì— ëŒ€í•´ weight ë¥¼ ë¶€ì—¬.\n\\(\\lambda_{coord} = 5\\) and \\(\\lambda_{noobj} = 0.5\\).\n  Sum-Squared ErrorëŠ” large boxesì™€ small boxes ë¥¼ ë™ì¼í•˜ê²Œ í‰ê°€.\n  large boxes ì— ëŒ€í•´ì„œ ì¤‘ìš”ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ width, height ëŠ” square root ì‚¬ìš©.\n  $$\\lambda_{coord}\\sum^{S^2}_{i=0}\\sum^B_{j=0}\\mathbb{I}^{obj}_{ij}(x_i-\\hat{x}_i)^2+(y_i-\\hat{y}_i)^2$$ $$+\\lambda_{coord}\\sum^{S^2}_{i=0}\\sum^B_{j=0}\\mathbb{I}^{obj}_{ij}(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2$$ $$+ \\sum^{S^2}_{i=0}\\sum^B_{j=0}\\mathbb{I}^{obj}_{ij}(C_i - \\hat{C}_i)^2$$ $$ + \\lambda_{noobj}\\sum^{S^2}_{i=0}\\sum^B_{j=0}\\mathbb{I}^{noobj}_{ij}(C_i - \\hat{C}_i)^2 $$ $$ + \\sum^{S^2}_{i=0}\\mathbb{I}^{obj}_{i}\\sum^B_{c\\in{classes}}(p_i(c) - \\hat{p}_i(c))^2 $$\n  \\(\\mathbb{I}^{obj}_{i}\\) : Objectê°€ ì¡´ì¬í•˜ëŠ” Grid Cell i.\n  \\(\\mathbb{I}^{obj}_{ij}\\) : Objectê°€ ì¡´ì¬í•˜ëŠ” Grid Cell iì˜ Bounding Box j.\n  Train ê´€ë ¨ Parameter\n Batch Szie : 64 Momentum : 0.9 Decay : 0.0005 Learning rate  \\(10^{-3}\\) ë¶€í„° \\(10^{-2}\\) ê¹Œì§€ ì²œì²œíˆ ì¦ê°€. \\(10^{-2}\\) ë¡œ 75 epochs í•™ìŠµ. \\(10^{-3}\\) ë¡œ ì¤„ì—¬ì„œ 30 epochs í•™ìŠµ. \\(10^{-4}\\) ë¡œ ì¤„ì—¬ì„œ 30 epochs í•™ìŠµ.   Dropout : 0.5 Data Augmentation  Random Scaling, Translation of up to 20% of the original image size. Random adjustment exposure and saturation      2.3 Inference  Large Object ë‚˜ ì—¬ëŸ¬ grid cellì— ê±¸ì³ìˆëŠ” objectëŠ” ì—¬ëŸ¬ ì…€ì— predict ë  ìˆ˜ ìˆìŒ. Non-maximal suppression ì‚¬ìš©í•˜ì—¬ í•´ê²°.  2.4 Limitations of YOLO  ê° Grid Cellì€ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë§Œ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— Grid Cell í•˜ë‚˜ì— ì‘ì€ objectê°€ ì—¬ëŸ¬ ê°œ ìˆì„ë•Œ ì œëŒ€ë¡œ ì˜ˆì¸¡í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤. ì˜ˆìƒì¹˜ ëª»í•œ aspect ratioë‚˜ configurationì„ ê°€ì§„ ê°ì²´ë¥¼ ì¼ë°˜í™” í•˜ëŠ”ë° ì–´ë ¤ì›€. Large boxì™€ Small boxë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬. Large boxì˜ small errorë³´ë‹¤ Small boxì˜ small errorê°€ IOUì— í›¨ì”¬ í° ì˜í–¥ì„ ë¼ì¹¨.  3. Comparison to Other Detection Systems  ìƒëµ  4. Experiments 4.1. Comparison to Other RealTime Systems  Fast YoloëŠ” ê°€ì¥ ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì—¬ì¤Œ. YOLO ëŠ” real-time ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©´ì„œ mAPë„ ë›°ì–´ë‚œê±¸ í™•ì¸í•  ìˆ˜ ìˆìŒ.     4.2. VOC 2007 Error Analysis  Object Localizationì€ Fast R-CNNì´ ë” ë›°ì–´ë‚¨. But, Background Error(False Positive)ê°€ í›¨ì”¬ ë†’ìŒ.     4.3. Combining Fast RCNN and YOLO  Fast R-CNNê³¼ YOLOë¥¼ ì•™ìƒë¸” í•œ ëª¨ë¸ì´ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ìŒ.     4.4. VOC 2012 Results   4.5 Generalizability  ìƒˆë¡œìš´ ë„ë©”ì¸, ì˜ˆìƒì¹˜ ëª»í•œ ì…ë ¥ì´ ë“¤ì–´ì™”ì„ë•Œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë›°ì–´ë‚¨. Picasso Dataset ê³¼ People-Art Datasetì„ ì´ìš©í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ ë¹„êµ. YOLOê°€ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ê²ƒì„ ë³´ì—¬ì¤Œ.       ","permalink":"https://jjerry-test.github.io/blog/yolo/","tags":["Paper"],"title":"Review: YOLO"},{"categories":["Living"],"contents":"ëŠ¦ì—ˆì§€ë§Œ í‚¤ë³´ë“œ ìë‘\u0026hellip;\në°”ë°€ë¡œ ì €ì†ŒìŒí‘ì¶• 108í‚¤ ì…ë‹ˆë‹¤.\në„˜ë‚˜ ì˜ë¡±í•˜ë‹¤\u0026hellip;\n    ","permalink":"https://jjerry-test.github.io/blog/keyboard/","tags":["Hardware"],"title":"ì²« ê¸°ê³„ì‹ í‚¤ë³´ë“œ !"},{"categories":["Python"],"contents":"ê±°ì˜ í•œë‹¬ ë°˜\u0026hellip;ë§Œì— ê¸€ì„ ì”ë‹ˆë‹¤..!\nì´ë²ˆì—ëŠ” Pythonì—ì„œ NIfTI í¬ë§·ì˜ ë°ì´í„°ë¥¼ load í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ í¬ìŠ¤íŒ…ì„ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.\nì €ëŠ” Anacondaë¥¼ ì‚¬ìš©ì¤‘ì´ê¸°ì—..ì œ ì‚¬ìš©í™˜ê²½ì— ë§ê²Œ ì„¤ëª…ì„ í•˜ê² ìŠµë‹ˆë‹¤.\nê°€ì¥ ë¨¼ì € ê´€ë ¨ íŒ¨í‚¤ì§€ì¸ Nibabel ì„ ì„¤ì¹˜ë¥¼ í•´ì¤ë‹ˆë‹¤.\nconda install -c conda-forge nibabel ì´ëŸ¬ë©´ ì„¤ì¹˜ëŠ” ëì…ë‹ˆë‹¤.\nimport nibabel as nib from matplotlib import pyplot as plt data = nib.load(\u0026#34;.nii ê²½ë¡œ\u0026#34;) img = data.get_data() #plt.imshow(img)# ìŠ¬ë¼ì´ìŠ¤ 1ì¥ì¼ ê²½ìš° plt.imshow(img[:,:,\u0026#34;slice ë²ˆí˜¸\u0026#34;]) plt.show() ì´ëŸ° ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\nì˜ˆì‹œë¥¼ ë³´ì—¬ë“œë¦¬ë©´\n  ì´ë ‡ìŠµë‹ˆë‹¤!\nì¶”í›„ì—” DICOM ë‹¤ë£¨ëŠ” ë²•ì— ëŒ€í•´ì„œ ì—…ë¡œë“œ í•´ë³´ê² ìŠµë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/nifti/","tags":["Usage"],"title":"Pythonìœ¼ë¡œ NIfTI ì˜ìƒì„ ì½ì–´ë³´ì!"},{"categories":["Ubuntu"],"contents":"ëŒ€ë¶€ë¶„ì´ ê·¸ë ‡ê² ì§€ë§Œ ë”¥ëŸ¬ë‹, ë¨¸ì‹ ëŸ¬ë‹ í•˜ì‹œëŠ” ë¶„ë“¤ì€ ìš°ë¶„íˆ¬ ì„œë²„ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.\në¬¼ë¡  ì €ë„ ê·¸ë ‡êµ¬ìš”. ê°€ë” íŒŒì¼ì„ ë¡œì»¬ì— ë³µì‚¬í•  ì¼ì´ ìˆìŠµë‹ˆë‹¤.\nmobaXterm ì´ë¼ë˜ê°€ xshell ê°™ì€ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ì˜®ê¸¸ ìˆ˜ë„ ìˆì§€ë§Œ ì œ ê²½í—˜ìƒ ëŒ€ìš©ëŸ‰ì´ë¼ë˜ê°€ íŒŒì¼ì´ ë§ë‹¤ë©´ ëŠê¸°ë”êµ¬ìš”..\nê·¸ë˜ì„œ CLI í™˜ê²½ì—ì„œ í•˜ëŠ” ë°©ë²•ì„ ì°¾ì•„ë³´ë‹¤ê°€ scp ë¼ëŠ” ëª…ë ¹ì–´ê°€ ìˆë”êµ°ìš”.\në°©ë²•ì€ ê°„ë‹¨í•©ë‹ˆë‹¤.\nscp -r (ê³„ì •ì´ë¦„)@(IP):(ë³µì‚¬í•  íŒŒì¼ ê²½ë¡œ) (ì €ì¥í•  ê²½ë¡œ) ì´ë ‡ê²Œ í•˜ë©´ ëë‚˜ë”êµ°ìš”. scpì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ì¶”í›„ì— ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤.\n","permalink":"https://jjerry-test.github.io/blog/scp/","tags":["Command"],"title":"scpë¥¼ ì´ìš©í•´ì„œ íŒŒì¼ or ë””ë ‰í† ë¦¬ ë³µì‚¬í•˜ê¸°!"},{"categories":["DeepLearning"],"contents":"ì•ˆë…•í•˜ì„¸ìš”!\nì˜¤ëŠ˜ì€ TensorFlow ì„¤ì¹˜ ë°©ë²•ì„ ê°„ë‹¨í•˜ê²Œ ì•Œë ¤ë“œë¦¬ë ¤ê³  í•©ë‹ˆë‹¤.\nCPUì™€ GPU ì¤‘ì—ì„œë„ GPU ë²„ì „ ì„¤ì¹˜ì— ëŒ€í•´ ì•Œë ¤ë“œë¦´ê±°ì—ìš”!\në§¤ìš° ê°„ë‹¨í•˜ë‹ˆ ë†€ë¼ì§€ë§ˆì‹œê¸° ë°”ëë‹ˆë‹¤.\n Anaconda ë¥¼ ì„¤ì¹˜í•œë‹¤. í„°ë¯¸ë„ or CMDì— conda install python=3.6 ë¼ê³  ì…ë ¥í•œë‹¤.   CMDì—ì„œ ì•ˆë ê²½ìš° Anaconda promptì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.  conda install tensorflow-gpu ë¼ê³  ì…ë ¥í•œë‹¤. ìì‹ ì´ ì›í•˜ëŠ” IDEë¡œ ì½”ë”©ì„ í•œë‹¤.  ì´ë ‡ê²Œ ëì…ë‹ˆë‹¤.\nAnaconda ì´ìš©ì‹œ ë¡œì»¬ì— ì¿ ë‹¤ë¥¼ ì„¤ì¹˜ ì•ˆí•´ì¤˜ë„ ë©ë‹ˆë‹¤.\nì°¸ ì‰½ì£ \u0026hellip;.?\në¬¸ì œê°€ ìˆë‹¤ë©´ ëŒ“ê¸€ ë‚¨ê²¨ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/anaconda_setting/","tags":["TensorFlow"],"title":"Anacondaë¥¼ ì´ìš©í•˜ì—¬ TensorFlow ì„¤ì¹˜í•˜ê¸°!"},{"categories":["Living"],"contents":"ì €ëŠ” í„°ë¯¸ë„ì„ ë§ˆìŒëŒ€ë¡œ ë°”ê¾¸ëŠ”ê±¸ ì¢‹ì•„í•©ë‹ˆë‹¤.\nê¸°ë³¸ í„°ë¯¸ë„ì„ ëŒ€ì²´í•  ì•±ì´ ìˆê¸´í•˜ì§€ë§Œ ì¶”ê°€ì ìœ¼ë¡œ ì„¤ì¹˜í•˜ëŠ”ê±¸ ì•ˆì¢‹ì•„í•´ì„œ..ã…ã…\nê·¸ë˜ì„œ Customizing í•˜ëŠ” ë²•ì„ í¬ìŠ¤íŒ…í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\nMac í„°ë¯¸ë„ì—ì„œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì€ .bash_profile ì—ì„œ í•©ë‹ˆë‹¤. Ubuntu ì—ì„œ .bashrcì™€ ê°™ë‹¤ê³  ìƒê°í•˜ì‹œë©´ ë˜ìš”.\n# terminal customize # ì»¬ëŸ¬ ì ìš© ì—¬ë¶€ export CLICOLOR=1 # ë””ë ‰í† ë¦¬ì˜ ìƒìƒ export LSCOLORS=GxFxCxDxBxegedabagaced # ìœ ì € ë° í˜¸ìŠ¤íŠ¸ì˜ ìƒ‰ìƒ export PS1=\u0026#39;\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;35m\\]\\w\\[\\033[00m\\]\\$\u0026#39; ì²«ë²ˆì§¸ ì˜µì…˜ì€ 1ì´ë©´ Customizingì‘ í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\në‘ë²ˆì§¸ ì˜µì…˜ì´ ì¢€ ë¬¸ì œì¸ë°ìš”..\nì•Œì•„ë³´ê¸° ë§¤ìš° í˜ë“­ë‹ˆë‹¤..\nì €ë„ ê¸°ì–µí•˜ê¸° í˜ë“¤ì–´ì„œ í¬ìŠ¤íŒ…ì„ í•˜ëŠ”ê±°ì£ .\nì..ë‘ë²ˆì§¸ ì˜µì…˜ì„ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\nGx Fx Cx Dx Bx 1x 2x 3x 4x 5x 1x : ë””ë ‰í† ë¦¬ ìƒ‰ìƒ 2x : symbolic link ìƒ‰ìƒ 3x : socket ìƒ‰ìƒ 4x : pipe ìƒ‰ 5x : ì‹¤í–‰íŒŒì¼ ìƒ‰ìƒ eg ed ab ag ac ed 1a 2b 3c 4d 5e 6f 1a : block special ìƒ‰ìƒ 2b : char special ìƒ‰ìƒ 3c : exe_setuid ìƒ‰ìƒ 4d : ext_setgid ìƒ‰ìƒ 5e : a-dir_writeothers_sticky ìƒ‰ìƒ 6f : b-dir_writeothers_NOsticky ìƒ‰ìƒ ì†”ì§íˆ socket, pipe, 1a ~ 6f ê¹Œì§€ëŠ” ë­”ì§€ ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤..\nì œëŒ€ë¡œ ë°°ìš°ì§€ ì•Šì•„ì„œ\u0026hellip;ã…ã…\nìƒ‰ìƒ ì„¤ëª…\na black b red c green d brown e blue f magenta g cyan h light grey A bold black B bold red C bold green D bold brown (ê±°ì˜ ë…¸ë€ìƒ‰) E bold blue F bold magenta G bold cyan H bold light grey (ê±°ì˜ í°ìƒ‰) x default foreground or background ëª¨ë“  ìƒ‰ìƒì„¤ì •ì€ ì•ŒíŒŒë²³ ë‘ê°œë¡œ êµ¬ì„±ì´ ë©ë‹ˆë‹¤.\nì•ì— ì•ŒíŒŒë²³ì€ ê¸€ìì˜ ìƒ‰ìƒì´ê³  ë’¤ì— ì•ŒíŒŒë²³ì€ ë°°ê²½ì˜ ìƒ‰ìƒì…ë‹ˆë‹¤.\nì„¸ë²ˆì§¸ ì˜µì…˜ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\nì´ ì˜µì…˜ ë˜í•œ ë³µì¡í•˜ê²Œ ì¨ìˆë„¤ìš”..\n'\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;35m\\]\\w\\[\\033[00m\\]\\$' ì´ê±´ í„°ë¯¸ë„ì„ ì¼°ì„ë•Œ ìœ ì €ì™€ PC ì´ë¦„ì„ ì–´ë–»ê²Œ ë³´ì—¬ì¤„ ê²ƒì¸ê°€ë¥¼ ì •í•©ë‹ˆë‹¤.\n'\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\] ì—ì„œ {USER_NAME}@{PC_NAME}ì„ ì´ˆë¡ìƒ‰ ë³¼ë“œì²´ë¡œ í•˜ê² ë‹¤ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. (01 : ë³¼ë“œì²´, 32m : ì´ˆë¡ìƒ‰)\n\\[\\033[01;35m\\]\\w\\[\\033[00m\\] ì—ì„œ ~ì„ ìì£¼ìƒ‰ ë³¼ë“œì²´ë¡œ í•˜ê² ë‹¤ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. (01 : ë³¼ë“œì²´, 35m : ìì£¼ìƒ‰)\nìì„¸í•œ ì •ë³´ëŠ” ì¶”í›„ì— ë” ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/terminal/","tags":["Macbook"],"title":"Mac Terminal Customizing"},{"categories":["Blog"],"contents":"ì•ˆë…•í•˜ì„¸ìš”! Jerry ì…ë‹ˆë‹¤.\nìš”ì¦˜ ê¹ƒí—™ìœ¼ë¡œ ë¸”ë¡œê·¸ë¥¼ í•˜ë ¤ê³  í•˜ì‹œëŠ” ë¶„ì´ ë§ì€ë°ìš”!\ní•œë²ˆ ë§Œë“œëŠ” ë²•ì„ í¬ìŠ¤íŒ…í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤!\nê¹ƒí—™ ë¸”ë¡œê·¸ëŠ” ë³´í†µ Jekyll ì´ë¼ëŠ” ì •ì  ì‚¬ì´íŠ¸ ìƒì„±ê¸°ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.\nìì„¸í•œ ì„¤ëª…ì€ ì—¬ê¸°ì„œ í™•ì¸ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\në¸”ë¡œê·¸ ë§Œë“œëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ê°€ ìˆëŠ”ë°ìš”.\n  í…Œë§ˆë¥¼ ìê¸°ê°€ ë§Œë“œëŠ” ë°©ë²•.\n  ê³µê°œëœ í…Œë§ˆë¥¼ ê°€ì ¸ì™€ì„œ ìˆ˜ì •í•˜ëŠ” ë°©ë²•.\n  ì „ 2ë²ˆ ë°©ë²• ìœ¼ë¡œ í–ˆìŠµë‹ˆë‹¤.\nì™œëƒí•˜ë©´ ì „ ì›¹ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ëª¨ë¥´ë‹ˆê¹Œìš”..ã…ã…\nê·¸ë˜ì„œ 2ë²ˆ ë°©ë²• ì— ëŒ€í•´ í¬ìŠ¤íŒ…í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\n1. ì›í•˜ëŠ” í…Œë§ˆ ë‹¤ìš´ë°›ê¸°. í…œí”Œë¦¿ ëª¨ìŒ1 í…œí”Œë¦¿ ëª¨ìŒ2\nìœ„ ë§í¬ëŠ” í…œí”Œë¦¿ì— ëŒ€í•œ ì •ë³´ë¥¼ ëª¨ì•„ë†“ì€ ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤.\në§í¬ì— ìì‹ ì´ ì›í•˜ëŠ” í…œí”Œë¦¿ì„ ë“¤ì–´ê°€ë³´ë©´ ëŒ€ë¶€ë¶„ ê¹ƒí—™ repository ë¡œ ì—°ê²°ë©ë‹ˆë‹¤.\nì˜ˆì‹œë¡œ í…œí”Œë¦¿ ëª¨ìŒ1 ì— ìˆëŠ” Prologue ë¼ëŠ” í…Œë§ˆë¥¼ ì ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.\n  Downloadë¥¼ ë°”ë¡œ ëˆ„ë¥´ì…”ë„ ë˜ê³  Homepageì— ë“¤ì–´ê°€ì„œ git clone, Download ZIP í•˜ì…”ë„ ìƒê´€ì—†ìŠµë‹ˆë‹¤.\në‹¤ìš´ë¡œë“œ ë°›ì€ í›„ì— ì•Œì§‘ì„ í’€ì–´ì£¼ì„¸ìš”!\nì „ Homeì— í’€ì—ˆêµ¬ìš”.\n    ê·¸ í´ë” ì•ˆì—ëŠ” ì´ë ‡ê²Œ êµ¬ì„±ì´ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n2. ë£¨ë¹„ ì ¬ ì„¤ì¹˜. í…œí”Œë¦¿ì„ ë§Œë“¤ë©´ì„œ ë°”ë¡œ ë°”ë¡œ ìˆ˜ì •ë˜ëŠ” ì‚¬í•­ì„ ë³¼ ìˆ˜ ìˆë‹¤ë©´ ì¢‹ê² ì£ ?\nê·¸ë ‡ê¸° ë•Œë¬¸ì— ë£¨ë¹„ ì ¬ ì´ë¼ëŠ” ê²ƒì„ ì„¤ì¹˜ í•´ì•¼í•©ë‹ˆë‹¤.\nOS ë³„ ì„¤ì¹˜ë²•ì€ ì—¬ê¸°ì„œ í™•ì¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\nì €ëŠ” Mac OS ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.\ngem install jekyll bundler ê¹Œì§€ ì§„í–‰í•˜ì„¸ìš”.\n3. localhost ì„œë²„ ì—´ê¸°. ë‹¤ìŒê³¼ ê°™ì´ í„°ë¯¸ë„ì„ ì—´ê³  ì••ì¶•í‘¼ ê²½ë¡œë¡œ ì´ë™í•´ì¤ë‹ˆë‹¤.\n  ê·¸ë¦¬ê³  bundle install ì´ë¼ê³  ì…ë ¥í•©ë‹ˆë‹¤.\n  ê·¸ëŸ¬ë©´ ë­ ì´ê²ƒ ì €ê²ƒ ì„¤ì¹˜ê°€ ë ê±°ì—ìš”.\n  ì„œë²„ë¥¼ ì—´ ì¤€ë¹„ëŠ” ëë‚¬ìŠµë‹ˆë‹¤.\nì´ì œ í„°ë¯¸ë„ì— bundle exec jekyll serve ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”.\n  ì‚¬ì§„ê³¼ ê°™ì´ ë‚˜ì˜¬ê±°ì—ìš”!\në°‘ì— http://127.0.0.1:4000/jekyll-theme-prologueë¼ê³  ë‚˜ì™€ìˆë„¤ìš”!\nê·¸ëŸ¼ ì¸í„°ë„· ë¸Œë¼ìš°ì €ë¥¼ ì¼œì£¼ì‹œê³  ì£¼ì†Œì°½ì— ì…ë ¥í•´ì£¼ì„¸ìš”!\nê·¸ëŸ¼ ë‹¤ìŒê³¼ ê°™ì€ ì°½ì´ ì—´ë¦½ë‹ˆë‹¤.\n  ì´ì œ ë¸”ë¡œê·¸ë¥¼ í•  ì¤€ë¹„ëŠ” ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!\në‹¤ìŒì—” í…Œë§ˆë¥¼ í† ëŒ€ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•˜ëŠ” í¬ìŠ¤íŒ…ì„ ì¤€ë¹„í•´ë³´ê² ìŠµë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/gitblog/","tags":["Jekyll"],"title":"Jekyll Blog"},{"categories":["Python"],"contents":"ì €ë²ˆ Python ì„¤ì¹˜ í¬ìŠ¤íŒ…ì—ì„œ pyenvë¡œ ì„¤ì¹˜ í•˜ëŠ” ë²•ì„ í¬ìŠ¤íŒ… í–ˆì—ˆìŠµë‹ˆë‹¤.\nì´ë²ˆì—ëŠ” ì œê°€ pyenv ë¡œ ì–´ë–¤ ë²„ì „ì„ ì„¤ì¹˜í–ˆê³  ë¬´ìŠ¨ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í–ˆëŠ”ì§€ í¬ìŠ¤íŒ… í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\në¨¼ì € ì„¤ì¹˜í•  ë²„ì „ì˜ ì´ë¦„ì„ ì •í™•íˆ ì•Œì•„ì•¼í•˜ë¯€ë¡œ ì„¤ì¹˜ ê°€ëŠ¥í•œ ë²„ì „ë“¤ì„ ë´…ë‹ˆë‹¤.\npyenv install --list ì´ë ‡ê²Œ í„°ë¯¸ë„ì— ì…ë ¥ì„ í•˜ë©´\nAvailable versions: 2.1.3 2.2.3 2.3.7 ... ì¤‘ëµ ... stackless-3.4.2 stackless-3.4.7 stackless-3.5.4 ì´ëŸ°ì‹ìœ¼ë¡œ êµ‰~~~ì¥íˆ ë§ì€ ë²„ì „ì´ ìˆìŠµë‹ˆë‹¤. (ì•½ 340ê°œ?)\nê·¸ì¤‘ì—ì„œ ì €ëŠ” anaconda3-5.2.0ì„ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤.\npyenv install anaconda3-5.2.0 pyenv global anaconda3-5.2.0 ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ ì„¤ì¹˜ ë..\nì´ì œ ì œê°€ ì‚¬ìš©í•˜ëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ê² ì£ .\në³´í†µ python ì´ë¼ë©´ pipë¥¼ ì“°ê² ì§€ë§Œ ì €ëŠ” ì•„ë‚˜ì½˜ë‹¤ë¥¼ ì„¤ì¹˜í–ˆì£ .\nê·¸ë˜ì„œ condaë¥¼ ì´ìš©í•´ì„œ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤.\nì œê°€ ì„¤ì¹˜ í•  íŒ¨í‚¤ì§€ëŠ” pytorch, tensorflow, keras, tqdm ì…ë‹ˆë‹¤.\nconda install tqdm # progress bar íŒ¨í‚¤ì§€ conda install keras tensorflow # machine learning íŒ¨í‚¤ì§€ conda install pytorch torchvision -c pytorch # machine learning íŒ¨í‚¤ì§€ ì´ë ‡ê²Œ í•˜ë©´ ëì…ë‹ˆë‹¤.\nnumpy, scipy, matplotlib, ...ë“± íŒ¨í‚¤ì§€ëŠ” anacondaë¥¼ ì„¤ì¹˜í•˜ë©´ì„œ ìë™ìœ¼ë¡œ ì„¤ì¹˜ê°€ ë©ë‹ˆë‹¤ ã…ã…\në§Œì•½ ì € í™˜ê²½ì„ ì‚­ì œí•˜ê³  ì‹¶ë‹¤?\npyenv uninstall anaconda3-5.2.0 ì´ë¼ê³  í•˜ì‹œë©´ ë°”ë¡œ ì‚­ì œë©ë‹ˆë‹¤.\në§ì€ ë¶„ë“¤ê»˜ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤!\nê°ì‚¬í•©ë‹ˆë‹¤~!\n","permalink":"https://jjerry-test.github.io/blog/anaconda/","tags":["Setting"],"title":"Anaconda ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•˜ê¸°!"},{"categories":["Blog"],"contents":"Markdownì€ 2004ë…„ ì¡´ê·¸ë£¨ë²„ì— ì˜í•´ ë§Œë“¤ì–´ì¡Œìœ¼ë©° ì‰½ê²Œ ì“°ê³  ì½ì„ ìˆ˜ ìˆìœ¼ë©° HTMLë¡œ ë³€í™˜ì´ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ë§ˆí¬ì—…ì–¸ì–´ì…ë‹ˆë‹¤. íŠ¹ìˆ˜ê¸°í˜¸ì™€ ë¬¸ìë¥¼ ì´ìš©í•œ ë§¤ìš° ê°„ë‹¨í•œ êµ¬ì¡°ì˜ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ì—ì„œë„ ë¹ ë¥´ê²Œ ì»¨í…ì¸ ë¥¼ ì‘ì„±í•˜ê³  ì§ê´€ì ìœ¼ë¡œ ì¸ì‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Githubì„ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒì´ë¼ë©´ ê°€ì¥ ë¨¼ì € ë§Œë‚˜ê²Œ ë˜ëŠ” íŒŒì¼ì´ README.md íŒŒì¼ì¸ë°ìš”. ì´ íŒŒì¼ë„ Markdownìœ¼ë¡œ ì‘ì„±ëœ íŒŒì¼ì…ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ì„ í†µí•´ì„œ ì„¤ì¹˜ë°©ë²•, ì†ŒìŠ¤ì½”ë“œ ì„¤ëª…, ì´ìŠˆ ë“±ì„ ê°„ë‹¨í•˜ê²Œ ê¸°ë¡í•˜ê³  ê°€ë…ì„±ì„ ë†’ì¼ ìˆ˜ ìˆì–´ì„œ ë§ì€ ì‚¬ëŒë“¤ì´ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤!\nTable of contents  ê¸°ë³¸ ì‚¬ìš©ë²•  Headings   h1 Heading  h2 Heading  h3 Heading  h4 Heading  h5 Heading  h6 Heading         Paragraphs Newline Horizontal Line Emphasis  Bold Italics   Blockquotes Lists  Unordered Ordered Time-saving Tip   Code  Inline code \u0026ldquo;Fenced\u0026rdquo; code block Indented code Syntax highlighting   Links  Autolinks Inline links   Images Raw HTML Escaping with backslashes   ê·¸ ì™¸ ì‚¬ìš©ë²•  Strikethrough Tables  Aligning cells      ê¸°ë³¸ ì‚¬ìš©ë²• Headings Heading ì€ h1 ë¶€í„° h6 ê¹Œì§€ ìˆê³  # ì˜ ê°œìˆ˜ë¡œ ë‹¨ê³„ê°€ ì •í•´ì§‘ë‹ˆë‹¤.\nì‚¬ìš©ë²• :\n# h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading ì ìš© í›„ :\nh1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading  Paragraphs ë¬¸ë‹¨ì„ ë‚˜ëˆ„ëŠ” ë²•ì…ë‹ˆë‹¤. ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” í•˜ë‚˜ ì´ìƒì˜ ë¹ˆ ì¤„ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.\nì‚¬ìš©ë²• :\n\u0026lt;p\u0026gt;ì´ ë¬¸ì¥ì€ ì²«ë²ˆì§¸ ë¬¸ë‹¨ì˜ ì²«ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;ì´ ë¬¸ì¥ì€ ë‘ë²ˆì§¸ ë¬¸ë‹¨ì˜ ì²«ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. ì´ ë¬¸ì¥ì€ ë‘ë²ˆì§¸ ë¬¸ë‹¨ì˜ ë‘ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤.\u0026lt;/p\u0026gt; ì ìš© í›„ :\n Newline ê³µë°± ë‘ì¹¸ í›„ ì—”í„°, \u0026lt;br\u0026gt;ì„ ì´ìš©í•˜ì—¬ ê°œí–‰ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n(\u0026lt;br\u0026gt;ì€ êµ³ì´\u0026hellip;.)\nì‚¬ìš©ë²• :\ntest1`ê³µë°± ë‘ì¹¸ í›„ ì—”í„°` test2 test1\u0026lt;br\u0026gt;test2 ì ìš© í›„ :\ntest1\ntest2\ntest1test2\nHorizontal Line Markdown ë¬¸ì„œì— ìˆ˜í‰ì„ ì„ ì¶”ê°€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì‚¬ìš©ë²• :\n`___`: ë°‘ì¤„(_) 3ê°œ `---`: ëŒ€ì‰¬(-) 3ê°œ `***`: ë³„í‘œ(*) 3ê°œ ì ìš© í›„ :\n    Emphasis Bold ë³¼ë“œì²´ ì ìš©ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n**ì´ë ‡ê²Œ í•˜ë©´ ë³¼ë“œì²´!** ì ìš© í›„ :\nì´ë ‡ê²Œ í•˜ë©´ ë³¼ë“œì²´!\nItalics ì´í…”ë¦­ì²´ ì ìš©ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n_ì´ë ‡ê²Œ í•˜ë©´ ì´í…”ë¦­ì²´!_ ì ìš© í›„ :\nì´ë ‡ê²Œ í•˜ë©´ ì´í…”ë¦­ì²´!\n Blockquotes ë¸”ëŸ­ì¸ìš©ì€ \u0026gt; ìœ¼ë¡œ í•©ë‹ˆë‹¤.\nHeadingê³¼ ì²˜ëŸ¼ \u0026gt; ê°œìˆ˜ë¡œ ì¸ìš© ì•ˆì— ì¸ìš©ì„ ì¶”ê°€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì‚¬ìš©ë²• :\n\u0026gt; ì´ê²ƒì€ ì¸ìš©êµ¬ì—ìš”. ì ìš© í›„ :\n ì´ê²ƒì€ ì¸ìš©êµ¬ì—ìš”.\n ë‹¤ì¤‘ ì¸ìš© ì‚¬ìš©ë²• :\n\u0026gt; ì´ê²ƒì€ ì²«ë²ˆì§¸ ì¸ìš©ì´ì—ìš”. \u0026gt;\u0026gt; ì´ê²ƒì€ ë‘ë²ˆì§¸ ì¸ìš©ì´ì—ìš”. \u0026gt;\u0026gt;\u0026gt; ì´ê²ƒì€ ì„¸ë²ˆì§¸ ì¸ìš©ì´ì—ìš”. ì ìš© í›„ :\n ì´ê²ƒì€ ì²«ë²ˆì§¸ ì¸ìš©ì´ì—ìš”.\n ì´ê²ƒì€ ë‘ë²ˆì§¸ ì¸ìš©ì´ì—ìš”.\n ì´ê²ƒì€ ì„¸ë²ˆì§¸ ì¸ìš©ì´ì—ìš”.\n    Lists ëª©ë¡ì—ëŠ” ìˆœì„œê°€ ì—†ëŠ” ëª©ë¡, ìˆœì„œê°€ ìˆëŠ” ëª©ë¡ì´ ìˆìŠµë‹ˆë‹¤.\nUnordered ìˆœì„œê°€ ì—†ëŠ” ëª©ë¡ì€ *, -, + ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì‚¬ìš©ë²• :\n+ ìŒì‹ + ê°€êµ¬ + ìš´ë™ - ë†êµ¬ - ì¶•êµ¬ - ì•¼êµ¬ ì ìš© í›„ :\n ìŒì‹ ê°€êµ¬ ìš´ë™  ë†êµ¬ ì¶•êµ¬ ì•¼êµ¬    ë˜í•œ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì‚¬ìš©ë²• :\n- ì‹ ì²´ - ë¨¸ë¦¬ - ìƒì²´ - í•˜ì²´ ì ìš© í›„:\n  ì‚¬ëŒ\n ë¨¸ë¦¬  ëˆˆ ì½” ì…   ìƒì²´ í•˜ì²´    ê°œë¯¸\n ë¨¸ë¦¬ ê°€ìŠ´ ë°°    Ordered ìˆœì„œê°€ ìˆëŠ” ëª©ë¡ì„ ë§Œë“¤ ë•ŒëŠ” ë²ˆí˜¸ë¥¼ ë§¤ê¸°ë©´ ë©ë‹ˆë‹¤.\n1. ë…¸íŠ¸ë¶ì„ ì¼ ë‹¤. 2. Atom ì—ë””í„°ë¥¼ ì¼ ë‹¤. 3. í”„ë¡œì íŠ¸ í´ë”ë¥¼ ì—°ë‹¤. 4. ì½”ë”©ì„ í•œë‹¤. ì ìš© í›„ :\n ë…¸íŠ¸ë¶ì„ ì¼ ë‹¤. Atom ì—ë””í„°ë¥¼ ì¼ ë‹¤. í”„ë¡œì íŠ¸ í´ë”ë¥¼ ì—°ë‹¤. ì½”ë”©ì„ í•œë‹¤.  Time-saving Tip ìˆœì„œê°€ ìˆëŠ” ëª©ë¡ì—ì„œ ì‹œê°„ì„ ì•„ë¼ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n1., 2.,\u0026hellip; ë¼ê³  ì ì„ í•„ìš” ì—†ì´ 1. ë§Œ ì ìœ¼ë©´ ìë™ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê¹ë‹ˆë‹¤.\n1. ë…¸íŠ¸ë¶ì„ ì¼ ë‹¤. 1. Atom ì—ë””í„°ë¥¼ ì¼ ë‹¤. 1. í”„ë¡œì íŠ¸ í´ë”ë¥¼ ì—°ë‹¤. 1. ì½”ë”©ì„ í•œë‹¤. ì ìš© í›„ :\n ë…¸íŠ¸ë¶ì„ ì¼ ë‹¤. Atom ì—ë””í„°ë¥¼ ì¼ ë‹¤. í”„ë¡œì íŠ¸ í´ë”ë¥¼ ì—°ë‹¤. ì½”ë”©ì„ í•œë‹¤.   Code Inline code ë¬¸ì¥ ì¤‘ê°„ì—  ë¥¼ ì´ìš©í•˜ì—¬ codeë¥¼ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\níŒŒì´ì¬ì—ì„œ ëª¨ë“ˆì„ ì“°ë ¤ë©´ `import 'ëª¨ë“ˆëª…'` ì´ë¼ê³  í•˜ë©´ ë©ë‹ˆë‹¤. ì ìš© í›„ :\níŒŒì´ì¬ì—ì„œ ëª¨ë“ˆì„ ì“°ë ¤ë©´ import 'ëª¨ë“ˆëª…' ì´ë¼ê³  í•˜ë©´ ë©ë‹ˆë‹¤.\n\u0026ldquo;Fenced\u0026rdquo; code block code blockì„ ë§Œë“¤ì–´ì„œ codeë¥¼ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì ìš© í›„ :\nimport os import sys Indented code ë“¤ì—¬ì“°ê¸°(ê³µë°± 4ê°œ)ë¥¼ ì´ìš©í•˜ì—¬ codeë¥¼ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\ní•˜ì§€ë§Œ ë³„ë¡œ ì¶”ì²œí•˜ëŠ” ë°©ë²•ì€ ì•„ë‹™ë‹ˆë‹¤.\nì™œëƒí•˜ë©´ syntax highlightingì´ ì•ˆë©ë‹ˆë‹¤.\n import sys impot os ì ìš© í›„ :\nimport sys impot os  Syntax highlighting \u0026ldquo;Fenced\u0026rdquo; code block ì€ ì–´ë–¤ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ code block ì„ ë§Œë“¤ê±´ì§€ ì ì–´ì„œ ì‚¬ìš©ì„ í•©ë‹ˆë‹¤.\nê·¸ëŸ¬ë©´ ê·¸ ì–¸ì–´ì— ë§ê²Œ Syntax highlightingì„ í•©ë‹ˆë‹¤.\nWhich renders to:\nimport os import sys a = 2 b = 5 print(\u0026#34;Hello, World!\u0026#34;) print(a + b)  Links ë§í¬ë¥¼ ê±°ëŠ” ë°©ë²•ì€ Autolinks, Inline links, Link titles, \u0026lsquo;Named Anchors\u0026rsquo; ì´ë ‡ê²Œ ë„¤ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\nì´ í¬ìŠ¤íŒ…ì—ì„  Autolinks, Inline links ì´ ë‘ ê°€ì§€ë¥¼ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\nAutolinks \u0026lt;, \u0026gt; ì‚¬ì´ì— ë§í¬ë¥¼ ì ìœ¼ë©´ ìë™ìœ¼ë¡œ ë§í¬ê°€ ìƒì„±ë©ë‹ˆë‹¤.\n\u0026lt;https://jjerry-k.github.io\u0026gt; ì ìš© í›„ :\nhttps://jjerry-k.github.io\nInline links ë¬¸ì¥ ì•ˆì— ë§í¬ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n[Jerry's Blog](https://jjerry-k.github.io) ì ìš© í›„ :\nJerry\u0026rsquo;s Blog\n Images ì´ë¯¸ì§€ë¥¼ ì˜¬ë¦¬ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\nì²«ë²ˆì§¸ ë°©ë²• ![Minion](http://octodex.github.com/images/minion.png) ì ìš© í›„ :\në‘ë²ˆì§¸ ë°©ë²• ![Alt text](http://octodex.github.com/images/stormtroopocat.jpg \u0026quot;The Stormtroopocat\u0026quot;) ì ìš© í›„ :\nìœ„ ë‘ ë°©ë²•ì€ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ì¡°ì ˆì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\në§Œì•½ ì‚¬ì´ì¦ˆ ì¡°ì ˆì´ í•„ìš”í•˜ë‹¤ë©´ \u0026lt;img src=\u0026quot;\u0026quot; height=\u0026quot;\u0026quot; width=\u0026quot;\u0026quot;\u0026gt;ë¥¼ ì´ìš©í•˜ë©´ ë©ë‹ˆë‹¤.\n\u0026lt;img src=\u0026quot;http://octodex.github.com/images/dojocat.jpg\u0026quot; height=\u0026quot;100\u0026quot; width=\u0026quot;100\u0026quot;\u0026gt; ì ìš© í›„ :  Raw HTML ì´ë¯¸ì§€ì—ì„œ HTML ë¬¸ë²•ì„ ì“¸ ìˆ˜ ìˆë‹¤ëŠ”ê±¸ í™•ì¸í–ˆë‹¤ì‹œí”¼ Markdown ì—ì„œ HTML ë¬¸ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì—¬ê¸°ê°€ **\u0026lt;a href=\u0026quot;https://jjerry-k.github.io\u0026quot;\u0026gt;Jerryì˜ ë¸”ë¡œê·¸\u0026lt;/a\u0026gt;**ì…ë‹ˆë‹¤. ì—¬ê¸°ê°€ **[Jerryì˜ ë¸”ë¡œê·¸](https://jjerry-k.github.io)**ì…ë‹ˆë‹¤. ì ìš© í›„ :\nì—¬ê¸°ê°€ Jerryì˜ ë¸”ë¡œê·¸ì…ë‹ˆë‹¤.\nì—¬ê¸°ê°€ Jerryì˜ ë¸”ë¡œê·¸ì…ë‹ˆë‹¤.\n Escaping with backslashes \\ ë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ì¥ì•ˆì— ê¸°í˜¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\*ë¥¼ ë¬¸ì¥ì— ì“°ê³  ì‹¶ë‹¤! ì ìš© í›„ :\n*ë¥¼ ë¬¸ì¥ì— ì“°ê³  ì‹¶ë‹¤!\n ê·¸ ì™¸ ì‚¬ìš©ë²• Strikethrough ë¬¸ì¥ì— ì¤„ì„ ê·¸ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n~~ë¸”ë¡œê·¸....ê·€ì°®ë‹¤....~~ ì ìš© í›„ :\në¸”ë¡œê·¸\u0026hellip;.ê·€ì°®ë‹¤\u0026hellip;.\n Tables í‘œë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. |, -ë¥¼ ì´ìš©í•˜ì—¬ ì‘ì„±í•©ë‹ˆë‹¤.\n| ì´ë¦„ | í•™ì  | | --- | --- | | ì œë¦¬ | B+ | | í†° | C+ | |ìŠ¤íŒŒì´í¬| B0 | ì ìš© í›„ :\n   ì´ë¦„ í•™ì      ì œë¦¬ B+   í†° C+   ìŠ¤íŒŒì´í¬ B0    Aligning cells ì…€ ì •ë ¬ì€ :ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.\nì™¼ìª½ ì •ë ¬ì€ :ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë©ë‹ˆë‹¤.\nCenter text in a column\nì¤‘ì•™ ì •ë ¬ì€ ë‹¤ìŒê³¼ ê°™ì´ í•©ë‹ˆë‹¤.\n| ì´ë¦„ | í•™ì  | | :-: | :-: | | ì œë¦¬ | B+ | | í†° | C+ | |ìŠ¤íŒŒì´í¬| B0 |    ì´ë¦„ í•™ì      ì œë¦¬ B+   í†° C+   ìŠ¤íŒŒì´í¬ B0    Right-align the text in a column\nì˜¤ë¥¸ìª½ ì •ë ¬ì€ ë‹¤ìŒê³¼ ê°™ì´ í•©ë‹ˆë‹¤.\n| ì´ë¦„ | í•™ì  | | --: | --: | | ì œë¦¬ | B+ | | í†° | C+ | |ìŠ¤íŒŒì´í¬| B0 | ì ìš© í›„ :\n   ì´ë¦„ í•™ì      ì œë¦¬ B+   í†° C+   ìŠ¤íŒŒì´í¬ B0    Markdown ì„¤ëª…ì— ëŒ€í•œ í¬ìŠ¤íŒ…\u0026hellip; ë!\n","permalink":"https://jjerry-test.github.io/blog/markdown/","tags":["Markdown"],"title":"Markdown ì‚¬ìš©ë²•"},{"categories":["DeepLearning"],"contents":"ì•ˆë…•í•˜ì„¸ìš”! Jerry ì…ë‹ˆë‹¤!\nì–´ì œ pip ë¡œ ì„¤ì¹˜í•˜ì§€ ë§ë¼êµ¬ìš”!? ë¼ëŠ” í¬ìŠ¤íŒ…ì„ í–ˆì£ ?\ní¬ìŠ¤íŒ…ì„ í•˜ê³  ê°‘ìê¸° í™• ê½‚í˜€ì„œ anaconda í™˜ê²½ì„ ì§€ì› ìŠµë‹ˆë‹¤.\nSetting ì„ ë‹¤ì‹œ í•˜ê¸° ìœ„í•´ì„œìš”! í•³í•³í•³\nê·¸ë˜ì„œ ì§€ìš°ê³  ë‹¤ì‹œ êµ¬ì¶•í–ˆì£ .\nì œê°€ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ Python Installation for mac ì— ì í˜€ìˆìŠµë‹ˆë‹¤!\nì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ ì„¤ì¹˜ í›„ì— ì´ì œ python packageë¥¼ ì„¤ì¹˜í•´ì•¼ê² ì£ ?\npip ê°€ ì•„ë‹Œ condaë¡œ ì „ë¶€ ì„¤ì¹˜í–ˆìŠµë‹ˆë‹¤.\nconda install tensorflow conda install pytorch torchvision -c pytorch conda install keras ê·¸ í›„ì— í…ŒìŠ¤íŠ¸ë¥¼ í•˜ê³ ì í„°ë¯¸ë„ì—ì„œ tensorflowë¥¼ import í•´ë´¤ìŠµë‹ˆë‹¤.\nimport tensorflow ê·¸ í›„ì— ë‚˜ì˜¨ warning ì…ë‹ˆë‹¤.\n/Users/jerry/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\në¼ê³  ë‚˜ì˜¤ë”êµ°ìš”.\nì´ê²Œ êµ‰ì¥íˆ ê±°ìŠ¬ë¦¬ëŠ” ë¶„ë“¤ ê³„ì‹¤ê²ë‹ˆë‹¤..\në³´í†µ h5py ë¼ëŠ” python íŒ¨í‚¤ì§€ê°€ 2.8.0 ë¯¸ë§Œì´ë©´ ì´ warningì´ ë°œìƒí•´ìš”!\ní•´ê²°ì±…ì€ ê°„ë‹¨í•©ë‹ˆë‹¤!\npip install --upgrade h5py ì´ê²ƒë§Œ í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤!\nê·¸ëŸ¼ ì¦ê±°ìš´ ì½”ë”©!\n","permalink":"https://jjerry-test.github.io/blog/warning/","tags":["TensorFlow"],"title":"Python 3.6 ì—ì„œ ì´ ê²½ê³  ë³´ê¸° ì‹«ì–´ìš”.."},{"categories":["DeepLearning"],"contents":"ì˜¤ëŠ˜ ê°‘ìê¸° ë‹¹í™©ìŠ¤ëŸ¬ìš´ í¬ìŠ¤íŒ…ì„ ë´¤ìŠµë‹ˆë‹¤.\nì¶œì²˜ : https://www.anaconda.com/blog/developer-blog/tensorflow-in-anaconda/\në‚´ìš©ì„ ë³´ë‹ˆ pip ë¡œ ì„¤ì¹˜í•˜ëŠ”ê²ƒ ë³´ë‹¤ condaë¥¼ ì´ìš©í•˜ë©´ ì¢‹ì€ ì ì´ ë‘ ê°€ì§€ê°€ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.\n  CPU ì„±ëŠ¥ì´ ë” ë¹¨ë¼ì¡Œë‹¤.     GPU ë²„ì „ ì„¤ì¹˜ê°€ ì‰½ë‹¤.\nì†”ì§íˆ ì´ê±´ ì˜ ëª¨ë¥´ê² ë„¤ìš”..\në‘˜ ë‹¤ ë˜‘ê°™ì´ ê·¸ë˜í”½ ë“œë¼ì´ë²„ ì„¤ì¹˜í•˜ê³  CUDA ì„¤ì¹˜í•˜ê³  í•´ì•¼í•˜ëŠ”ë° ë­ê°€ ì‰¬ì›Œì§„ë‹¤ëŠ”ê±´ì§€..\në­.. ì–´ë–¤ ì¿ ë‹¤ ë²„ì „ì„ ì„¤ì¹˜í–ˆë˜ ì•Œì•„ì„œ ì¡ì•„ì„œ TensorFlow ë¥¼ ì„¤ì¹˜í•´ì¤€ë‹¤ë©´ í¸í•´ì§„ê±´ ë§ê² êµ°ìš”.\n  ê·¸ë¦¬ê³  CPU ì„±ëŠ¥ ì¦ê°€ëŠ” Intel CPUì— í•œí•´ì„œ ë¹¨ë¼ì§€ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n  ì‚¬ì§„ì„ ë³´ì‹œë©´ the IntelÂ® Math Kernel Library for Deep Neural Networks (IntelÂ® MKL-DNN) ì„ ì‚¬ìš©í•´ì„œ ì˜¬ë ¸ë‹¤ëŠ”ê±°ì£ . (AMD ì‚¬ìš©ì ì¥¬ë¥µ..)\në­ ì•„ë¬´íŠ¼\u0026hellip; ì¸í…”ì— í…í”Œì“°ì‹œëŠ” ë¶„ë“¤ì€ ì½˜ë‹¤ë¡œ ë„˜ì–´ê°€ì‹¬ì´ ì¢‹ì„ ë“¯í•©ë‹ˆë‹¤.\nì „ ìš”ì¦˜ pytorch ë¥¼ ì“°ê³  ìˆì–´ì„œ ã…ã…..\nì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ ã…ã…\n","permalink":"https://jjerry-test.github.io/blog/condavspip/","tags":["TensorFlow"],"title":"pip ë¡œ ì„¤ì¹˜í•˜ì§€ ë§ë¼êµ¬ìš”!?"},{"categories":["Ubuntu"],"contents":"í˜„ì¬ ì €ëŠ” í•˜ë‚˜ì˜ GPU ì„œë²„ë¥¼ ê°€ì§€ê³  ì—¬ëŸ¬ ëª…ì´ Deep Learning ì„ ëŒë ¤ì•¼ í•©ë‹ˆë‹¤.\nê·¸ëŸ¬ë©´ GPU ê°ê°ì„ ë¶„ë°°í•˜ê±°ë‚˜ ë©”ëª¨ë¦¬ë¥¼ ë¶„ì‚°í•´ì•¼ê² ì£ .\nì œ ì—°êµ¬ì‹¤ ê°™ì€ ê²½ìš°ì—” ì „ìë¥¼ íƒí–ˆìŠµë‹ˆë‹¤.\nê·¸ë˜ì„œ ì´ë²ˆì—” íŠ¹ì • GPUë§Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ê°„ ! ë‹¨ ! í•˜ ! ê²Œ í¬ìŠ¤íŒ…í•˜ë ¤ê³  í•©ë‹ˆë‹¤. (Just ëª…ë ¹ì–´ë§Œ ì“¸êº¼ì„.)\nCUDA_VISIBLE_DEVICES=0 python ~~~.py # 0ë²ˆ GPUë§Œ ì‚¬ìš©. #CUDA_VISIBLE_DEVICES=0,3 python ~~~.py # 0, 3ë²ˆ GPU ì‚¬ìš©. ì´ë ‡ê²Œ ì‹¤í–‰í•˜ì‹œë©´ ~~~.py ë¥¼ ì‹¤í–‰í•´ì„œ GPU ë¥¼ ì‚¬ìš©í•  ê²½ìš° 0ë²ˆ GPUë§Œ ì‚¬ìš©í•´ì„œ ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤! ë§ì€ ë¶„ë“¤ì´ TensorFlowë¥¼ ì‚¬ìš©í•˜ì‹¤í…ë° ì´ê²ƒìœ¼ë¡œ ì˜ˆë¥¼ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.\nGPUê°€ ì—¬ëŸ¬ ëŒ€ì¸ ìƒí™©ì—ì„œ ì•„ë¬´ ì˜µì…˜ë„ ì£¼ì§€ ì•Šê³  (ìŠ¤í¬ë¦½íŠ¸ì—ì„œë„ ì•ˆì¤¬ë‹¤ëŠ” ê°€ì •í•˜ì—..) tf.Session() ì„ ì‹¤í–‰í•˜ê²Œ ë˜ë©´ ëª¨ë“  GPUì˜ ë©”ëª¨ë¦¬ë¥¼ í˜¼ìì„œ ë‹¤ ì¡ê³  ìˆëŠ”ê±¸ ë³´ì…¨ì„ ê²ë‹ˆë‹¤.\ní•˜ì§€ë§Œ ë§Œì•½ ìœ„ì— ì½”ë“œ ì²˜ëŸ¼ ì‹¤í–‰í•œë‹¤ë©´ íŠ¹ì • ë²ˆí˜¸ì— í•´ë‹¹í•˜ëŠ” GPUì˜ ë©”ëª¨ë¦¬ë§Œ ì¡ê³  ìˆëŠ”ê±¸ ë³´ì‹¤ ìˆ˜ ìˆì„ ê²ë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/gpu_masking/","tags":["Usage"],"title":"GPUë¥¼ ê³¨ë¼ì„œ ì¨ë³´ì!"},{"categories":["Python"],"contents":"# brew ì„¤ì¹˜. /usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#34; # wget ì„¤ì¹˜. brew install wget #pyenv ì„¤ì¹˜. brew install pyenv # bash_profile ì— ê²½ë¡œ ì¶”ê°€. echo \u0026#39;export PYENV_ROOT=\u0026#34;${HOME}/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=\u0026#34;${PYENV_ROOT}bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;eval \u0026#34;$(pyenv init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc exec $SHELL # ì„¤ì¹˜í•  ìˆ˜ ìˆëŠ” í™˜ê²½ ë³´ì—¬ì¤Œ. pyenv install --list # ìœ„ì—ì„œ í™˜ê²½ í™•ì¸ í›„ ì›í•˜ëŠ” ë²„ì „ ì…ë ¥. pyenv install \u0026lt;ë²„ì „ ì´ë¦„\u0026gt; # system ê³¼ \u0026lt;ë²„ì „ ì´ë¦„\u0026gt; ë‘ í™˜ê²½ì´ ì¡´ì¬. pyenv versions # \u0026lt;ë²„ì „ ì´ë¦„\u0026gt;ìœ¼ë¡œ default version ë³€ê²½. pyenv global \u0026lt;ë²„ì „ ì´ë¦„\u0026gt; pyenv versions # tmp í´ë” ì—ì„  system ì´ë¼ëŠ” ë²„ì „ìœ¼ë¡œ ì‚¬ìš©. mkdir tmp cd tmp pyenv local system pyenv ë¥¼ ì‚¬ìš©í•˜ë©´ íŠ¹ì • í´ë”ì—ì„œëŠ” python2 ë¡œ ì‘ë™í•˜ê³  ê·¸ ì™¸ì—ëŠ” python3 ë¡œ ì‘ë™í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\nê°ì‚¬í•©ë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/python4mac/","tags":["Setting"],"title":"Python Installation for mac"},{"categories":["DeepLearning"],"contents":"ì´ë²ˆì— V.aisì—ì„œ TensorFlow Object Detection API ì‚¬ìš© ë§¤ë‰´ì–¼ì„ ì‘ì„±í•´ë´¤ìŠµë‹ˆë‹¤.\nhttps://github.com/V-AIS/tensorflow\nì•„ì§ ì™„ë²½í•˜ê²Œ ì •ë¦¬ëœê±´ ì•„ë‹ˆì§€ë§Œ\u0026hellip;\nTensorFlow Object Detection APIë¥¼ ì²˜ìŒ ì ‘í•˜ì‹œëŠ” ë¶„ì´ë¼ë©´ ì‰½ê²Œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡ ì ì–´ë´¤ìŠµë‹ˆë‹¤.\nìˆ˜ì •ë¬ìœ¼ë©´ í•˜ëŠ” ë¶€ë¶„ì´ ìˆìœ¼ë©´ ëŒ“ê¸€ ë‚¨ê²¨ì£¼ì„¸ìš”!\n","permalink":"https://jjerry-test.github.io/blog/tfod/","tags":["TensorFlow"],"title":"Object Detection API"},{"categories":["Living"],"contents":"ì•½ ë‘ ë‹¬ì •ë„ ë‹¬ë ¤ì˜¨ ì±Œë¦°ì§€ê°€ ëë‚˜ê³  ì²˜ìŒìœ¼ë¡œ íœ´ê°€ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.\nì¼ë‹¨ì€ ì´ë²ˆ ì£¼ ì‰¬ê³  ë‹¤ìŒ ì£¼ë¶€í„° ë‹¤ì‹œ ì—°êµ¬ ì‹œì‘..!\nì‰¬ë©´ì„œ TensorFlow Object Detection API ì„¤ëª…ì„œë‚˜ ì¨ë³´ë µë‹ˆë‹¤.\n","permalink":"https://jjerry-test.github.io/blog/vacation/","tags":["Daily"],"title":"íœ´ê°€"},{"categories":["DeepLearning"],"contents":"TensorFlow LSTM ì˜ˆì œ ì½”ë“œ ì—¬ê¸°ì €ê¸°ì„œ TensorFlow LSTM ì½”ë“œë¥¼ ì°¾ë‹¤ê°€ ì˜ˆì œë¥¼ ì œê°€ ë³´ê¸° í¸í•˜ê²Œ ì‘ì„±í–‡ìŠµë‹ˆë‹¤.\nì¶”í›„ì— ìƒˆë¡œìš´ ì½”ë“œë¡œ ì—…ë°ì´íŠ¸ í•  ì˜ˆì •ì…ë‹ˆë‹¤. import tensorflow as tf from tensorflow.keras import models, layers, optimizers, losses from tensorflow.keras.utills import to_categorical # Setting hyper parameter learning_rate = 0.001 total_epoch = 30 batch_size = 128 n_input = 28 n_step = 28 n_hidden1 = 128 n_class = 10 # Loading Mnist Data mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train/255., x_test/255. y_train, y_test = to_categorical(y_train, n_class), to_categorical(y_test, n_class) tf.set_random_seed(777) X = tf.placeholder(tf.float32, [None, n_step, n_input]) Y = tf.placeholder(tf.float32, [None, n_class]) W = tf.Variable(tf.random_normal([n_hidden1, n_class])) b = tf.Variable(tf.random_normal([n_class])) # LSTM cell ì„ ì–¸. # RNNì„ ì“°ê³  ì‹¶ìœ¼ë©´ BasicRNNCellë¡œ ë°”ê¾¸ë©´ ë¨. # Stacked LSTMì„ í•˜ê³  ì‹¶ìœ¼ë©´ cell2 ì„ ì–¸. cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden1) #cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden1) # ì„ ì–¸ëœ LSTM cell, Xë¥¼ ì´ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ìƒì„±. outputs_1, states_1 = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32,scope=\u0026#34;LSTM1\u0026#34;) # Stacked LSTMì„ í•˜ê³  ì‹¶ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì„ ì–¸. #outputs_2, states_2 = tf.nn.dynamic_rnn(cell2, outputs_1, dtype=tf.float32, scope=\u0026#34;LSTM2\u0026#34;) # LSTM -\u0026gt; Fully Connected Layer -\u0026gt; Classification # outputs_1 : [ ? , num_step, num_hidden # -\u0026gt; [num_step, ? , num_hidden] outputs = tf.transpose(outputs_1, [1, 0, 2]) # Sequenceì˜ ë§ˆì§€ë§‰ ì¶œë ¥ê°’ outputs = outputs[-1] model = tf.matmul(outputs, W) + b cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y)) optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) # GPU ë©”ëª¨ë¦¬ í• ë‹¹. config = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config) sess.run(tf.global_variables_initializer()) total_batch = int(mnist.train.num_examples/batch_size) for epoch in range(total_epoch): total_cost = 0 for i in range(total_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) batch_xs = batch_xs.reshape((batch_size, n_step, n_input)) _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys}) total_cost += cost_val print(\u0026#39;Epoch : %04d\u0026#39;% (epoch + 1),\u0026#39;Avg cost : {:f}\u0026#39;.format(total_cost / total_batch)) print(\u0026#39;Optimization Done\u0026#39;) is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1)) accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) test_batch_size = len(mnist.test.images) test_xs = mnist.test.images.reshape(test_batch_size, n_step, n_input) test_ys = mnist.test.labels print(\u0026#39;Test Accuracy : \u0026#39;, sess.run(accuracy, feed_dict={X: test_xs, Y: test_ys})) ","permalink":"https://jjerry-test.github.io/blog/lstm/","tags":["TensorFlow"],"title":"LSTM ì˜ˆì œ ì½”ë“œ"},{"categories":["Living"],"contents":"ê·¼ë˜ì— ë¼ì¦ˆë² ë¦¬íŒŒì´ë¡œ ë¯¸ë‹ˆ PCë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤!\nê·¸ë˜ì„œ ë§Œë“œëŠ” ê³¼ì •ì— ëŒ€í•´ ê¸€ì„ ì¨ë³´ë ¤ê³  í•©ë‹ˆë‹¤!\nì œê°€ ì‚¬ìš©í•œ ë³´ë“œëŠ”\u0026hellip;. https://www.raspberrypi.org/products/raspberry-pi-3-model-b-plus/\nì˜¬í•´ ì¶œì‹œí•œ ë¼ì¦ˆë² ë¦¬íŒŒì´ 3 B+ ì…ë‹ˆë‹¤!(ê°œì¸ì ìœ¼ë¡œ ë¼ì¦ˆë² ë¦¬ë¥¼ ì²˜ìŒ ì¨ë´…ë‹ˆë‹¤\u0026hellip;)\në°©ì—´íŒë„ ë¶™ì—¬ì¤¬ì§€ìš”!\n  ì•„ë˜ ì‚¬ì§„ì€ ì œê°€ HDDë¥¼ ì¥ì°© í•  ê²ƒì´ê¸° ë•Œë¬¸ì—\u0026hellip;.SATA í™•ì¥ë³´ë“œ\u0026hellip;ê·¸ë¦¬ê³  ì¼€ì´ìŠ¤, ì–´ëŒ‘í„° êµ¬ë§¤ ë‚´ì—­ì…ë‹ˆë‹¤!\n  ì†”ì§íˆ\u0026hellip;ë¶€í’ˆë“¤ ëšë”± ëšë”± ì œì‘(ì¡°ë¦½ì´ ë§ì„ ë“¯\u0026hellip;)í•˜ëŠ” ê²ƒ ë³´ë‹¤ ì•Œë¦¬ë°œ ìƒí’ˆë“¤ ê¸°ë‹¤ë¦¬ëŠ”ê²Œ ë” í˜ë“¤ì—ˆì–´ìš”\u0026hellip;. ì¥¬ë¥µ\u0026hellip;.\n7ì¼ ì •ë„ ê¸°ë‹¤ë¦°ê²ƒ ê°™ë„¤ìš”\u0026hellip;..\në°•ìŠ¤ ì˜¤ìë§ˆì ì •ë§ ê´‘ì†ìœ¼ë¡œ ëœ¯ì—ˆìŠµë‹ˆë‹¤\u0026hellip;..\nì¼€ì´ìŠ¤   SATA í™•ì¥ë³´ë“œ   ì „ìš© ì–´ëŒ‘í„° (5V 4A)   ë„˜ë‚˜\u0026hellip;ê°ë™\u0026hellip;\nê¸ˆë°© ì¡°ë¦½í–ˆìŠµë‹ˆë‹¤..\n  ì¡°ë¦½í•˜ëŠ”ë° ì§‘ì¤‘í•´ì„œ ì‚¬ì§„ì„ ì˜ ì•ˆì°ì—ˆì–´ìš”\u0026hellip;ì¥¬ë¥µ\u0026hellip;\n  ì•„\u0026hellip;ì € ë§¨ ì•„ë˜ì— ë‹¤ë¦¬ëŠ” ì•ˆë‹¬ì•„ë„ ë˜ëŠ”ê±°ì˜€ì–´ìš”\u0026hellip;.\n  ì¼€ì´ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ë©´ ì´ë ‡ê²Œ ìƒê²¼ì–´ìš”\u0026hellip;(ëˆ„ê°€ë´ë„ íŒ¬ì´ ë‹¬ë¦´ ë¶€ë¶„..)\n  ë‹¬ì•„ì¤ë‹ˆë‹¤..\n  ì°¨ë¡€ ì°¨ë¡€ ì „ì› ì—°ê²°\u0026hellip;\n  ë’·íŒì„ ì¡°ë¦½í•˜ê³ !\n  ë¸Œë¦¿ì§€? ë¥¼ ì—°ê²°!\n  ì–´ëŒ‘í„° ì—°ê²° í›„ ì¼œì‹œë©´ ë©ë‹ˆë‹¤\u0026hellip; ì¢€ ë” ì •í™•í•œ í¬ê¸° í™•ì¸ì„ ìœ„í•´ ì‚¬ì§„ì„ ì¢€ ì°ì–´ë´¤ì–´ìš”!\n  ì œ ì£¼ë¨¹ì´ë‘ í•œ ì»·\u0026hellip;\n  ë§ˆì§€ë§‰ìœ¼ë¡œ ì œ 12ì¸ì¹˜ ë§¥ë¶ì´ë‘ í•œì»·\u0026hellip;.\n  ì²˜ìŒìœ¼ë¡œ ë¼ì¦ˆë² ë¦¬íŒŒì´ë¥¼ ì‚¬ìš©í•´ë³´ë©´ì„œ\u0026hellip;ì •ë§ ì¬ë°Œê³  ì‹ ê¸°í•˜ë„¤ìš” ã…ã…ã… (ë¬¼ë¡  ê°€ë” ì„¤ì •í•˜ë‹¤ê°€ ë»‘ë‚˜ë©´\u0026hellip;ì§œì¦\u0026hellip;.)\në‚˜ì¤‘ì—” ìº ì„ ë‹¬ì•„ì„œ ì‚¬ë¬¼ì¸ì‹ ê°™ì€ ê±¸ í•´ë³¼ê¹Œ í•©ë‹ˆë‹¤!\në„ˆë¬´\u0026hellip;.ë¶ˆì¹œì ˆí•œ ê¸€ ê°™ë„¤ìš”\u0026hellip;.\në‚˜ì¤‘ì— ë˜ ë­”ê°€ ì¬ë¯¸ë‚œê±¸ í•˜ë©´ ì˜¬ë¦¬ê² ìŠµë‹ˆë‹¤!\nê°ì‚¬í•©ë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/raspberry-mini-pc/","tags":["Hardware"],"title":"ë¼ì¦ˆë² ë¦¬íŒŒì´ë¡œ ë¯¸ë‹ˆ PC ë§Œë“¤ê¸°!"},{"categories":["Living"],"contents":"Building Desktop í•™ë¶€ìƒ ì—°êµ¬ì›ìœ¼ë¡œ ì—°êµ¬ì‹¤ì—ì„œ ê³µë¶€ì™€ ì—°êµ¬ë¥¼ í•˜ê³  ìˆì§€ë§Œ..\nê°œì¸ìš© ë°ìŠ¤í¬íƒ‘ì´ ì—†ì–´ì„œ ë…¸íŠ¸ë¶ì´ êµ‰ì¥íˆ ê³ ìƒì„ ë§ì´ í–ˆìŠµë‹ˆë‹¤..\n1ì°¨ í”¼í•´ì Dell Inspiron 15 7559..\ní˜„ì¬ëŠ” ë‹¤ë¥¸ ì£¼ì¸ì—ê²Œë¡œ ë„˜ì–´ê°„ ìƒíƒœ..\n2ì°¨ í”¼í•´ì Lenovo yoga 2 pro..\nê·¼ë˜ì— ê³ ìƒí•˜ë‹¤ê°€ ì¹œêµ¬í•œí…Œ ë„˜ì–´ê°..\nê²°êµ­!\nì–´ì§œí”¼ ë‚˜ì¤‘ì— ì§‘ì—ì„œ ì“¸ ë°ìŠ¤í¬íƒ‘ì„ ë§ì¶˜ë‹¤ê³  ìƒê°í•˜ê³ !\në¶€í’ˆì„ ê°ê° ì‚¬ì„œ ì¡°ë¦½ì„ í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤!!\nCPU : ì¸í…” i5-7600\në©”ì¸ë³´ë“œ : ê¸°ê°€ë°”ì´íŠ¸ Z270N-WIFI ë“€ëŸ¬ë¸”ì—ë””ì…˜\në©”ëª¨ë¦¬ : ì»¤ì„¸ì–´ VENGEANCE PC4-21300 8G x 2\nVGA : EVGA GeForce GTX 1050ti sc gaming\nSSD : ì‚¼ì„± 850 EVO M.2 (256GB)\nHDD : Toshiba 1TB MQ01ABD100\nì¼€ì´ìŠ¤ : Fractal Design node 202\níŒŒì›Œ : ì»¤ì„¸ì–´ SF450 80PLUS GOLD\nì˜¤ìë§ˆì í¬ìŠ¤íŒ… ìƒê°ì—†ì´ ì¡°ë¦½ì„ í•´ë²„ë¦¼\u0026hellip;\n  ì»¤ì„¸ì–´ì˜ ìœ„ì—„\u0026hellip;..ë²ˆì©ë²ˆì© ë¨\u0026hellip;.ì´ì˜ë‹¤\u0026hellip;\n  ì‚¬ì´íŠ¸ì—ëŠ” Fractal ë¡œê³ ê°€ ìœ„ë¡œ ì˜¬ë¼ì˜¤ê²Œ í•´ë†¨ì§€ë§Œ\u0026hellip;\nê·¸ë ‡ê²Œí•˜ë©´ íŒŒì›Œì˜ ìœ„ì¹˜ê°€ ìœ„ê°€ ë˜ë²„ë ¤ì„œ ë‚œ ë°˜ëŒ€ë¡œ ë¡œê³ ê°€ ì•„ë˜ë¡œ ë‚´ë ¤ê°€ê²Œ í–ˆìŠµë‹ˆë‹¤.\ní›„ê¸°\n  ë°ìŠ¤í¬íƒ‘ì„ ì‚¬ìš©ë§Œ í•´ë´¤ì§€ ì¡°ë¦½ì€ ì²˜ìŒì´ì˜€ìŒ..\n  ê´œíˆ ì‘ì€ ì¼€ì´ìŠ¤ë¡œ í•´ì„œ ì„ ì •ë¦¬ê°€ í˜ë“¤ì—ˆìŒ..\n  ì‚´ ìˆ˜ ìˆëŠ”ê±´ ì¤‘ê³ ë‚˜ë¼ë¥¼ ì´ìš©í•´ì„œ ë¯¸ê°œë´‰í’ˆì„ ì‹¸ê²Œ ì‚¼.\n  25ë§Œì› ì •ë„ ì ˆì•½í•¨.\n  ë‚´ ëˆˆì—ë§Œ ì´ì˜ë©´ ë¬ìŒ.\n  ì•„.. ITX ì¼€ì´ìŠ¤ë¥¼ ì¡°ë¦½í•  ë• ëŒ€ë¶€ë¶„ SFX ê·œê²©ì„ ì“°ì§€ë§Œ..\nê°™ì€ SFXë¼ë„ í¬ê¸°ë¥¼ ê³ ë ¤í•´ì•¼í•¨ . .\n  ","permalink":"https://jjerry-test.github.io/blog/desktop/","tags":["Hardware"],"title":"ë°ìŠ¤í¬íƒ‘ ì¡°ë¦½ê¸°"},{"categories":["Ubuntu"],"contents":"í™˜ê²½ êµ¬ì¶•\u0026hellip;. Deep Learningì„ í•˜ëŠ”ë° GPUë¥¼ ì‚¬ìš©í•´ì•¼í•˜ë‹ˆ CUDAì™€ cuDNNì„ ì„¤ì¹˜í•˜ê¸°ë¡œ í•˜ì£ .\n1. NVIDIA ê·¸ë˜í”½ ë“œë¼ì´ë²„ ì„¤ì¹˜ Ubuntu 14.04 ì—ì„  ê·¸ë˜í”½ ë“œë¼ì´ë²„ë¥¼ ì„¤ì¹˜í•˜ëŠ”ë° êµ‰ì¥íˆ\u0026hellip;\në§ì€ ê³ ë‚œê³¼ ì—­ê²½ì„ ê²ªì—ˆìŠµë‹ˆë‹¤\u0026hellip;\n(ê·¸ë˜ì„œ ê²°êµ­ ì„¤ì¹˜ ëª»í•´ë´„..)\ní•˜ì§€ë§Œ 16.04ì—ì„  ì–´ë µì§€ ì•ŠìŒ!!!!!!\n(ê°ˆì•„íƒ„ ê²°ì •ì ì¸ ì´ìœ \u0026hellip;)\nê·¸ë ‡ë‹¤ë©´ ì„¤ì¹˜ë¥¼ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.\nsudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get install nvidia-364 (ì œ ë…¸íŠ¸ë¶ì€ GTX 960m ì´ë¯€ë¡œ..)\nê·¸ëƒ¥ í„°ë¯¸ë„ì— ì…ë ¥í•˜ì‹œë©´ ë˜ìš”.\nNVIDIA ì‚¬ì´íŠ¸ì—ì„œ GPU ë²„ì „ì´ë‘ ë§ëŠ”ê±¸ë¡œ!!\nê·¸ê²Œ ë‚˜ì„ ë“¯í•©ë‹ˆë‹¤.\nê·¸ í›„ ë¡œê·¸ì•„ì›ƒì„ í•˜ê³  ë‹¤ì‹œ ë¡œê·¸ì¸ì„ í•˜ë ¤ê³  í•˜ë©´ ë¬´í•œ ë¡œê·¸ì¸ì´ ë  ê±°ì—ìš”.\n(ì•ˆê·¸ëŸ¬ë©´ ì¢‹ê³ ..)\nCtrl + Alt + F1ì„ ëˆŒëŸ¬ tty1ìœ¼ë¡œ ë“¤ì–´ê°€ì„œ ë¡œê·¸ì¸ í›„ rebootì„ ì‹¤í–‰í•´ì¤ì‹œë‹¤.\në¡œê·¸ì¸ í›„ NVIDIA X Server Settings ë¥¼ ì¼œë©´\n  ìš”ë˜ ì„¤ì¹˜ ëœê±¸ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n2. CUDA ì„¤ì¹˜ NVIDIA ë“œë¼ì´ë²„ë¥¼ ì„¤ì¹˜ í–ˆìœ¼ë‹ˆ ì´ì œ CUDAë¥¼ ì„¤ì¹˜í•´ë³´ë„ë¡ í•˜ì£ .\nhttps://developer.nvidia.com/cuda-downloads ì— ë“¤ì–´ê°€ì‹œë©´ ìš”ë¡œì½”ë¡¬ ëœ¨ëŠ”ë°ìš”.\nê°ìì˜ í™˜ê²½ì— ë§ì¶°ì„œ ì„ íƒí•œ í›„ ë‹¤ìš´ë¡œë“œ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\nrun íŒŒì¼ì„ ë‹¤ìš´ì„ ë°›ì€ í›„ì— í„°ë¯¸ë„ì„ ì—¬ì‹œê³  run íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œëœ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•©ë‹ˆë‹¤.\nê·¸ë¦¬ê³  íŒŒì¼ì˜ ê¶Œí•œì„ ëª¨ë“  ì‚¬ìš©ìê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.\nê·¸ë¦¬ê³  ì‹¤í–‰!\ncd Downloads sudo chmod a+r cuda_7.5.18_linux.run sudo ./cuda_7.5.18_linux.run --override or sudo sh cuda_7.5.18_linux.run --override 16.04ëŠ” gcc, g++ ë²„ì „ì´ 5.x ë¼ override ë¥¼ í•´ì¤˜ì•¼í•©ë‹ˆë‹¤..\nì‹¤í–‰í•˜ë©´ ì–´ì©Œê³  ì €ì©Œê³  ì•½ê´€ ë‚´ìš©..? ì´ ë‚˜ì˜¤ëŠ”ë°ìš”\u0026hellip;\nì‚´í¬ì‹œ q ë¥¼ ëˆŒëŸ¬ì¤ë‹ˆë‹¤.\nê·¸ëŸ¬ë©´\u0026hellip;ì‚¬ìš©ì ì •ì˜ ì„¤ì¹˜ë¼ê³  í• ê¹Œìš”??\nì´ê²ƒ ì €ê²ƒ ë¬¼ì–´ë´…ë‹ˆë‹¤.\nDo you accept the previously read EULA? (accept/decline/quit): accept You are attempting to install on an unsupported configuration. Do you wish to continue? ((y)es/(n)o) [ default is no ]: yes Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 352.39? ((y)es/(n)o/(q)uit): no Install the CUDA 7.5 Toolkit? ((y)es/(n)o/(q)uit): yes Enter Toolkit Location [ default is /usr/local/cuda-7.5 ]: ê·¸ëƒ¥ ì—”í„° Do you want to install a symbolic link at /usr/local/cuda? ((y)es/(n)o/(q)uit): yes Install the CUDA 7.5 Samples? ((y)es/(n)o/(q)uit): no ì´ëŸ°ì‹ìœ¼ë¡œ ì„¸íŒ…ì„ í•´ì¤ë‹ˆë‹¤.\nê·¸ë¦¬ê³  ê·¸ì € ê¸°ë‹¤ë¦¬ê³  ê¸°ë‹¤ë¦¼\u0026hellip;.\nì„¤ì¹˜ê°€ ë‹¤ ë˜ì—ˆìœ¼ë©´ ë­ ë”°ë¡œ ì‹¤í–‰í•  ê±´ ì—†ìŠµë‹ˆë‹¤.\në°”ë¡œ cuDNN ì„¤ì¹˜ë¥¼ í•˜ì£ .\nì´ê±´ ì„¤ì¹˜\u0026hellip;.ëŠ” ì•„ë‹ˆê³  ê·¸ëƒ¥ cuda ë””ë ‰í† ë¦¬ì— íŒŒì¼ì„ íŒŒì¼ ë³µì‚¬? í•˜ëŠ”ê±°ì—ìš”.\n3. cuDNN ì„¤ì¹˜ ë¨¼ì € cuDNN íŒŒì¼ì„ ë°›ìœ¼ì…”ì•¼í•˜ëŠ”ë°ìš”.\nhttps://developer.nvidia.com/rdp/cudnn-download\nì—¬ê¸°ì— ë“¤ì–´ê°€ì…”ì„œ ê³„ì • ë§Œë“œì‹œê³ \u0026hellip;.\nì‚¬ìš© ìš©ë„\u0026hellip; ì‚¬ìš© í•  ë¼ì´ë¸ŒëŸ¬ë¦¬.. ë“±ë“± ì„ íƒí•˜ì‹œê³  ì›í•˜ì‹œëŠ” ë²„ì „ download í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\nì €ëŠ” 4.0ìœ¼ë¡œ í–ˆì–´ìš”.\nTensorflowë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì—\u0026hellip; ê·¸ë¦¬ê³  ë‚˜ì„œ í„°ë¯¸ë„ì„ ì¼œê³ \ncd /usr/local sudo tar zxf ~/Downloads/cudnn-7.0-linux-x64-v4.0-prod.tgz ì´ë ‡ê²Œ í•˜ì‹œê±°ë‚˜ ì•„ë‹ˆë©´\ncd cudnn ë‹¤ìš´ë¡œë“œê²½ë¡œ sudo tar zxf cudnn-7.0-linux-x64-v4.0-prod.tgz sudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 ì´ë ‡ê²Œ í•´ì£¼ì‹œêµ¬ìš”.\ní™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤. í„°ë¯¸ë„ì„ ê»ë‹¤ê°€ ë‹¤ì‹œ ì¼œì£¼ì‹œê³ \nsudo gedit ~/.bashrc ê·¸ë ¤ë©´ ì–´ë–¤ ë©”ëª¨ì¥? ë¬¸ì„œ íŒŒì¼ì´ ì—´ë¦½ë‹ˆë‹¤. ë§¤~~~~ì•¤ ì•„ë˜ ë¶€ë¶„ì—\nexport PATH=/usr/local/cuda-7.5/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda-7.5/lib64:$LD_LIBRARY_PATH ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.\nì´ë ‡ê²Œ GPU ì„¤ì •ì´ ëë‚¬ìŠµë‹ˆë‹¤\u0026hellip;\nì´ì œ caffeì™€ tensorflowë¥¼ ì„¤ì¹˜í•´ì•¼ê² êµ°ìš”..\ní—˜ë‚œí—ˆë‹¤\u0026hellip;\nê°œê°•ì´ë¼ ì£½ê² ëŠ”ë°..\n","permalink":"https://jjerry-test.github.io/blog/cuda/","tags":["Setting"],"title":"Ubuntu CUDA \u0026 Cudnn ì„¤ì¹˜í•˜ê¸°"},{"categories":["Ubuntu"],"contents":"ìš°ë¶„íˆ¬ë¥¼ ì„¤ì¹˜ë¥¼ í–ˆìœ¼ë‚˜!!!! í•œê¸€ í‚¤ë³´ë“œë¥¼ ì„¤ì •ì„ í•´ì•¼í•©ë‹ˆë‹¤. ê·¸ëŸ¼ ì‹œì‘í•´ë³¼ê¹Œìš”. (ì´ë²ˆì—ë„ ì‚¬ì§„ ì´¤ë¼ë¼ë½..)\n1. ì‹œìŠ¤í…œ í•œê¸€ ì„¤ì • ë¶€íŒ…ì„ í–ˆìŠµë‹ˆë‹¤. í¬ìœ¼\u0026hellip; ê¹”ë”ê¹”ë”\u0026hellip;\n  ì˜¤ë¥¸ìª½ ìœ„ì— ì „ì› ë²„íŠ¼ ëˆ„ë¥´ê³ \nSystem Settingsë¡œ ë“¤ì–´ê°€ì„œ Language Supportë¥¼ ëˆŒëŸ¬ì¤ë‹ˆë‹¤.\n  ê·¸ëŸ¼ ì„¤ì¹˜ë¥¼ í•´ì•¼í•œë‹¤ê³  í•˜ë„¤ìš”. ë¬»ì§€ë„ ë”°ì§€ì§€ë„ ë§ê³  ì„¤ì¹˜í•©ì‹œë‹¤. (ë’¤ì— ì‚¬ì§„ì°ì€ê±´ ì• êµë¡œ ë„˜ì–´ê°‘ì‹œë‹¤\u0026hellip;)\n  ë¡œê·¸ì¸ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥í•˜êµ¬ìš”. (ì—¬ê¸°ë„ ìˆë„¤\u0026hellip;)\n  ê·¸ëŸ¼ ì„¤ì¹˜ê°€ ì‹œì‘ë©ë‹ˆë‹¤.\n  ì„¤ì¹˜ê°€ ëë‚˜ë©´ í•œêµ­ì–´ë¼ê³  ë˜¿! ë‚˜ì˜µë‹ˆë‹¤.\n  ì•„ë˜ì— ìˆëŠ” í•œêµ­ì–´ë¥¼ ë“œë˜ê·¸í•´ì„œ ë§¨ ìœ„ë¡œ ì˜¬ë ¤ì¤ë‹ˆë‹¤.\n    ê·¸ë¦¬ê³  \u0026lsquo;Apply System-wide\u0026rsquo;ë¥¼ í´ë¦­!\n  ë¹„ë°€ë²ˆí˜¸ ì…ë ¥í•˜ì‹œêµ¬ìš”.\n  Regional FormatsëŠ” ì„¤ì¹˜í• ë•Œ ì´ë¯¸ Seoulë¡œ í•´ì„œ ìƒê´€ì€ ì—†ì§€ë§Œ ê·¸ëƒ¥ ë“¤ì–´ê°€ì„œ \u0026lsquo;Apply system-wide\u0026rsquo;ëˆŒëŸ¬ì¤ë‹ˆë‹¤.\n  ë¡œê·¸ì•„ì›ƒì„ í•˜ê³  ë‹¤ì‹œ ë¡œê·¸ì¸ í•´ì£¼ì„¸ìš”!! ê·¸ëŸ¼ ë‹¤ìŒê³¼ ê°™ì´ ì°½ì´ ëœ¨ëŠ”ë° ì €ëŠ” ì˜ë¬¸ì´ í¸í•´ì„œ ê·¸ëƒ¥ ì´ë¦„ì„ ìœ ì§€í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.\n  2. í•œê¸€ í‚¤ë³´ë“œ ì„¤ì •í•˜ê¸° í‚¤ë³´ë“œë¥¼ ì„¤ì •í•´ë³´ì£ .\nì‹œìŠ¤í…œ ì„¤ì •ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.\nê·¸ë¦¬ê³  í…ìŠ¤íŠ¸ ì…ë ¥ì°½ì„ ëˆŒëŸ¬ì¤ë‹ˆë‹¤.\n  ë‹¤ìŒê³¼ ê°™ì´ ëœ¨ëŠ”ë°ìš”. ì €ê¸°ì— +ë¥¼ ëˆŒëŸ¬ì¤ë‹ˆë‹¤. (ì •ë§ ë”ëŸ½ê²Œ ëª»ê·¸ë¦¬ë„¤..)\n  ì­‰ì­‰ì­ˆìš±~ ë‚´ë ¤ì„œ \u0026lsquo;í•œêµ­ì–´ (Hangul)(IBus)\u0026lsquo;ë¥¼ ì„ íƒí•˜ê³  \u0026lsquo;ì¶”ê°€\u0026rsquo;ë²„íŠ¼ì„ ëˆ„ë¦…ë‹ˆë‹¤.\n  ì§€ê¸ˆì€ í‚¤ ë³€í™˜ì´ Super+ìŠ¤í˜ì´ìŠ¤ë¡œ ë˜ì–´ìˆë„¤ìš”. ì´ëŒ€ë¡œ ì“°ì‹¤ë¶„ë“¤ì€ ì“°ì…”ë„ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ!! ì €ëŠ” í•œ/ì˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ê¸° ë•Œë¬¸ì—\u0026hellip; ì € í‚¤ë¥¼ ë°”ê¿”ë³´ë„ë¡ í•˜ì£ . \u0026lsquo;í‚¤ë³´ë“œ ì„¤ì •\u0026hellip;\u0026lsquo;ì„ ëˆ„ë¦…ë‹ˆë‹¤.\n  ìíŒì…ë ¥ íƒ­ì—ì„œ êµ¬ì„±í‚¤ë¥¼ \u0026lsquo;ì˜¤ë¥¸ìª½ Alt\u0026rsquo;ë¡œ ë°”ê¾¼ í›„ì— \u0026lsquo;ë‹¤ìŒ ì…ë ¥ ì†ŒìŠ¤ë¡œ ì „í™˜\u0026rsquo; í‚¤ëŠ” í•œ/ì˜ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n  ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ ëŒì•„ì™€ì„œ í•œêµ­ì–´ë¥¼ ì„ íƒí•œ í›„ ì‚¬ì§„ì— í‘œì‹œëœ ë„êµ¬ ë²„íŠ¼ì„ ëˆ„ë¦…ë‹ˆë‹¤.\n  ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ì˜¤ëŠ”ë°ìš”.\n\u0026lsquo;í•œê¸€ ëª¨ë“œë¡œ ì‹œì‘\u0026rsquo;ì„ ì²´í¬í•œ í›„ í™•ì¸ì„ ëˆŒëŸ¬ì¤ë‹ˆë‹¤.\n  ì´ì œ í•œê¸€ì— ëŒ€í•œ ì„¤ì •ì´ ëë‚¬ìŠµë‹ˆë‹¤!\në‹¤ì‹œ ë¡œê·¸ì•„ì›ƒì„ í•œ í›„ ë¡œê·¸ì¸ì„ í•˜ì‹œë©´ í•œê¸€ì„ ì“°ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!!\nì˜¤ëŠ˜ì€ í¬ìŠ¤íŒ… Dayì¸ë“¯í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ì˜¤ëŠ˜ ìš°ë¶„íˆ¬ë¥¼ ì¬ì„¤ì¹˜ í–ˆê¸° ë•Œë¬¸ì´ì£ \u0026hellip; ìœ¼í•˜í•˜í•˜í•˜í•˜í•˜í•˜í•³\u0026hellip;\u0026hellip; ìš°ë¶„íˆ¬ ë‹¤ì‹œ ì…‹íŒ…í•˜ë©´ì„œ í¬ìŠ¤íŒ…ì¤‘\u0026hellip;. ë‹¤ìŒì€ ì†Œí”„íŠ¸ì›¨ì–´ ê´€ë ¨ ì„¸íŒ… í¬ìŠ¤íŒ…í•˜ê² ìŠµë‹ˆë‹¤.\n","permalink":"https://jjerry-test.github.io/blog/setting/","tags":["Setting"],"title":"Ubuntu í•œê¸€ ì„¤ì •"},{"categories":["Ubuntu"],"contents":"Deep Learningì„ í•˜ê¸° ìœ„í•´ windowsë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ\u0026hellip;\nì €ëŠ” Ubuntuë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ Ubuntuë¡œ ë°°ì› ìœ¼ë‹ˆ..ã…ã…\nì, ê·¸ëŸ¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤ .\nì œê°€ ì˜ˆì „ì— 16.04ë¥¼ ë‹¤ìš´ë°›ì•„ì„œ ì„¤ì¹˜í•œê±°ë¼..\në‹¤ìš´ë¡œë“œëŠ” 16.04.1ë¡œ ë˜ì–´ìˆëŠ”ë° ì„¤ì¹˜ëŠ” 16.04ë¡œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\ní•˜ì§€ë§Œ ì„¤ì¹˜ ë°©ë²•ì€ ë˜‘ê°™ìŒ!\nê·¸ë¦¬ê³  ë“€ì–¼ë¶€íŒ…ì€ Windowsê°€ ë¨¼ì € ì„¤ì¹˜ë˜ì–´ì•¼ í•œë‹¤ëŠ”ê±° ìŠì§€ë§ˆì„¸ìš”.\n1. ì„¤ì¹˜ ì „ì— í™•ì¼ í•´ì•¼í•  ì‚¬í•­  BIOSì—ì„œ secure booting disable Windows ì—ì„œ ë¹ ë¥¸ ì‹œì‘ ì¼œê¸° ì˜µì…˜ ë„ê¸°          1.1 ì„¤ì¹˜ í•  ë””ìŠ¤í¬ ë³¼ë¥¨ ë§Œë“¤ê¸°   ì €ëŠ” HDDì— ì„¤ì¹˜í•  ê±°ë‹ˆê¹Œ HDD ì£¼ íŒŒí‹°ì…˜ì—ì„œ ë§ˆìš°ìŠ¤ ì˜¤ë¥¸ìª½ í´ë¦­!\n  ë³¼ë¥¨ì„ ì¶•ì†Œí•©ì‹œë‹¤.\n  ì›í•˜ì‹œëŠ” ë§Œí¼ ì…ë ¥í•˜ì„¸ìš”. 102,400 MBëŠ” ì˜ˆì‹œì¼ë¿\u0026hellip;\n  í• ë‹¹ë˜ì§€ ì•Šì€ ë¶€ë¶„ì´ 100GB ìƒê²¼ë„¤ìš”!\n  2. Ubuntu ì„¤ì¹˜ ê³¼ì • 2.1 Ubuntu 16.04.1 LTS image Download http://www.ubuntu.com/download/desktopì— ë“¤ì–´ê°‘ë‹ˆë‹¤.\n  Downloadë¥¼ í´ë¦­í•˜ë©´ ë‹¤ìŒ ì‚¬ì§„ê³¼ ê°™ì´ ë‚˜ì˜¤ëŠ”ë° í \u0026hellip;ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.\ní›„ì›..ê°™ì§€ë§Œ \u0026lsquo;Not now, take me to the download\u0026rsquo;ë¥¼ í´ë¦­í•©ë‹ˆë‹¤.\n  ë‹¤ìš´ë¡œë“œ ì°½ì´ ìë™ìœ¼ë¡œ ë‚˜ì˜µë‹ˆë‹¤.\nì €ì¥í•  ê²½ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”!\n  2.2 Ubuntu 16.04.1 LTS ë¶€íŒ… ë””ìŠ¤í¬ ë§Œë“¤ê¸° https://rufus.akeo.ieì—ì„œ utilì„ ë°›ìŠµë‹ˆë‹¤.\n  Utilì„ ì‹¤í–‰!\nì²˜ìŒìœ¼ë¡œ ë¶€íŒ… ë””ìŠ¤í¬ë¡œ ì“¸ USBë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n  OS imageë¥¼ ë¶ˆëŸ¬ì™€ì•¼ê² ì£ ?\nì €~~ê¸° ì•„ì´ì½˜ ëˆŒëŸ¬ì£¼ì„¸ìš”!\n  ISO íŒŒì¼ì´ ìˆëŠ” ê³³ìœ¼ë¡œ ê°€ì„œ ì„ íƒí•˜ê³  ì—´ê¸°!\n  ê·¸ë¦¬ê³  ì‹œì‘ì„ ëˆ„ë¦…ë‹ˆë‹¤!\n  êµ¬êµ¬ì ˆì ˆ\u0026hellip; êµ¬êµ¬ì ˆì ˆ\u0026hellip; \u0026lsquo;OK\u0026rsquo; ëˆŒëŸ¬ì£¼ì„¸ìš”.\n  ì´ëŸ° ê²½ê³ ëŠ”\u0026hellip;ì €ì—ê²Œ í†µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤..\n\u0026lsquo;í™•ì¸\u0026rsquo;ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!\n  ì§„í–‰ì´ ë©ë‹ˆë‹¤.\n  ë˜ê³  ìˆëŠ”\u0026hellip;ê±´ê°€..?\n  6ë¶„ 36ì´ˆ ì •ë„ ê±¸ë ¸ë„¤ìš”..\n  ê·¸ëŸ¼ ì´ì œ ì„¤ì¹˜ë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤.\n2.3 Ubuntu ì„¤ì¹˜í•˜ê¸° BIOS ì„¸íŒ…ì—ì„œ ë¶€íŒ… ìˆœì„œë¥¼ USBë¡œ ë°”ê¿”ì£¼ì‹œê±°ë‚˜\nBoot Optionì—ì„œ USBë¥¼ ì„ íƒí•´ì„œ ë¶€íŒ…í•©ë‹ˆë‹¤.\nê²€ì€ í™”ë©´ì´ ë‚˜ì˜¤ë©´ì„œ \u0026lsquo;Try Ubuntu without installing\u0026rsquo;, \u0026lsquo;Install ì–´ì©Œê³ \u0026hellip;\u0026rsquo; ë“±ì´ ìˆëŠ”ë°ìš”.\nì €ëŠ” ìŠ¤í¬ë¦°ìƒ·ì„ í•´ì•¼í•˜ë¯€ë¡œ \u0026lsquo;Try Ubuntu without installing\u0026rsquo; ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.\n\u0026lsquo;ë˜ë¡œë¡±\u0026rsquo;í•˜ë©´ì„œ í™”ë©´ì´ ëœ¹ë‹ˆë‹¤!\n  ë°”íƒ•í™”ë©´ì— \u0026lsquo;Install Ubuntu 16.04 LTS\u0026rsquo; ì‹¤í–‰í•©ë‹ˆë‹¤. ë””ë ‰í† ë¦¬ ì´ë¦„ì´ ì˜ì–´ì¸ê²Œ ì¢‹ìœ¼ë‹ˆê¹Œ ì¼ë‹¨ Englishë¡œ ì§„í–‰.\n  ì„¤ì¹˜í•˜ë©´ì„œ ì—…ë°ì´íŠ¸, ë‹¤ë¥¸ ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ì¹˜ ì—¬ë¶€ëŠ”..ë§ˆìŒëŒ€ë¡œ\u0026hellip;\n  ì„¤ì¹˜ ë°©ì‹ì— ëŒ€í•œ ì˜µì…˜ì´ ë‚˜ì˜µë‹ˆë‹¤.\n\u0026lsquo;Something else\u0026rsquo;ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.\nê·¸ë˜ì•¼ ë” ì•ˆì‹¬í•˜ê³  ì„¤ì¹˜í•˜ë‹ˆê¹Œìš”\u0026hellip;\n  ìœˆë„ìš°ì—ì„œ íŒŒí‹°ì…˜ì„ ë‚˜ëˆ´ì—ˆì£ .\nì €ëŠ” SSDì— ìœˆë„ìš°, HDDì— ìš°ë¶„íˆ¬ë¥¼ ì„¤ì¹˜í•  ê²ƒì´ë‹ˆ..\nHDD íŒŒí‹°ì…˜ì„ ë‚˜ëˆ ë†¨ì—ˆìŠµë‹ˆë‹¤.\nì–¼ë§ˆë‚˜ ë‚˜ëˆŒê±´ì§€ëŠ”..ì—¬ëŸ¬ë¶„ ë§ˆìŒ\u0026hellip; ì „ 64GB ì •ë„ ë‚˜ëˆ´ìŠµë‹ˆë‹¤. (100GB ëºë‹¤ê°€ ë‹¤ì‹œ ì¤„ì„\u0026hellip;) \u0026lsquo;free space\u0026rsquo;ë¥¼ ì„ íƒí•˜ê³  ì™¼ìª½ ì•„ë˜ì— \u0026lsquo;+\u0026lsquo;ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.\n  ë¨¼ì € Swap íŒŒí‹°ì…˜ë¶€í„° ë§Œë“¤ì£ .\nìì‹ ì˜ RAM ìš©ëŸ‰ê³¼ ë˜‘ê°™ì´ ë§ì¶”ë¼ëŠ” ë¶„ë„ ê³„ì‹œê³ \nìš”ì¦˜ì€ RAM ìš©ëŸ‰ì´ ë‹¤ ì»¤ì„œ í•„ìš”ì—†ë‹¤ëŠ” ë¶„ë„ ê³„ì…”ì„œ\nì €ëŠ” ê·¸ ì¤‘ê°„ì¸ 8GBë¥¼ Swapìœ¼ë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\nì„¤ì •ë²•ì€ ì•„ë˜ ì‚¬ì§„ê³¼ ê°™ì´!\nê·¸ë¦¬ê³  \u0026lsquo;OK\u0026rsquo;ë¥¼ ëˆ„ë¦…ì‹œë‹¤.\n  ì•„ë˜ ì‚¬ì§„ê³¼ ê°™ì´ swap ì˜ì—­ì´ unknownìœ¼ë¡œ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.\nì´ì œ Ubuntu íŒŒí‹°ì…˜ì„ ë§Œë“¤ì–´ë³´ì£ .\në˜‘ê°™ì´ \u0026lsquo;free space\u0026rsquo; ë¥¼ ì„ íƒí•˜ê³  \u0026lsquo;+\u0026rsquo; ë²„íŠ¼ì„ ëˆŒëŸ¬ì¤ë‹ˆë‹¤.\n  \u0026lsquo;Size\u0026rsquo;ì—ëŠ” ë‚¨ì€ ìš©ëŸ‰ì„ ë‹¤ ì”ë‹ˆë‹¤.\në‚˜ë¨¸ì§€ëŠ” ì•„ë˜ ì‚¬ì§„ê³¼ ê°™ì´ ì„¤ì • !\nê·¸ë¦¬ê³  \u0026lsquo;OK\u0026rsquo;ë¥¼ ëˆ„ë¦…ì‹œë‹¤.\n  ì´ì œ Boot loader ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼í•˜ëŠ”ë°ìš”.\nì•„ë« ë¶€ë¶„ì— \u0026lsquo;Device for boot loader installation\u0026rsquo;ì—ì„œ\në””ìŠ¤í¬ë¥¼ ì„ íƒí•˜ë©´ ë©ë‹ˆë‹¤.\nSSDì— ì§€ì •ì„ í•˜ì‹œê¸°ë„ í•˜ì§€ë§Œ ì €ëŠ” HDD ì§€ì •ì„ í–ˆìŠµë‹ˆë‹¤.\nê·¸ë¦¬ê³  \u0026lsquo;Install Now\u0026rsquo;ì„ í´ë¦­í•©ë‹ˆë‹¤.\n  ë””ìŠ¤í¬ê°€ ë³€ê²½ë©ë‹ˆë‹¤. ë­ ì´ëŸ° ë‚´ìš©ì´ ëœ¨ëŠ”ë°\nê·¸ëƒ¥. ì¡°ìš©íˆ. ì‚´í¬ì‹œ.\n\u0026lsquo;Continue\u0026rsquo; ë¥¼ ëˆŒëŸ¬ì¤ì‹œë‹¤.\n(ë¶ˆì•ˆí•˜ì‹œë©´ ë‹¤ì‹œ í™•ì¸ í•˜ì‹œêµ¬ìš”..)\n  ìœ„ì¹˜ ì„¤ì •í•˜ì‹œêµ¬ìš”!\n  í‚¤ë³´ë“œ ì„¤ì •ì€ ê·¸ëƒ¥ ì˜ì–´ë¡œ ë†”ë‘ì„¸ìš”. ë‚˜ì¤‘ì— ì„¤ì¹˜ í›„ì— ì„¤ì • í• ê²ë‹ˆë‹¤!\n  ê³„ì • ì„¤ì •ì„ í•©ë‹ˆë‹¤!\nì €ëŠ” Jerryë‹ˆê¹Œ\u0026hellip; ìš°ë¶„íˆ¬ ê³„ì •ë„ Jerry.\n  ì„¤ì¹˜ ì‹œì‘!!\n  ê·¸ì €\u0026hellip; ê¸°ë‹¤ë¦¼\u0026hellip;\n    ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\nê³¼ê°íˆ ë°”ë¡œ \u0026lsquo;Restart Now\u0026rsquo;ë¥¼ í´ë¦½í–ˆìŠµë‹ˆë‹¤.\n  ì²« í¬ìŠ¤íŒ…ì´ë¼ êµ‰ì¥íˆ ë‚œì¡í•˜ê³  ìŠ¤í¬ë¡¤ì´ ì´¤ë¼ë¼ë¼ë½í•˜ë„¤ìš”\u0026hellip;\nì¢€ ë” ì •ë¦¬í•´ì„œ ì“°ëŠ” ìŠµê´€ì„ ë“¤ì—¬ì•¼ì§€\u0026hellip;\nUbuntuë¥¼ ì ‘í•˜ê²Œ ë˜ë©´ì„œ êµ‰ì¥íˆ ë‹¤ì‚¬ë‹¤ë‚œí•œ ê²½í—˜ì„ í•˜ëŠ” ì €ì™€ ì œ ë…¸íŠ¸ë¶\u0026hellip;.\nì–´ì©Œë©´ ì¡°ë§Œê°„ Windows ì™€ Ubuntuë¥¼ ì¬ì„¤ì¹˜ í•  ìˆ˜ë„ ìˆì„ ë“¯í•©ë‹ˆë‹¤..\në‹¤ìŒ í¬ìŠ¤íŒ…ì€ ì„¤ì¹˜ í›„ ì„¸íŒ…ì— ê´€ë ¨í•˜ì—¬ í•˜ê² ìŠµë‹ˆë‹¤!\n","permalink":"https://jjerry-test.github.io/blog/dualboot/","tags":["Setting"],"title":"Windows 10 \u0026 Ubuntu 16.04 ë“€ì–¼ë¶€íŒ…"}]